{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BBC Text Representations - Dense Methods\n",
        "\n",
        "**Roll Number:** SE22UARI195\n",
        "\n",
        "**Tasks:**\n",
        "1. Word2Vec Skip-gram with Negative Sampling (NS)\n",
        "2. Word2Vec CBOW with Negative Sampling (NS)\n",
        "3. Word2Vec Skip-gram with Hierarchical Softmax (HS)\n",
        "4. Word2Vec CBOW with Hierarchical Softmax (HS)\n",
        "5. GloVe (pretrained 100d)\n",
        "6. TF-IDF weighted pooling for document vectors\n",
        "7. Compare training speed and quality\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Load Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# Gensim for Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Roll Number: SE22UARI195\n",
            "Cache Directory: ../cache\n",
            "Models Directory: ../models\n",
            "Data Directory: ../data\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "ROLL = \"SE22UARI195\"\n",
        "CACHE_DIR = Path(\"../cache\")\n",
        "MODELS_DIR = Path(\"../models\")\n",
        "DATA_DIR = Path(\"../data\")\n",
        "\n",
        "print(f\"Roll Number: {ROLL}\")\n",
        "print(f\"Cache Directory: {CACHE_DIR}\")\n",
        "print(f\"Models Directory: {MODELS_DIR}\")\n",
        "print(f\"Data Directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Loading preprocessed data...\n",
            "\n",
            "‚úÖ TRAIN: 1335 documents\n",
            "‚úÖ DEV: 445 documents\n",
            "‚úÖ TEST: 445 documents\n",
            "‚úÖ Vocabulary: 20,404 unique tokens\n"
          ]
        }
      ],
      "source": [
        "# Load preprocessed data\n",
        "print(\"üìÇ Loading preprocessed data...\\n\")\n",
        "\n",
        "with open(CACHE_DIR / 'train_processed.pkl', 'rb') as f:\n",
        "    train_df = pickle.load(f)\n",
        "print(f\"‚úÖ TRAIN: {len(train_df)} documents\")\n",
        "\n",
        "with open(CACHE_DIR / 'dev_processed.pkl', 'rb') as f:\n",
        "    dev_df = pickle.load(f)\n",
        "print(f\"‚úÖ DEV: {len(dev_df)} documents\")\n",
        "\n",
        "with open(CACHE_DIR / 'test_processed.pkl', 'rb') as f:\n",
        "    test_df = pickle.load(f)\n",
        "print(f\"‚úÖ TEST: {len(test_df)} documents\")\n",
        "\n",
        "with open(CACHE_DIR / 'vocab_counter.pkl', 'rb') as f:\n",
        "    vocab_counter = pickle.load(f)\n",
        "print(f\"‚úÖ Vocabulary: {len(vocab_counter):,} unique tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TF-IDF vectorizer loaded\n",
            "   Vocabulary size: 11,515\n"
          ]
        }
      ],
      "source": [
        "# Load TF-IDF vectorizer (needed for weighted pooling)\n",
        "with open(MODELS_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "print(f\"‚úÖ TF-IDF vectorizer loaded\")\n",
        "print(f\"   Vocabulary size: {len(tfidf_vectorizer.vocabulary_):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Tokenized sentences:\n",
            "  TRAIN: 1335 documents\n",
            "  DEV: 445 documents\n",
            "  TEST: 445 documents\n",
            "\n",
            "  Sample: ['worldcom', 'bos', 'left', 'book', 'alone', 'former', 'worldcom', 'bos', 'bernie', 'ebbers', 'accused', 'overseeing', '11bn', '8bn', 'fraud']...\n"
          ]
        }
      ],
      "source": [
        "# Prepare tokenized sentences for Word2Vec\n",
        "train_sentences = train_df['tokens'].tolist()\n",
        "dev_sentences = dev_df['tokens'].tolist()\n",
        "test_sentences = test_df['tokens'].tolist()\n",
        "\n",
        "print(\"\\nüìä Tokenized sentences:\")\n",
        "print(f\"  TRAIN: {len(train_sentences)} documents\")\n",
        "print(f\"  DEV: {len(dev_sentences)} documents\")\n",
        "print(f\"  TEST: {len(test_sentences)} documents\")\n",
        "print(f\"\\n  Sample: {train_sentences[0][:15]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "def tfidf_weighted_pooling(tokens_list, word_vectors, tfidf_vec, tfidf_vocab):\n",
        "    \"\"\"\n",
        "    Convert list of token lists to document vectors using TF-IDF weighted mean.\n",
        "    \n",
        "    Args:\n",
        "        tokens_list: List of token lists\n",
        "        word_vectors: Word embedding model (Word2Vec or dict of embeddings)\n",
        "        tfidf_vec: Fitted TfidfVectorizer\n",
        "        tfidf_vocab: TF-IDF vocabulary dict\n",
        "    \n",
        "    Returns:\n",
        "        numpy array of shape (n_docs, embedding_dim)\n",
        "    \"\"\"\n",
        "    # Get embedding dimension\n",
        "    if hasattr(word_vectors, 'wv'):\n",
        "        # Word2Vec model\n",
        "        vector_size = word_vectors.wv.vector_size\n",
        "        vocab = word_vectors.wv\n",
        "    else:\n",
        "        # Dictionary of embeddings (GloVe)\n",
        "        vector_size = len(next(iter(word_vectors.values())))\n",
        "        vocab = word_vectors\n",
        "    \n",
        "    doc_vectors = []\n",
        "    \n",
        "    for tokens in tqdm(tokens_list, desc=\"Pooling\"):\n",
        "        if len(tokens) == 0:\n",
        "            # Empty document - use zero vector\n",
        "            doc_vectors.append(np.zeros(vector_size))\n",
        "            continue\n",
        "        \n",
        "        # Get TF-IDF weights for this document\n",
        "        text = ' '.join(tokens)\n",
        "        tfidf_vec_doc = tfidf_vec.transform([text]).toarray()[0]\n",
        "        \n",
        "        # Accumulate weighted vectors\n",
        "        weighted_sum = np.zeros(vector_size)\n",
        "        total_weight = 0.0\n",
        "        \n",
        "        for token in tokens:\n",
        "            # Check if token is in both embeddings and TF-IDF vocab\n",
        "            if token in tfidf_vocab:\n",
        "                tfidf_idx = tfidf_vocab[token]\n",
        "                tfidf_weight = tfidf_vec_doc[tfidf_idx]\n",
        "                \n",
        "                # Get word vector\n",
        "                if hasattr(vocab, '__contains__'):\n",
        "                    if token in vocab:\n",
        "                        if hasattr(vocab, 'get_vector'):\n",
        "                            word_vec = vocab.get_vector(token)\n",
        "                        else:\n",
        "                            word_vec = vocab[token]\n",
        "                        weighted_sum += tfidf_weight * word_vec\n",
        "                        total_weight += tfidf_weight\n",
        "                else:\n",
        "                    if token in vocab:\n",
        "                        word_vec = vocab[token]\n",
        "                        weighted_sum += tfidf_weight * word_vec\n",
        "                        total_weight += tfidf_weight\n",
        "        \n",
        "        # Average by total weight\n",
        "        if total_weight > 0:\n",
        "            doc_vectors.append(weighted_sum / total_weight)\n",
        "        else:\n",
        "            # No valid tokens - use zero vector\n",
        "            doc_vectors.append(np.zeros(vector_size))\n",
        "    \n",
        "    return np.array(doc_vectors)\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Word2Vec - Skip-gram with Negative Sampling (NS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Training Word2Vec Skip-gram with Negative Sampling...\n",
            "\n",
            "Total training tokens: 285,829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training complete in 6.25s\n",
            "   Tokens/sec: 45,760\n",
            "   Vocabulary size: 9,848\n",
            "   Vector size: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîß Training Word2Vec Skip-gram with Negative Sampling...\\n\")\n",
        "\n",
        "# Count total tokens for speed calculation\n",
        "total_tokens_train = sum(len(sent) for sent in train_sentences)\n",
        "print(f\"Total training tokens: {total_tokens_train:,}\")\n",
        "\n",
        "# Train Word2Vec Skip-gram with NS\n",
        "start_time = time.time()\n",
        "\n",
        "w2v_sg_ns = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    sg=1,  # Skip-gram\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=3,\n",
        "    negative=5,  # Negative sampling with k=5\n",
        "    hs=0,  # No hierarchical softmax\n",
        "    epochs=10,\n",
        "    workers=4,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "train_time_sg_ns = time.time() - start_time\n",
        "tokens_per_sec_sg_ns = total_tokens_train / train_time_sg_ns\n",
        "\n",
        "print(f\"‚úÖ Training complete in {train_time_sg_ns:.2f}s\")\n",
        "print(f\"   Tokens/sec: {tokens_per_sec_sg_ns:,.0f}\")\n",
        "print(f\"   Vocabulary size: {len(w2v_sg_ns.wv):,}\")\n",
        "print(f\"   Vector size: {w2v_sg_ns.wv.vector_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "w2v_sg_ns.save(str(MODELS_DIR / 'w2v_sg_ns.model'))\n",
        "print(\"üíæ Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Nearest neighbors (Skip-gram NS):\n",
            "\n",
            "  government:\n",
            "    rebate          : 0.6258\n",
            "    curb            : 0.6149\n",
            "    quango          : 0.6146\n",
            "    regulation      : 0.6135\n",
            "    rethink         : 0.6120\n",
            "\n",
            "  film:\n",
            "    movie           : 0.6730\n",
            "    cinema          : 0.6549\n",
            "    hollywood       : 0.6513\n",
            "    festival        : 0.6410\n",
            "    documentary     : 0.6254\n",
            "\n",
            "  music:\n",
            "    label           : 0.5955\n",
            "    urban           : 0.5926\n",
            "    collection      : 0.5680\n",
            "    digital         : 0.5666\n",
            "    downloading     : 0.5654\n",
            "\n",
            "  technology:\n",
            "    optical         : 0.6866\n",
            "    uwb             : 0.6835\n",
            "    intel           : 0.6748\n",
            "    matsushita      : 0.6559\n",
            "    nokia           : 0.6469\n",
            "\n",
            "  economy:\n",
            "    export          : 0.7806\n",
            "    moderate        : 0.7782\n",
            "    growth          : 0.7702\n",
            "    economic        : 0.7675\n",
            "    stable          : 0.7641\n"
          ]
        }
      ],
      "source": [
        "# Test nearest neighbors\n",
        "print(\"\\nüîç Nearest neighbors (Skip-gram NS):\")\n",
        "test_words = ['government', 'film', 'music', 'technology', 'economy']\n",
        "\n",
        "for word in test_words:\n",
        "    if word in w2v_sg_ns.wv:\n",
        "        similar = w2v_sg_ns.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\n  {word}:\")\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Creating document vectors with TF-IDF weighted pooling...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07fbddf487b04791bc53ece6621d7061",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/1335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb880088098a473182c663ab6e02524f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7183f49685b6492f8a5661561c3b1cc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Document vectors created:\n",
            "   TRAIN: (1335, 100)\n",
            "   DEV: (445, 100)\n",
            "   TEST: (445, 100)\n"
          ]
        }
      ],
      "source": [
        "# Create document vectors using TF-IDF weighted pooling\n",
        "print(\"\\nüìä Creating document vectors with TF-IDF weighted pooling...\")\n",
        "\n",
        "X_train_w2v_sg_ns = tfidf_weighted_pooling(\n",
        "    train_sentences, w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_dev_w2v_sg_ns = tfidf_weighted_pooling(\n",
        "    dev_sentences, w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_test_w2v_sg_ns = tfidf_weighted_pooling(\n",
        "    test_sentences, w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Document vectors created:\")\n",
        "print(f\"   TRAIN: {X_train_w2v_sg_ns.shape}\")\n",
        "print(f\"   DEV: {X_dev_w2v_sg_ns.shape}\")\n",
        "print(f\"   TEST: {X_test_w2v_sg_ns.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Document vectors saved!\n"
          ]
        }
      ],
      "source": [
        "# Save document vectors\n",
        "np.save(MODELS_DIR / 'X_train_w2v_sg_ns.npy', X_train_w2v_sg_ns)\n",
        "np.save(MODELS_DIR / 'X_dev_w2v_sg_ns.npy', X_dev_w2v_sg_ns)\n",
        "np.save(MODELS_DIR / 'X_test_w2v_sg_ns.npy', X_test_w2v_sg_ns)\n",
        "print(\"\\nüíæ Document vectors saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Word2Vec - CBOW with Negative Sampling (NS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Training Word2Vec CBOW with Negative Sampling...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training complete in 2.01s\n",
            "   Tokens/sec: 142,017\n",
            "   Vocabulary size: 9,848\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîß Training Word2Vec CBOW with Negative Sampling...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "w2v_cbow_ns = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    sg=0,  # CBOW\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=3,\n",
        "    negative=5,\n",
        "    hs=0,\n",
        "    epochs=10,\n",
        "    workers=4,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "train_time_cbow_ns = time.time() - start_time\n",
        "tokens_per_sec_cbow_ns = total_tokens_train / train_time_cbow_ns\n",
        "\n",
        "print(f\"‚úÖ Training complete in {train_time_cbow_ns:.2f}s\")\n",
        "print(f\"   Tokens/sec: {tokens_per_sec_cbow_ns:,.0f}\")\n",
        "print(f\"   Vocabulary size: {len(w2v_cbow_ns.wv):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "w2v_cbow_ns.save(str(MODELS_DIR / 'w2v_cbow_ns.model'))\n",
        "print(\"üíæ Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Nearest neighbors (CBOW NS):\n",
            "\n",
            "  government:\n",
            "    policy          : 0.9564\n",
            "    pension         : 0.9528\n",
            "    local           : 0.9464\n",
            "    plan            : 0.9414\n",
            "    proposal        : 0.9406\n",
            "\n",
            "  film:\n",
            "    actor           : 0.9144\n",
            "    aviator         : 0.9063\n",
            "    hollywood       : 0.8926\n",
            "    award           : 0.8903\n",
            "    abortionist     : 0.8807\n",
            "\n",
            "  music:\n",
            "    digital         : 0.9171\n",
            "    song            : 0.9030\n",
            "    mp3             : 0.8964\n",
            "    screen          : 0.8952\n",
            "    recorder        : 0.8951\n",
            "\n",
            "  technology:\n",
            "    network         : 0.9499\n",
            "    apple           : 0.9454\n",
            "    content         : 0.9358\n",
            "    using           : 0.9300\n",
            "    internet        : 0.9265\n",
            "\n",
            "  economy:\n",
            "    growth          : 0.9554\n",
            "    unece           : 0.9527\n",
            "    rise            : 0.9443\n",
            "    forecast        : 0.9433\n",
            "    debt            : 0.9431\n"
          ]
        }
      ],
      "source": [
        "# Test nearest neighbors\n",
        "print(\"\\nüîç Nearest neighbors (CBOW NS):\")\n",
        "\n",
        "for word in test_words:\n",
        "    if word in w2v_cbow_ns.wv:\n",
        "        similar = w2v_cbow_ns.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\n  {word}:\")\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Creating document vectors with TF-IDF weighted pooling...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5f9be2f64944a9c90ff2614b6aa9b99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/1335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61dc9e4f7f3048ea9b62004be7523d5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13f81c03a56a4094a96626d373f7dd4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Document vectors created:\n",
            "   TRAIN: (1335, 100)\n",
            "   DEV: (445, 100)\n",
            "   TEST: (445, 100)\n"
          ]
        }
      ],
      "source": [
        "# Create document vectors\n",
        "print(\"\\nüìä Creating document vectors with TF-IDF weighted pooling...\")\n",
        "\n",
        "X_train_w2v_cbow_ns = tfidf_weighted_pooling(\n",
        "    train_sentences, w2v_cbow_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_dev_w2v_cbow_ns = tfidf_weighted_pooling(\n",
        "    dev_sentences, w2v_cbow_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_test_w2v_cbow_ns = tfidf_weighted_pooling(\n",
        "    test_sentences, w2v_cbow_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Document vectors created:\")\n",
        "print(f\"   TRAIN: {X_train_w2v_cbow_ns.shape}\")\n",
        "print(f\"   DEV: {X_dev_w2v_cbow_ns.shape}\")\n",
        "print(f\"   TEST: {X_test_w2v_cbow_ns.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Document vectors saved!\n"
          ]
        }
      ],
      "source": [
        "# Save document vectors\n",
        "np.save(MODELS_DIR / 'X_train_w2v_cbow_ns.npy', X_train_w2v_cbow_ns)\n",
        "np.save(MODELS_DIR / 'X_dev_w2v_cbow_ns.npy', X_dev_w2v_cbow_ns)\n",
        "np.save(MODELS_DIR / 'X_test_w2v_cbow_ns.npy', X_test_w2v_cbow_ns)\n",
        "print(\"\\nüíæ Document vectors saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Word2Vec - Skip-gram with Hierarchical Softmax (HS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Training Word2Vec Skip-gram with Hierarchical Softmax...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training complete in 6.38s\n",
            "   Tokens/sec: 44,775\n",
            "   Vocabulary size: 9,848\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîß Training Word2Vec Skip-gram with Hierarchical Softmax...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "w2v_sg_hs = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    sg=1,  # Skip-gram\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=3,\n",
        "    negative=0,  # No negative sampling\n",
        "    hs=1,  # Hierarchical softmax\n",
        "    epochs=10,\n",
        "    workers=4,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "train_time_sg_hs = time.time() - start_time\n",
        "tokens_per_sec_sg_hs = total_tokens_train / train_time_sg_hs\n",
        "\n",
        "print(f\"‚úÖ Training complete in {train_time_sg_hs:.2f}s\")\n",
        "print(f\"   Tokens/sec: {tokens_per_sec_sg_hs:,.0f}\")\n",
        "print(f\"   Vocabulary size: {len(w2v_sg_hs.wv):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "w2v_sg_hs.save(str(MODELS_DIR / 'w2v_sg_hs.model'))\n",
        "print(\"üíæ Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Nearest neighbors (Skip-gram HS):\n",
            "\n",
            "  government:\n",
            "    raynsford       : 0.6564\n",
            "    local           : 0.6053\n",
            "    taxation        : 0.5943\n",
            "    expenditure     : 0.5806\n",
            "    outcry          : 0.5751\n",
            "\n",
            "  film:\n",
            "    hollywood       : 0.6844\n",
            "    sundance        : 0.6640\n",
            "    nod             : 0.6541\n",
            "    festival        : 0.6458\n",
            "    gritty          : 0.6447\n",
            "\n",
            "  music:\n",
            "    1xtra           : 0.6151\n",
            "    napster         : 0.5973\n",
            "    digital         : 0.5964\n",
            "    showcasing      : 0.5692\n",
            "    downloading     : 0.5669\n",
            "\n",
            "  technology:\n",
            "    lucent          : 0.6971\n",
            "    evolution       : 0.6619\n",
            "    samsung         : 0.6588\n",
            "    matsushita      : 0.6579\n",
            "    souped          : 0.6449\n",
            "\n",
            "  economy:\n",
            "    growth          : 0.8142\n",
            "    economic        : 0.7490\n",
            "    unece           : 0.7368\n",
            "    export          : 0.7129\n",
            "    indicator       : 0.6685\n"
          ]
        }
      ],
      "source": [
        "# Test nearest neighbors\n",
        "print(\"\\nüîç Nearest neighbors (Skip-gram HS):\")\n",
        "\n",
        "for word in test_words:\n",
        "    if word in w2v_sg_hs.wv:\n",
        "        similar = w2v_sg_hs.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\n  {word}:\")\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Creating document vectors with TF-IDF weighted pooling...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b38266a790e43eebc5191910b7090b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/1335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d03af8a4e6c54e8e8ca94d3c04b25d00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2d339cba6fd40068404728049df7866",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Document vectors created:\n",
            "   TRAIN: (1335, 100)\n",
            "   DEV: (445, 100)\n",
            "   TEST: (445, 100)\n"
          ]
        }
      ],
      "source": [
        "# Create document vectors\n",
        "print(\"\\nüìä Creating document vectors with TF-IDF weighted pooling...\")\n",
        "\n",
        "X_train_w2v_sg_hs = tfidf_weighted_pooling(\n",
        "    train_sentences, w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_dev_w2v_sg_hs = tfidf_weighted_pooling(\n",
        "    dev_sentences, w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_test_w2v_sg_hs = tfidf_weighted_pooling(\n",
        "    test_sentences, w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Document vectors created:\")\n",
        "print(f\"   TRAIN: {X_train_w2v_sg_hs.shape}\")\n",
        "print(f\"   DEV: {X_dev_w2v_sg_hs.shape}\")\n",
        "print(f\"   TEST: {X_test_w2v_sg_hs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Document vectors saved!\n"
          ]
        }
      ],
      "source": [
        "# Save document vectors\n",
        "np.save(MODELS_DIR / 'X_train_w2v_sg_hs.npy', X_train_w2v_sg_hs)\n",
        "np.save(MODELS_DIR / 'X_dev_w2v_sg_hs.npy', X_dev_w2v_sg_hs)\n",
        "np.save(MODELS_DIR / 'X_test_w2v_sg_hs.npy', X_test_w2v_sg_hs)\n",
        "print(\"\\nüíæ Document vectors saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Word2Vec - CBOW with Hierarchical Softmax (HS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Training Word2Vec CBOW with Hierarchical Softmax...\n",
            "\n",
            "‚úÖ Training complete in 1.98s\n",
            "   Tokens/sec: 144,672\n",
            "   Vocabulary size: 9,848\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîß Training Word2Vec CBOW with Hierarchical Softmax...\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "w2v_cbow_hs = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    sg=0,  # CBOW\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=3,\n",
        "    negative=0,\n",
        "    hs=1,  # Hierarchical softmax\n",
        "    epochs=10,\n",
        "    workers=4,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "train_time_cbow_hs = time.time() - start_time\n",
        "tokens_per_sec_cbow_hs = total_tokens_train / train_time_cbow_hs\n",
        "\n",
        "print(f\"‚úÖ Training complete in {train_time_cbow_hs:.2f}s\")\n",
        "print(f\"   Tokens/sec: {tokens_per_sec_cbow_hs:,.0f}\")\n",
        "print(f\"   Vocabulary size: {len(w2v_cbow_hs.wv):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "w2v_cbow_hs.save(str(MODELS_DIR / 'w2v_cbow_hs.model'))\n",
        "print(\"üíæ Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Nearest neighbors (CBOW HS):\n",
            "\n",
            "  government:\n",
            "    health          : 0.7455\n",
            "    reform          : 0.6846\n",
            "    immigration     : 0.6764\n",
            "    policy          : 0.6737\n",
            "    plan            : 0.6399\n",
            "\n",
            "  film:\n",
            "    nominated       : 0.6774\n",
            "    star            : 0.6751\n",
            "    hollywood       : 0.6722\n",
            "    movie           : 0.6621\n",
            "    aviator         : 0.6564\n",
            "\n",
            "  music:\n",
            "    digital         : 0.6575\n",
            "    song            : 0.5569\n",
            "    artist          : 0.5523\n",
            "    medium          : 0.5421\n",
            "    gadget          : 0.5310\n",
            "\n",
            "  technology:\n",
            "    data            : 0.6571\n",
            "    content         : 0.6557\n",
            "    network         : 0.6553\n",
            "    wireless        : 0.6543\n",
            "    us              : 0.6477\n",
            "\n",
            "  economy:\n",
            "    growth          : 0.8156\n",
            "    economic        : 0.7897\n",
            "    export          : 0.7758\n",
            "    forecast        : 0.7201\n",
            "    overall         : 0.6969\n"
          ]
        }
      ],
      "source": [
        "# Test nearest neighbors\n",
        "print(\"\\nüîç Nearest neighbors (CBOW HS):\")\n",
        "\n",
        "for word in test_words:\n",
        "    if word in w2v_cbow_hs.wv:\n",
        "        similar = w2v_cbow_hs.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\n  {word}:\")\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Creating document vectors with TF-IDF weighted pooling...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8897874b155c46eeaf289bb23d979ce4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/1335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d791a27ac174488b95d5b420e243a05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab8b0e946fba4ea9a28783590f22c500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Document vectors created:\n",
            "   TRAIN: (1335, 100)\n",
            "   DEV: (445, 100)\n",
            "   TEST: (445, 100)\n"
          ]
        }
      ],
      "source": [
        "# Create document vectors\n",
        "print(\"\\nüìä Creating document vectors with TF-IDF weighted pooling...\")\n",
        "\n",
        "X_train_w2v_cbow_hs = tfidf_weighted_pooling(\n",
        "    train_sentences, w2v_cbow_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_dev_w2v_cbow_hs = tfidf_weighted_pooling(\n",
        "    dev_sentences, w2v_cbow_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_test_w2v_cbow_hs = tfidf_weighted_pooling(\n",
        "    test_sentences, w2v_cbow_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Document vectors created:\")\n",
        "print(f\"   TRAIN: {X_train_w2v_cbow_hs.shape}\")\n",
        "print(f\"   DEV: {X_dev_w2v_cbow_hs.shape}\")\n",
        "print(f\"   TEST: {X_test_w2v_cbow_hs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Document vectors saved!\n"
          ]
        }
      ],
      "source": [
        "# Save document vectors\n",
        "np.save(MODELS_DIR / 'X_train_w2v_cbow_hs.npy', X_train_w2v_cbow_hs)\n",
        "np.save(MODELS_DIR / 'X_dev_w2v_cbow_hs.npy', X_dev_w2v_cbow_hs)\n",
        "np.save(MODELS_DIR / 'X_test_w2v_cbow_hs.npy', X_test_w2v_cbow_hs)\n",
        "print(\"\\nüíæ Document vectors saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. GloVe Embeddings (Pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì• Loading GloVe embeddings...\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5a519239de84c28a0c52e06381c12cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading GloVe:   0%|          | 0/400000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Loaded 400,000 word vectors\n",
            "   Vector dimension: 100\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüì• Loading GloVe embeddings...\\n\")\n",
        "\n",
        "glove_path = DATA_DIR / 'glove.6B.100d.txt'\n",
        "\n",
        "if not glove_path.exists():\n",
        "    print(\"‚ùå ERROR: GloVe file not found!\")\n",
        "    print(f\"   Expected location: {glove_path}\")\n",
        "    print(\"\\n   Please download from: http://nlp.stanford.edu/data/glove.6B.zip\")\n",
        "    print(\"   Extract glove.6B.100d.txt to the data/ folder\")\n",
        "    raise FileNotFoundError(f\"GloVe embeddings not found at {glove_path}\")\n",
        "\n",
        "# Load GloVe embeddings into dictionary\n",
        "glove_embeddings = {}\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in tqdm(f, desc=\"Loading GloVe\", total=400000):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = vector\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(glove_embeddings):,} word vectors\")\n",
        "print(f\"   Vector dimension: {len(next(iter(glove_embeddings.values())))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Testing GloVe embeddings:\n",
            "\n",
            "  government:\n",
            "    administration  : 0.7937\n",
            "    governments     : 0.7701\n",
            "    officials       : 0.7590\n",
            "    authorities     : 0.7442\n",
            "    opposition      : 0.7372\n",
            "\n",
            "  film:\n",
            "    movie           : 0.9055\n",
            "    films           : 0.8914\n",
            "    directed        : 0.8124\n",
            "    documentary     : 0.8076\n",
            "    drama           : 0.7929\n",
            "\n",
            "  music:\n",
            "    musical         : 0.8128\n",
            "    songs           : 0.7978\n",
            "    dance           : 0.7897\n",
            "    pop             : 0.7863\n",
            "    recording       : 0.7651\n",
            "\n",
            "  technology:\n",
            "    technologies    : 0.8506\n",
            "    computer        : 0.7642\n",
            "    tech            : 0.7489\n",
            "    software        : 0.7359\n",
            "    systems         : 0.7293\n",
            "\n",
            "  economy:\n",
            "    economic        : 0.8279\n",
            "    growth          : 0.7947\n",
            "    recession       : 0.7692\n",
            "    economies       : 0.7545\n",
            "    recovery        : 0.7491\n"
          ]
        }
      ],
      "source": [
        "# Test nearest neighbors for GloVe\n",
        "print(\"\\nüîç Testing GloVe embeddings:\")\n",
        "\n",
        "def get_most_similar_glove(word, embeddings, topn=5):\n",
        "    \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
        "    if word not in embeddings:\n",
        "        return None\n",
        "    \n",
        "    word_vec = embeddings[word]\n",
        "    similarities = {}\n",
        "    \n",
        "    # Calculate similarities with a subset for speed\n",
        "    for other_word, other_vec in list(embeddings.items())[:50000]:\n",
        "        if other_word == word:\n",
        "            continue\n",
        "        sim = np.dot(word_vec, other_vec) / (np.linalg.norm(word_vec) * np.linalg.norm(other_vec))\n",
        "        similarities[other_word] = sim\n",
        "    \n",
        "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:topn]\n",
        "\n",
        "for word in test_words:\n",
        "    if word in glove_embeddings:\n",
        "        similar = get_most_similar_glove(word, glove_embeddings, topn=5)\n",
        "        print(f\"\\n  {word}:\")\n",
        "        if similar:\n",
        "            for sim_word, score in similar:\n",
        "                print(f\"    {sim_word:15s} : {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n  {word}: NOT IN GLOVE VOCABULARY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Creating document vectors with GloVe + TF-IDF pooling...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4d56c62aa104526a3a3353b525f675f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/1335 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f7b6b9292354c90b5f706c32ac0d464",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc4f7b3501f4a02ab7cb5d4281c959f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pooling:   0%|          | 0/445 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Document vectors created:\n",
            "   TRAIN: (1335, 100)\n",
            "   DEV: (445, 100)\n",
            "   TEST: (445, 100)\n"
          ]
        }
      ],
      "source": [
        "# Create document vectors using GloVe\n",
        "print(\"\\nüìä Creating document vectors with GloVe + TF-IDF pooling...\")\n",
        "\n",
        "X_train_glove = tfidf_weighted_pooling(\n",
        "    train_sentences, glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_dev_glove = tfidf_weighted_pooling(\n",
        "    dev_sentences, glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "X_test_glove = tfidf_weighted_pooling(\n",
        "    test_sentences, glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Document vectors created:\")\n",
        "print(f\"   TRAIN: {X_train_glove.shape}\")\n",
        "print(f\"   DEV: {X_dev_glove.shape}\")\n",
        "print(f\"   TEST: {X_test_glove.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ GloVe document vectors saved!\n"
          ]
        }
      ],
      "source": [
        "# Save GloVe document vectors\n",
        "np.save(MODELS_DIR / 'X_train_glove.npy', X_train_glove)\n",
        "np.save(MODELS_DIR / 'X_dev_glove.npy', X_dev_glove)\n",
        "np.save(MODELS_DIR / 'X_test_glove.npy', X_test_glove)\n",
        "print(\"\\nüíæ GloVe document vectors saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Calculate Health Metrics for Dense Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Calculating health metrics for dense representations...\n",
            "\n",
            "‚úÖ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìä Calculating health metrics for dense representations...\\n\")\n",
        "\n",
        "# Calculate OOV rates\n",
        "def calculate_oov_rate(tokens_list, vocab):\n",
        "    \"\"\"Calculate out-of-vocabulary rate.\"\"\"\n",
        "    total_tokens = 0\n",
        "    oov_tokens = 0\n",
        "    \n",
        "    for tokens in tokens_list:\n",
        "        for token in tokens:\n",
        "            total_tokens += 1\n",
        "            if token not in vocab:\n",
        "                oov_tokens += 1\n",
        "    \n",
        "    return (oov_tokens / total_tokens * 100) if total_tokens > 0 else 0.0\n",
        "\n",
        "# Calculate coverage\n",
        "def calculate_coverage(tokens_list, vocab, k):\n",
        "    \"\"\"Calculate top-k coverage.\"\"\"\n",
        "    token_counts = Counter()\n",
        "    for tokens in tokens_list:\n",
        "        token_counts.update(tokens)\n",
        "    \n",
        "    top_k_tokens = [token for token, _ in token_counts.most_common(k)]\n",
        "    covered = sum(1 for token in top_k_tokens if token in vocab)\n",
        "    \n",
        "    return (covered / k * 100) if k > 0 else 0.0\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec Skip-gram NS:\n",
            "  Vocabulary size: 9,848\n",
            "  OOV rate (TEST): 8.28%\n",
            "  Top-100 coverage: 100.00%\n",
            "  Top-500 coverage: 100.00%\n",
            "  Training time: 6.25s\n",
            "  Tokens/sec: 45,760\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec Skip-gram NS metrics\n",
        "oov_sg_ns = calculate_oov_rate(test_sentences, w2v_sg_ns.wv)\n",
        "cov100_sg_ns = calculate_coverage(test_sentences, w2v_sg_ns.wv, 100)\n",
        "cov500_sg_ns = calculate_coverage(test_sentences, w2v_sg_ns.wv, 500)\n",
        "\n",
        "print(\"Word2Vec Skip-gram NS:\")\n",
        "print(f\"  Vocabulary size: {len(w2v_sg_ns.wv):,}\")\n",
        "print(f\"  OOV rate (TEST): {oov_sg_ns:.2f}%\")\n",
        "print(f\"  Top-100 coverage: {cov100_sg_ns:.2f}%\")\n",
        "print(f\"  Top-500 coverage: {cov500_sg_ns:.2f}%\")\n",
        "print(f\"  Training time: {train_time_sg_ns:.2f}s\")\n",
        "print(f\"  Tokens/sec: {tokens_per_sec_sg_ns:,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word2Vec CBOW NS:\n",
            "  Vocabulary size: 9,848\n",
            "  OOV rate (TEST): 8.28%\n",
            "  Top-100 coverage: 100.00%\n",
            "  Top-500 coverage: 100.00%\n",
            "  Training time: 2.01s\n",
            "  Tokens/sec: 142,017\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec CBOW NS metrics\n",
        "oov_cbow_ns = calculate_oov_rate(test_sentences, w2v_cbow_ns.wv)\n",
        "cov100_cbow_ns = calculate_coverage(test_sentences, w2v_cbow_ns.wv, 100)\n",
        "cov500_cbow_ns = calculate_coverage(test_sentences, w2v_cbow_ns.wv, 500)\n",
        "\n",
        "print(\"\\nWord2Vec CBOW NS:\")\n",
        "print(f\"  Vocabulary size: {len(w2v_cbow_ns.wv):,}\")\n",
        "print(f\"  OOV rate (TEST): {oov_cbow_ns:.2f}%\")\n",
        "print(f\"  Top-100 coverage: {cov100_cbow_ns:.2f}%\")\n",
        "print(f\"  Top-500 coverage: {cov500_cbow_ns:.2f}%\")\n",
        "print(f\"  Training time: {train_time_cbow_ns:.2f}s\")\n",
        "print(f\"  Tokens/sec: {tokens_per_sec_cbow_ns:,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word2Vec Skip-gram HS:\n",
            "  Vocabulary size: 9,848\n",
            "  OOV rate (TEST): 8.28%\n",
            "  Top-100 coverage: 100.00%\n",
            "  Top-500 coverage: 100.00%\n",
            "  Training time: 6.38s\n",
            "  Tokens/sec: 44,775\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec Skip-gram HS metrics\n",
        "oov_sg_hs = calculate_oov_rate(test_sentences, w2v_sg_hs.wv)\n",
        "cov100_sg_hs = calculate_coverage(test_sentences, w2v_sg_hs.wv, 100)\n",
        "cov500_sg_hs = calculate_coverage(test_sentences, w2v_sg_hs.wv, 500)\n",
        "\n",
        "print(\"\\nWord2Vec Skip-gram HS:\")\n",
        "print(f\"  Vocabulary size: {len(w2v_sg_hs.wv):,}\")\n",
        "print(f\"  OOV rate (TEST): {oov_sg_hs:.2f}%\")\n",
        "print(f\"  Top-100 coverage: {cov100_sg_hs:.2f}%\")\n",
        "print(f\"  Top-500 coverage: {cov500_sg_hs:.2f}%\")\n",
        "print(f\"  Training time: {train_time_sg_hs:.2f}s\")\n",
        "print(f\"  Tokens/sec: {tokens_per_sec_sg_hs:,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word2Vec CBOW HS:\n",
            "  Vocabulary size: 9,848\n",
            "  OOV rate (TEST): 8.28%\n",
            "  Top-100 coverage: 100.00%\n",
            "  Top-500 coverage: 100.00%\n",
            "  Training time: 1.98s\n",
            "  Tokens/sec: 144,672\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec CBOW HS metrics\n",
        "oov_cbow_hs = calculate_oov_rate(test_sentences, w2v_cbow_hs.wv)\n",
        "cov100_cbow_hs = calculate_coverage(test_sentences, w2v_cbow_hs.wv, 100)\n",
        "cov500_cbow_hs = calculate_coverage(test_sentences, w2v_cbow_hs.wv, 500)\n",
        "\n",
        "print(\"\\nWord2Vec CBOW HS:\")\n",
        "print(f\"  Vocabulary size: {len(w2v_cbow_hs.wv):,}\")\n",
        "print(f\"  OOV rate (TEST): {oov_cbow_hs:.2f}%\")\n",
        "print(f\"  Top-100 coverage: {cov100_cbow_hs:.2f}%\")\n",
        "print(f\"  Top-500 coverage: {cov500_cbow_hs:.2f}%\")\n",
        "print(f\"  Training time: {train_time_cbow_hs:.2f}s\")\n",
        "print(f\"  Tokens/sec: {tokens_per_sec_cbow_hs:,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GloVe (pretrained):\n",
            "  Vocabulary size: 400,000\n",
            "  OOV rate (TEST): 0.61%\n",
            "  Top-100 coverage: 100.00%\n",
            "  Top-500 coverage: 100.00%\n",
            "  Training time: 0.00s (pretrained)\n",
            "  Tokens/sec: N/A (pretrained)\n"
          ]
        }
      ],
      "source": [
        "# GloVe metrics\n",
        "oov_glove = calculate_oov_rate(test_sentences, glove_embeddings)\n",
        "cov100_glove = calculate_coverage(test_sentences, glove_embeddings, 100)\n",
        "cov500_glove = calculate_coverage(test_sentences, glove_embeddings, 500)\n",
        "\n",
        "print(\"\\nGloVe (pretrained):\")\n",
        "print(f\"  Vocabulary size: {len(glove_embeddings):,}\")\n",
        "print(f\"  OOV rate (TEST): {oov_glove:.2f}%\")\n",
        "print(f\"  Top-100 coverage: {cov100_glove:.2f}%\")\n",
        "print(f\"  Top-500 coverage: {cov500_glove:.2f}%\")\n",
        "print(f\"  Training time: 0.00s (pretrained)\")\n",
        "print(f\"  Tokens/sec: N/A (pretrained)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Dense methods results saved to cache/dense_results.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Compile dense methods results\n",
        "dense_results = {\n",
        "    'w2v_sg_ns': {\n",
        "        'vocab_size': len(w2v_sg_ns.wv),\n",
        "        'vector_dim': w2v_sg_ns.wv.vector_size,\n",
        "        'oov_rate': float(oov_sg_ns),\n",
        "        'top100_coverage': float(cov100_sg_ns),\n",
        "        'top500_coverage': float(cov500_sg_ns),\n",
        "        'train_time_sec': float(train_time_sg_ns),\n",
        "        'tokens_per_sec': float(tokens_per_sec_sg_ns),\n",
        "        'train_shape': list(X_train_w2v_sg_ns.shape),\n",
        "        'dev_shape': list(X_dev_w2v_sg_ns.shape),\n",
        "        'test_shape': list(X_test_w2v_sg_ns.shape)\n",
        "    },\n",
        "    'w2v_cbow_ns': {\n",
        "        'vocab_size': len(w2v_cbow_ns.wv),\n",
        "        'vector_dim': w2v_cbow_ns.wv.vector_size,\n",
        "        'oov_rate': float(oov_cbow_ns),\n",
        "        'top100_coverage': float(cov100_cbow_ns),\n",
        "        'top500_coverage': float(cov500_cbow_ns),\n",
        "        'train_time_sec': float(train_time_cbow_ns),\n",
        "        'tokens_per_sec': float(tokens_per_sec_cbow_ns),\n",
        "        'train_shape': list(X_train_w2v_cbow_ns.shape),\n",
        "        'dev_shape': list(X_dev_w2v_cbow_ns.shape),\n",
        "        'test_shape': list(X_test_w2v_cbow_ns.shape)\n",
        "    },\n",
        "    'w2v_sg_hs': {\n",
        "        'vocab_size': len(w2v_sg_hs.wv),\n",
        "        'vector_dim': w2v_sg_hs.wv.vector_size,\n",
        "        'oov_rate': float(oov_sg_hs),\n",
        "        'top100_coverage': float(cov100_sg_hs),\n",
        "        'top500_coverage': float(cov500_sg_hs),\n",
        "        'train_time_sec': float(train_time_sg_hs),\n",
        "        'tokens_per_sec': float(tokens_per_sec_sg_hs),\n",
        "        'train_shape': list(X_train_w2v_sg_hs.shape),\n",
        "        'dev_shape': list(X_dev_w2v_sg_hs.shape),\n",
        "        'test_shape': list(X_test_w2v_sg_hs.shape)\n",
        "    },\n",
        "    'w2v_cbow_hs': {\n",
        "        'vocab_size': len(w2v_cbow_hs.wv),\n",
        "        'vector_dim': w2v_cbow_hs.wv.vector_size,\n",
        "        'oov_rate': float(oov_cbow_hs),\n",
        "        'top100_coverage': float(cov100_cbow_hs),\n",
        "        'top500_coverage': float(cov500_cbow_hs),\n",
        "        'train_time_sec': float(train_time_cbow_hs),\n",
        "        'tokens_per_sec': float(tokens_per_sec_cbow_hs),\n",
        "        'train_shape': list(X_train_w2v_cbow_hs.shape),\n",
        "        'dev_shape': list(X_dev_w2v_cbow_hs.shape),\n",
        "        'test_shape': list(X_test_w2v_cbow_hs.shape)\n",
        "    },\n",
        "    'glove': {\n",
        "        'vocab_size': len(glove_embeddings),\n",
        "        'vector_dim': 100,\n",
        "        'oov_rate': float(oov_glove),\n",
        "        'top100_coverage': float(cov100_glove),\n",
        "        'top500_coverage': float(cov500_glove),\n",
        "        'train_time_sec': 0.0,\n",
        "        'tokens_per_sec': None,\n",
        "        'train_shape': list(X_train_glove.shape),\n",
        "        'dev_shape': list(X_dev_glove.shape),\n",
        "        'test_shape': list(X_test_glove.shape)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open(CACHE_DIR / 'dense_results.json', 'w') as f:\n",
        "    json.dump(dense_results, f, indent=2)\n",
        "\n",
        "print(\"\\nüíæ Dense methods results saved to cache/dense_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DENSE METHODS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Method               Vocab      OOV%     Top100%   Top500%   Time(s)    Tok/s       \n",
            "--------------------------------------------------------------------------------\n",
            "w2v_sg_ns            9,848      8.28     100.00    100.00    6.25       45,760      \n",
            "w2v_cbow_ns          9,848      8.28     100.00    100.00    2.01       142,017     \n",
            "w2v_sg_hs            9,848      8.28     100.00    100.00    6.38       44,775      \n",
            "w2v_cbow_hs          9,848      8.28     100.00    100.00    1.98       144,672     \n",
            "glove                400,000    0.61     100.00    100.00    0.00       N/A         \n",
            "\n",
            "================================================================================\n",
            "‚úÖ Dense methods training complete!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DENSE METHODS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Method':<20} {'Vocab':<10} {'OOV%':<8} {'Top100%':<9} {'Top500%':<9} {'Time(s)':<10} {'Tok/s':<12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for method_name, metrics in dense_results.items():\n",
        "    tok_s = f\"{metrics['tokens_per_sec']:,.0f}\" if metrics['tokens_per_sec'] else \"N/A\"\n",
        "    print(f\"{method_name:<20} {metrics['vocab_size']:<10,} {metrics['oov_rate']:<8.2f} \"\n",
        "          f\"{metrics['top100_coverage']:<9.2f} {metrics['top500_coverage']:<9.2f} \"\n",
        "          f\"{metrics['train_time_sec']:<10.2f} {tok_s:<12}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ Dense methods training complete!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ Notebook 03: Dense Methods - COMPLETE!\n",
            "\n",
            "Next steps:\n",
            "  1. Run notebook 04: Classification\n",
            "  2. Train classifiers on all representations\n",
            "  3. Compare performance on TEST set\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüéâ Notebook 03: Dense Methods - COMPLETE!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Run notebook 04: Classification\")\n",
        "print(\"  2. Train classifiers on all representations\")\n",
        "print(\"  3. Compare performance on TEST set\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
