{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# BBC Text Representations - Dense Methods\n",
          "\n",
          "**Roll Number:** SE22UARI195\n",
          "\n",
          "**Tasks:**\n",
          "1. Word2Vec Skip-gram with Negative Sampling (NS)\n",
          "2. Word2Vec CBOW with Negative Sampling (NS)\n",
          "3. Word2Vec Skip-gram with Hierarchical Softmax (HS)\n",
          "4. Word2Vec CBOW with Hierarchical Softmax (HS)\n",
          "5. GloVe (pretrained 100d)\n",
          "6. TF-IDF weighted pooling for document vectors\n",
          "7. Compare training speed and quality\n",
          "\n",
          "---"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 1. Setup & Load Preprocessed Data"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Core libraries\n",
          "import pandas as pd\n",
          "import numpy as np\n",
          "import pickle\n",
          "import time\n",
          "from pathlib import Path\n",
          "from collections import Counter\n",
          "\n",
          "# Gensim for Word2Vec\n",
          "from gensim.models import Word2Vec\n",
          "\n",
          "# Sklearn\n",
          "from sklearn.feature_extraction.text import TfidfVectorizer\n",
          "from sklearn.metrics.pairwise import cosine_similarity\n",
          "\n",
          "# Progress bar\n",
          "from tqdm.notebook import tqdm\n",
          "\n",
          "print(\"âœ… Imports successful!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Configuration\n",
          "ROLL = \"SE22UARI195\"\n",
          "CACHE_DIR = Path(\"../cache\")\n",
          "MODELS_DIR = Path(\"../models\")\n",
          "DATA_DIR = Path(\"../data\")\n",
          "\n",
          "print(f\"Roll Number: {ROLL}\")\n",
          "print(f\"Cache Directory: {CACHE_DIR}\")\n",
          "print(f\"Models Directory: {MODELS_DIR}\")\n",
          "print(f\"Data Directory: {DATA_DIR}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Load preprocessed data\n",
          "print(\"ðŸ“‚ Loading preprocessed data...\\n\")\n",
          "\n",
          "with open(CACHE_DIR / 'train_processed.pkl', 'rb') as f:\n",
          "    train_df = pickle.load(f)\n",
          "print(f\"âœ… TRAIN: {len(train_df)} documents\")\n",
          "\n",
          "with open(CACHE_DIR / 'dev_processed.pkl', 'rb') as f:\n",
          "    dev_df = pickle.load(f)\n",
          "print(f\"âœ… DEV: {len(dev_df)} documents\")\n",
          "\n",
          "with open(CACHE_DIR / 'test_processed.pkl', 'rb') as f:\n",
          "    test_df = pickle.load(f)\n",
          "print(f\"âœ… TEST: {len(test_df)} documents\")\n",
          "\n",
          "with open(CACHE_DIR / 'vocab_counter.pkl', 'rb') as f:\n",
          "    vocab_counter = pickle.load(f)\n",
          "print(f\"âœ… Vocabulary: {len(vocab_counter):,} unique tokens\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Load TF-IDF vectorizer (needed for weighted pooling)\n",
          "with open(MODELS_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
          "    tfidf_vectorizer = pickle.load(f)\n",
          "\n",
          "print(f\"âœ… TF-IDF vectorizer loaded\")\n",
          "print(f\"   Vocabulary size: {len(tfidf_vectorizer.vocabulary_):,}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Prepare tokenized sentences for Word2Vec\n",
          "train_sentences = train_df['tokens'].tolist()\n",
          "dev_sentences = dev_df['tokens'].tolist()\n",
          "test_sentences = test_df['tokens'].tolist()\n",
          "\n",
          "print(\"\\nðŸ“Š Tokenized sentences:\")\n",
          "print(f\"  TRAIN: {len(train_sentences)} documents\")\n",
          "print(f\"  DEV: {len(dev_sentences)} documents\")\n",
          "print(f\"  TEST: {len(test_sentences)} documents\")\n",
          "print(f\"\\n  Sample: {train_sentences[0][:15]}...\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 2. Helper Functions"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def tfidf_weighted_pooling(tokens_list, word_vectors, tfidf_vec, tfidf_vocab):\n",
          "    \"\"\"\n",
          "    Convert list of token lists to document vectors using TF-IDF weighted mean.\n",
          "    \n",
          "    Args:\n",
          "        tokens_list: List of token lists\n",
          "        word_vectors: Word embedding model (Word2Vec or dict of embeddings)\n",
          "        tfidf_vec: Fitted TfidfVectorizer\n",
          "        tfidf_vocab: TF-IDF vocabulary dict\n",
          "    \n",
          "    Returns:\n",
          "        numpy array of shape (n_docs, embedding_dim)\n",
          "    \"\"\"\n",
          "    # Get embedding dimension\n",
          "    if hasattr(word_vectors, 'wv'):\n",
          "        # Word2Vec model\n",
          "        vector_size = word_vectors.wv.vector_size\n",
          "        vocab = word_vectors.wv\n",
          "    else:\n",
          "        # Dictionary of embeddings (GloVe)\n",
          "        vector_size = len(next(iter(word_vectors.values())))\n",
          "        vocab = word_vectors\n",
          "    \n",
          "    doc_vectors = []\n",
          "    \n",
          "    for tokens in tqdm(tokens_list, desc=\"Pooling\"):\n",
          "        if len(tokens) == 0:\n",
          "            # Empty document - use zero vector\n",
          "            doc_vectors.append(np.zeros(vector_size))\n",
          "            continue\n",
          "        \n",
          "        # Get TF-IDF weights for this document\n",
          "        text = ' '.join(tokens)\n",
          "        tfidf_vec_doc = tfidf_vec.transform([text]).toarray()[0]\n",
          "        \n",
          "        # Accumulate weighted vectors\n",
          "        weighted_sum = np.zeros(vector_size)\n",
          "        total_weight = 0.0\n",
          "        \n",
          "        for token in tokens:\n",
          "            # Check if token is in both embeddings and TF-IDF vocab\n",
          "            if token in tfidf_vocab:\n",
          "                tfidf_idx = tfidf_vocab[token]\n",
          "                tfidf_weight = tfidf_vec_doc[tfidf_idx]\n",
          "                \n",
          "                # Get word vector\n",
          "                if hasattr(vocab, '__contains__'):\n",
          "                    if token in vocab:\n",
          "                        if hasattr(vocab, 'get_vector'):\n",
          "                            word_vec = vocab.get_vector(token)\n",
          "                        else:\n",
          "                            word_vec = vocab[token]\n",
          "                        weighted_sum += tfidf_weight * word_vec\n",
          "                        total_weight += tfidf_weight\n",
          "                else:\n",
          "                    if token in vocab:\n",
          "                        word_vec = vocab[token]\n",
          "                        weighted_sum += tfidf_weight * word_vec\n",
          "                        total_weight += tfidf_weight\n",
          "        \n",
          "        # Average by total weight\n",
          "        if total_weight > 0:\n",
          "            doc_vectors.append(weighted_sum / total_weight)\n",
          "        else:\n",
          "            # No valid tokens - use zero vector\n",
          "            doc_vectors.append(np.zeros(vector_size))\n",
          "    \n",
          "    return np.array(doc_vectors)\n",
          "\n",
          "print(\"âœ… Helper functions defined!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 3. Word2Vec - Skip-gram with Negative Sampling (NS)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ”§ Training Word2Vec Skip-gram with Negative Sampling...\\n\")\n",
          "\n",
          "# Count total tokens for speed calculation\n",
          "total_tokens_train = sum(len(sent) for sent in train_sentences)\n",
          "print(f\"Total training tokens: {total_tokens_train:,}\")\n",
          "\n",
          "# Train Word2Vec Skip-gram with NS\n",
          "start_time = time.time()\n",
          "\n",
          "w2v_sg_ns = Word2Vec(\n",
          "    sentences=train_sentences,\n",
          "    sg=1,  # Skip-gram\n",
          "    vector_size=100,\n",
          "    window=5,\n",
          "    min_count=3,\n",
          "    negative=5,  # Negative sampling with k=5\n",
          "    hs=0,  # No hierarchical softmax\n",
          "    epochs=10,\n",
          "    workers=4,\n",
          "    seed=42\n",
          ")\n",
          "\n",
          "train_time_sg_ns = time.time() - start_time\n",
          "tokens_per_sec_sg_ns = total_tokens_train / train_time_sg_ns\n",
          "\n",
          "print(f\"âœ… Training complete in {train_time_sg_ns:.2f}s\")\n",
          "print(f\"   Tokens/sec: {tokens_per_sec_sg_ns:,.0f}\")\n",
          "print(f\"   Vocabulary size: {len(w2v_sg_ns.wv):,}\")\n",
          "print(f\"   Vector size: {w2v_sg_ns.wv.vector_size}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save model\n",
          "w2v_sg_ns.save(str(MODELS_DIR / 'w2v_sg_ns.model'))\n",
          "print(\"ðŸ’¾ Model saved!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Test nearest neighbors\n",
          "print(\"\\nðŸ” Nearest neighbors (Skip-gram NS):\")\n",
          "test_words = ['government', 'film', 'music', 'technology', 'economy']\n",
          "\n",
          "for word in test_words:\n",
          "    if word in w2v_sg_ns.wv:\n",
          "        similar = w2v_sg_ns.wv.most_similar(word, topn=5)\n",
          "        print(f\"\\n  {word}:\")\n",
          "        for sim_word, score in similar:\n",
          "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
          "    else:\n",
          "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create document vectors using TF-IDF weighted pooling\n",
          "print(\"\\nðŸ“Š Creating document vectors with TF-IDF weighted pooling...\")\n",
          "\n",
          "X_train_w2v_sg_ns = tfidf_weighted_pooling(\n",
          "    train_sentences, w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_dev_w2v_sg_ns = tfidf_weighted_pooling(\n",
          "    dev_sentences, w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_test_w2v_sg_ns = tfidf_weighted_pooling(\n",
          "    test_sentences, w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "print(f\"\\nâœ… Document vectors created:\")\n",
          "print(f\"   TRAIN: {X_train_w2v_sg_ns.shape}\")\n",
          "print(f\"   DEV: {X_dev_w2v_sg_ns.shape}\")\n",
          "print(f\"   TEST: {X_test_w2v_sg_ns.shape}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save document vectors\n",
          "np.save(MODELS_DIR / 'X_train_w2v_sg_ns.npy', X_train_w2v_sg_ns)\n",
          "np.save(MODELS_DIR / 'X_dev_w2v_sg_ns.npy', X_dev_w2v_sg_ns)\n",
          "np.save(MODELS_DIR / 'X_test_w2v_sg_ns.npy', X_test_w2v_sg_ns)\n",
          "print(\"\\nðŸ’¾ Document vectors saved!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 4. Word2Vec - CBOW with Negative Sampling (NS)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ”§ Training Word2Vec CBOW with Negative Sampling...\\n\")\n",
          "\n",
          "start_time = time.time()\n",
          "\n",
          "w2v_cbow_ns = Word2Vec(\n",
          "    sentences=train_sentences,\n",
          "    sg=0,  # CBOW\n",
          "    vector_size=100,\n",
          "    window=5,\n",
          "    min_count=3,\n",
          "    negative=5,\n",
          "    hs=0,\n",
          "    epochs=10,\n",
          "    workers=4,\n",
          "    seed=42\n",
          ")\n",
          "\n",
          "train_time_cbow_ns = time.time() - start_time\n",
          "tokens_per_sec_cbow_ns = total_tokens_train / train_time_cbow_ns\n",
          "\n",
          "print(f\"âœ… Training complete in {train_time_cbow_ns:.2f}s\")\n",
          "print(f\"   Tokens/sec: {tokens_per_sec_cbow_ns:,.0f}\")\n",
          "print(f\"   Vocabulary size: {len(w2v_cbow_ns.wv):,}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save model\n",
          "w2v_cbow_ns.save(str(MODELS_DIR / 'w2v_cbow_ns.model'))\n",
          "print(\"ðŸ’¾ Model saved!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Test nearest neighbors\n",
          "print(\"\\nðŸ” Nearest neighbors (CBOW NS):\")\n",
          "\n",
          "for word in test_words:\n",
          "    if word in w2v_cbow_ns.wv:\n",
          "        similar = w2v_cbow_ns.wv.most_similar(word, topn=5)\n",
          "        print(f\"\\n  {word}:\")\n",
          "        for sim_word, score in similar:\n",
          "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
          "    else:\n",
          "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create document vectors\n",
          "print(\"\\nðŸ“Š Creating document vectors with TF-IDF weighted pooling...\")\n",
          "\n",
          "X_train_w2v_cbow_ns = tfidf_weighted_pooling(\n",
          "    train_sentences, w2v_cbow_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_dev_w2v_cbow_ns = tfidf_weighted_pooling(\n",
          "    dev_sentences, w2v_cbow_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_test_w2v_cbow_ns = tfidf_weighted_pooling(\n",
          "    test_sentences, w2v_cbow_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "print(f\"\\nâœ… Document vectors created:\")\n",
          "print(f\"   TRAIN: {X_train_w2v_cbow_ns.shape}\")\n",
          "print(f\"   DEV: {X_dev_w2v_cbow_ns.shape}\")\n",
          "print(f\"   TEST: {X_test_w2v_cbow_ns.shape}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save document vectors\n",
          "np.save(MODELS_DIR / 'X_train_w2v_cbow_ns.npy', X_train_w2v_cbow_ns)\n",
          "np.save(MODELS_DIR / 'X_dev_w2v_cbow_ns.npy', X_dev_w2v_cbow_ns)\n",
          "np.save(MODELS_DIR / 'X_test_w2v_cbow_ns.npy', X_test_w2v_cbow_ns)\n",
          "print(\"\\nðŸ’¾ Document vectors saved!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 5. Word2Vec - Skip-gram with Hierarchical Softmax (HS)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ”§ Training Word2Vec Skip-gram with Hierarchical Softmax...\\n\")\n",
          "\n",
          "start_time = time.time()\n",
          "\n",
          "w2v_sg_hs = Word2Vec(\n",
          "    sentences=train_sentences,\n",
          "    sg=1,  # Skip-gram\n",
          "    vector_size=100,\n",
          "    window=5,\n",
          "    min_count=3,\n",
          "    negative=0,  # No negative sampling\n",
          "    hs=1,  # Hierarchical softmax\n",
          "    epochs=10,\n",
          "    workers=4,\n",
          "    seed=42\n",
          ")\n",
          "\n",
          "train_time_sg_hs = time.time() - start_time\n",
          "tokens_per_sec_sg_hs = total_tokens_train / train_time_sg_hs\n",
          "\n",
          "print(f\"âœ… Training complete in {train_time_sg_hs:.2f}s\")\n",
          "print(f\"   Tokens/sec: {tokens_per_sec_sg_hs:,.0f}\")\n",
          "print(f\"   Vocabulary size: {len(w2v_sg_hs.wv):,}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save model\n",
          "w2v_sg_hs.save(str(MODELS_DIR / 'w2v_sg_hs.model'))\n",
          "print(\"ðŸ’¾ Model saved!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Test nearest neighbors\n",
          "print(\"\\nðŸ” Nearest neighbors (Skip-gram HS):\")\n",
          "\n",
          "for word in test_words:\n",
          "    if word in w2v_sg_hs.wv:\n",
          "        similar = w2v_sg_hs.wv.most_similar(word, topn=5)\n",
          "        print(f\"\\n  {word}:\")\n",
          "        for sim_word, score in similar:\n",
          "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
          "    else:\n",
          "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create document vectors\n",
          "print(\"\\nðŸ“Š Creating document vectors with TF-IDF weighted pooling...\")\n",
          "\n",
          "X_train_w2v_sg_hs = tfidf_weighted_pooling(\n",
          "    train_sentences, w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_dev_w2v_sg_hs = tfidf_weighted_pooling(\n",
          "    dev_sentences, w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_test_w2v_sg_hs = tfidf_weighted_pooling(\n",
          "    test_sentences, w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "print(f\"\\nâœ… Document vectors created:\")\n",
          "print(f\"   TRAIN: {X_train_w2v_sg_hs.shape}\")\n",
          "print(f\"   DEV: {X_dev_w2v_sg_hs.shape}\")\n",
          "print(f\"   TEST: {X_test_w2v_sg_hs.shape}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save document vectors\n",
          "np.save(MODELS_DIR / 'X_train_w2v_sg_hs.npy', X_train_w2v_sg_hs)\n",
          "np.save(MODELS_DIR / 'X_dev_w2v_sg_hs.npy', X_dev_w2v_sg_hs)\n",
          "np.save(MODELS_DIR / 'X_test_w2v_sg_hs.npy', X_test_w2v_sg_hs)\n",
          "print(\"\\nðŸ’¾ Document vectors saved!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 6. Word2Vec - CBOW with Hierarchical Softmax (HS)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ”§ Training Word2Vec CBOW with Hierarchical Softmax...\\n\")\n",
          "\n",
          "start_time = time.time()\n",
          "\n",
          "w2v_cbow_hs = Word2Vec(\n",
          "    sentences=train_sentences,\n",
          "    sg=0,  # CBOW\n",
          "    vector_size=100,\n",
          "    window=5,\n",
          "    min_count=3,\n",
          "    negative=0,\n",
          "    hs=1,  # Hierarchical softmax\n",
          "    epochs=10,\n",
          "    workers=4,\n",
          "    seed=42\n",
          ")\n",
          "\n",
          "train_time_cbow_hs = time.time() - start_time\n",
          "tokens_per_sec_cbow_hs = total_tokens_train / train_time_cbow_hs\n",
          "\n",
          "print(f\"âœ… Training complete in {train_time_cbow_hs:.2f}s\")\n",
          "print(f\"   Tokens/sec: {tokens_per_sec_cbow_hs:,.0f}\")\n",
          "print(f\"   Vocabulary size: {len(w2v_cbow_hs.wv):,}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save model\n",
          "w2v_cbow_hs.save(str(MODELS_DIR / 'w2v_cbow_hs.model'))\n",
          "print(\"ðŸ’¾ Model saved!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Test nearest neighbors\n",
          "print(\"\\nðŸ” Nearest neighbors (CBOW HS):\")\n",
          "\n",
          "for word in test_words:\n",
          "    if word in w2v_cbow_hs.wv:\n",
          "        similar = w2v_cbow_hs.wv.most_similar(word, topn=5)\n",
          "        print(f\"\\n  {word}:\")\n",
          "        for sim_word, score in similar:\n",
          "            print(f\"    {sim_word:15s} : {score:.4f}\")\n",
          "    else:\n",
          "        print(f\"\\n  {word}: NOT IN VOCABULARY\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create document vectors\n",
          "print(\"\\nðŸ“Š Creating document vectors with TF-IDF weighted pooling...\")\n",
          "\n",
          "X_train_w2v_cbow_hs = tfidf_weighted_pooling(\n",
          "    train_sentences, w2v_cbow_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_dev_w2v_cbow_hs = tfidf_weighted_pooling(\n",
          "    dev_sentences, w2v_cbow_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_test_w2v_cbow_hs = tfidf_weighted_pooling(\n",
          "    test_sentences, w2v_cbow_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "print(f\"\\nâœ… Document vectors created:\")\n",
          "print(f\"   TRAIN: {X_train_w2v_cbow_hs.shape}\")\n",
          "print(f\"   DEV: {X_dev_w2v_cbow_hs.shape}\")\n",
          "print(f\"   TEST: {X_test_w2v_cbow_hs.shape}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save document vectors\n",
          "np.save(MODELS_DIR / 'X_train_w2v_cbow_hs.npy', X_train_w2v_cbow_hs)\n",
          "np.save(MODELS_DIR / 'X_dev_w2v_cbow_hs.npy', X_dev_w2v_cbow_hs)\n",
          "np.save(MODELS_DIR / 'X_test_w2v_cbow_hs.npy', X_test_w2v_cbow_hs)\n",
          "print(\"\\nðŸ’¾ Document vectors saved!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 7. GloVe Embeddings (Pretrained)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“¥ Loading GloVe embeddings...\\n\")\n",
          "\n",
          "glove_path = DATA_DIR / 'glove.6B.100d.txt'\n",
          "\n",
          "if not glove_path.exists():\n",
          "    print(\"âŒ ERROR: GloVe file not found!\")\n",
          "    print(f\"   Expected location: {glove_path}\")\n",
          "    print(\"\\n   Please download from: http://nlp.stanford.edu/data/glove.6B.zip\")\n",
          "    print(\"   Extract glove.6B.100d.txt to the data/ folder\")\n",
          "    raise FileNotFoundError(f\"GloVe embeddings not found at {glove_path}\")\n",
          "\n",
          "# Load GloVe embeddings into dictionary\n",
          "glove_embeddings = {}\n",
          "\n",
          "with open(glove_path, 'r', encoding='utf-8') as f:\n",
          "    for line in tqdm(f, desc=\"Loading GloVe\", total=400000):\n",
          "        values = line.split()\n",
          "        word = values[0]\n",
          "        vector = np.asarray(values[1:], dtype='float32')\n",
          "        glove_embeddings[word] = vector\n",
          "\n",
          "print(f\"\\nâœ… Loaded {len(glove_embeddings):,} word vectors\")\n",
          "print(f\"   Vector dimension: {len(next(iter(glove_embeddings.values())))}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Test nearest neighbors for GloVe\n",
          "print(\"\\nðŸ” Testing GloVe embeddings:\")\n",
          "\n",
          "def get_most_similar_glove(word, embeddings, topn=5):\n",
          "    \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
          "    if word not in embeddings:\n",
          "        return None\n",
          "    \n",
          "    word_vec = embeddings[word]\n",
          "    similarities = {}\n",
          "    \n",
          "    # Calculate similarities with a subset for speed\n",
          "    for other_word, other_vec in list(embeddings.items())[:50000]:\n",
          "        if other_word == word:\n",
          "            continue\n",
          "        sim = np.dot(word_vec, other_vec) / (np.linalg.norm(word_vec) * np.linalg.norm(other_vec))\n",
          "        similarities[other_word] = sim\n",
          "    \n",
          "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:topn]\n",
          "\n",
          "for word in test_words:\n",
          "    if word in glove_embeddings:\n",
          "        similar = get_most_similar_glove(word, glove_embeddings, topn=5)\n",
          "        print(f\"\\n  {word}:\")\n",
          "        if similar:\n",
          "            for sim_word, score in similar:\n",
          "                print(f\"    {sim_word:15s} : {score:.4f}\")\n",
          "    else:\n",
          "        print(f\"\\n  {word}: NOT IN GLOVE VOCABULARY\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Create document vectors using GloVe\n",
          "print(\"\\nðŸ“Š Creating document vectors with GloVe + TF-IDF pooling...\")\n",
          "\n",
          "X_train_glove = tfidf_weighted_pooling(\n",
          "    train_sentences, glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_dev_glove = tfidf_weighted_pooling(\n",
          "    dev_sentences, glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "X_test_glove = tfidf_weighted_pooling(\n",
          "    test_sentences, glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_\n",
          ")\n",
          "\n",
          "print(f\"\\nâœ… Document vectors created:\")\n",
          "print(f\"   TRAIN: {X_train_glove.shape}\")\n",
          "print(f\"   DEV: {X_dev_glove.shape}\")\n",
          "print(f\"   TEST: {X_test_glove.shape}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save GloVe document vectors\n",
          "np.save(MODELS_DIR / 'X_train_glove.npy', X_train_glove)\n",
          "np.save(MODELS_DIR / 'X_dev_glove.npy', X_dev_glove)\n",
          "np.save(MODELS_DIR / 'X_test_glove.npy', X_test_glove)\n",
          "print(\"\\nðŸ’¾ GloVe document vectors saved!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 8. Calculate Health Metrics for Dense Methods"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“Š Calculating health metrics for dense representations...\\n\")\n",
          "\n",
          "# Calculate OOV rates\n",
          "def calculate_oov_rate(tokens_list, vocab):\n",
          "    \"\"\"Calculate out-of-vocabulary rate.\"\"\"\n",
          "    total_tokens = 0\n",
          "    oov_tokens = 0\n",
          "    \n",
          "    for tokens in tokens_list:\n",
          "        for token in tokens:\n",
          "            total_tokens += 1\n",
          "            if token not in vocab:\n",
          "                oov_tokens += 1\n",
          "    \n",
          "    return (oov_tokens / total_tokens * 100) if total_tokens > 0 else 0.0\n",
          "\n",
          "# Calculate coverage\n",
          "def calculate_coverage(tokens_list, vocab, k):\n",
          "    \"\"\"Calculate top-k coverage.\"\"\"\n",
          "    token_counts = Counter()\n",
          "    for tokens in tokens_list:\n",
          "        token_counts.update(tokens)\n",
          "    \n",
          "    top_k_tokens = [token for token, _ in token_counts.most_common(k)]\n",
          "    covered = sum(1 for token in top_k_tokens if token in vocab)\n",
          "    \n",
          "    return (covered / k * 100) if k > 0 else 0.0\n",
          "\n",
          "print(\"âœ… Helper functions defined!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec Skip-gram NS metrics\n",
          "oov_sg_ns = calculate_oov_rate(test_sentences, w2v_sg_ns.wv)\n",
          "cov100_sg_ns = calculate_coverage(test_sentences, w2v_sg_ns.wv, 100)\n",
          "cov500_sg_ns = calculate_coverage(test_sentences, w2v_sg_ns.wv, 500)\n",
          "\n",
          "print(\"Word2Vec Skip-gram NS:\")\n",
          "print(f\"  Vocabulary size: {len(w2v_sg_ns.wv):,}\")\n",
          "print(f\"  OOV rate (TEST): {oov_sg_ns:.2f}%\")\n",
          "print(f\"  Top-100 coverage: {cov100_sg_ns:.2f}%\")\n",
          "print(f\"  Top-500 coverage: {cov500_sg_ns:.2f}%\")\n",
          "print(f\"  Training time: {train_time_sg_ns:.2f}s\")\n",
          "print(f\"  Tokens/sec: {tokens_per_sec_sg_ns:,.0f}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec CBOW NS metrics\n",
          "oov_cbow_ns = calculate_oov_rate(test_sentences, w2v_cbow_ns.wv)\n",
          "cov100_cbow_ns = calculate_coverage(test_sentences, w2v_cbow_ns.wv, 100)\n",
          "cov500_cbow_ns = calculate_coverage(test_sentences, w2v_cbow_ns.wv, 500)\n",
          "\n",
          "print(\"\\nWord2Vec CBOW NS:\")\n",
          "print(f\"  Vocabulary size: {len(w2v_cbow_ns.wv):,}\")\n",
          "print(f\"  OOV rate (TEST): {oov_cbow_ns:.2f}%\")\n",
          "print(f\"  Top-100 coverage: {cov100_cbow_ns:.2f}%\")\n",
          "print(f\"  Top-500 coverage: {cov500_cbow_ns:.2f}%\")\n",
          "print(f\"  Training time: {train_time_cbow_ns:.2f}s\")\n",
          "print(f\"  Tokens/sec: {tokens_per_sec_cbow_ns:,.0f}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec Skip-gram HS metrics\n",
          "oov_sg_hs = calculate_oov_rate(test_sentences, w2v_sg_hs.wv)\n",
          "cov100_sg_hs = calculate_coverage(test_sentences, w2v_sg_hs.wv, 100)\n",
          "cov500_sg_hs = calculate_coverage(test_sentences, w2v_sg_hs.wv, 500)\n",
          "\n",
          "print(\"\\nWord2Vec Skip-gram HS:\")\n",
          "print(f\"  Vocabulary size: {len(w2v_sg_hs.wv):,}\")\n",
          "print(f\"  OOV rate (TEST): {oov_sg_hs:.2f}%\")\n",
          "print(f\"  Top-100 coverage: {cov100_sg_hs:.2f}%\")\n",
          "print(f\"  Top-500 coverage: {cov500_sg_hs:.2f}%\")\n",
          "print(f\"  Training time: {train_time_sg_hs:.2f}s\")\n",
          "print(f\"  Tokens/sec: {tokens_per_sec_sg_hs:,.0f}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec CBOW HS metrics\n",
          "oov_cbow_hs = calculate_oov_rate(test_sentences, w2v_cbow_hs.wv)\n",
          "cov100_cbow_hs = calculate_coverage(test_sentences, w2v_cbow_hs.wv, 100)\n",
          "cov500_cbow_hs = calculate_coverage(test_sentences, w2v_cbow_hs.wv, 500)\n",
          "\n",
          "print(\"\\nWord2Vec CBOW HS:\")\n",
          "print(f\"  Vocabulary size: {len(w2v_cbow_hs.wv):,}\")\n",
          "print(f\"  OOV rate (TEST): {oov_cbow_hs:.2f}%\")\n",
          "print(f\"  Top-100 coverage: {cov100_cbow_hs:.2f}%\")\n",
          "print(f\"  Top-500 coverage: {cov500_cbow_hs:.2f}%\")\n",
          "print(f\"  Training time: {train_time_cbow_hs:.2f}s\")\n",
          "print(f\"  Tokens/sec: {tokens_per_sec_cbow_hs:,.0f}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# GloVe metrics\n",
          "oov_glove = calculate_oov_rate(test_sentences, glove_embeddings)\n",
          "cov100_glove = calculate_coverage(test_sentences, glove_embeddings, 100)\n",
          "cov500_glove = calculate_coverage(test_sentences, glove_embeddings, 500)\n",
          "\n",
          "print(\"\\nGloVe (pretrained):\")\n",
          "print(f\"  Vocabulary size: {len(glove_embeddings):,}\")\n",
          "print(f\"  OOV rate (TEST): {oov_glove:.2f}%\")\n",
          "print(f\"  Top-100 coverage: {cov100_glove:.2f}%\")\n",
          "print(f\"  Top-500 coverage: {cov500_glove:.2f}%\")\n",
          "print(f\"  Training time: 0.00s (pretrained)\")\n",
          "print(f\"  Tokens/sec: N/A (pretrained)\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 9. Save Results Summary"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "import json\n",
          "\n",
          "# Compile dense methods results\n",
          "dense_results = {\n",
          "    'w2v_sg_ns': {\n",
          "        'vocab_size': len(w2v_sg_ns.wv),\n",
          "        'vector_dim': w2v_sg_ns.wv.vector_size,\n",
          "        'oov_rate': float(oov_sg_ns),\n",
          "        'top100_coverage': float(cov100_sg_ns),\n",
          "        'top500_coverage': float(cov500_sg_ns),\n",
          "        'train_time_sec': float(train_time_sg_ns),\n",
          "        'tokens_per_sec': float(tokens_per_sec_sg_ns),\n",
          "        'train_shape': list(X_train_w2v_sg_ns.shape),\n",
          "        'dev_shape': list(X_dev_w2v_sg_ns.shape),\n",
          "        'test_shape': list(X_test_w2v_sg_ns.shape)\n",
          "    },\n",
          "    'w2v_cbow_ns': {\n",
          "        'vocab_size': len(w2v_cbow_ns.wv),\n",
          "        'vector_dim': w2v_cbow_ns.wv.vector_size,\n",
          "        'oov_rate': float(oov_cbow_ns),\n",
          "        'top100_coverage': float(cov100_cbow_ns),\n",
          "        'top500_coverage': float(cov500_cbow_ns),\n",
          "        'train_time_sec': float(train_time_cbow_ns),\n",
          "        'tokens_per_sec': float(tokens_per_sec_cbow_ns),\n",
          "        'train_shape': list(X_train_w2v_cbow_ns.shape),\n",
          "        'dev_shape': list(X_dev_w2v_cbow_ns.shape),\n",
          "        'test_shape': list(X_test_w2v_cbow_ns.shape)\n",
          "    },\n",
          "    'w2v_sg_hs': {\n",
          "        'vocab_size': len(w2v_sg_hs.wv),\n",
          "        'vector_dim': w2v_sg_hs.wv.vector_size,\n",
          "        'oov_rate': float(oov_sg_hs),\n",
          "        'top100_coverage': float(cov100_sg_hs),\n",
          "        'top500_coverage': float(cov500_sg_hs),\n",
          "        'train_time_sec': float(train_time_sg_hs),\n",
          "        'tokens_per_sec': float(tokens_per_sec_sg_hs),\n",
          "        'train_shape': list(X_train_w2v_sg_hs.shape),\n",
          "        'dev_shape': list(X_dev_w2v_sg_hs.shape),\n",
          "        'test_shape': list(X_test_w2v_sg_hs.shape)\n",
          "    },\n",
          "    'w2v_cbow_hs': {\n",
          "        'vocab_size': len(w2v_cbow_hs.wv),\n",
          "        'vector_dim': w2v_cbow_hs.wv.vector_size,\n",
          "        'oov_rate': float(oov_cbow_hs),\n",
          "        'top100_coverage': float(cov100_cbow_hs),\n",
          "        'top500_coverage': float(cov500_cbow_hs),\n",
          "        'train_time_sec': float(train_time_cbow_hs),\n",
          "        'tokens_per_sec': float(tokens_per_sec_cbow_hs),\n",
          "        'train_shape': list(X_train_w2v_cbow_hs.shape),\n",
          "        'dev_shape': list(X_dev_w2v_cbow_hs.shape),\n",
          "        'test_shape': list(X_test_w2v_cbow_hs.shape)\n",
          "    },\n",
          "    'glove': {\n",
          "        'vocab_size': len(glove_embeddings),\n",
          "        'vector_dim': 100,\n",
          "        'oov_rate': float(oov_glove),\n",
          "        'top100_coverage': float(cov100_glove),\n",
          "        'top500_coverage': float(cov500_glove),\n",
          "        'train_time_sec': 0.0,\n",
          "        'tokens_per_sec': None,\n",
          "        'train_shape': list(X_train_glove.shape),\n",
          "        'dev_shape': list(X_dev_glove.shape),\n",
          "        'test_shape': list(X_test_glove.shape)\n",
          "    }\n",
          "}\n",
          "\n",
          "# Save results\n",
          "with open(CACHE_DIR / 'dense_results.json', 'w') as f:\n",
          "    json.dump(dense_results, f, indent=2)\n",
          "\n",
          "print(\"\\nðŸ’¾ Dense methods results saved to cache/dense_results.json\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 10. Summary Comparison"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\n\" + \"=\"*80)\n",
          "print(\"DENSE METHODS SUMMARY\")\n",
          "print(\"=\"*80)\n",
          "\n",
          "print(f\"\\n{'Method':<20} {'Vocab':<10} {'OOV%':<8} {'Top100%':<9} {'Top500%':<9} {'Time(s)':<10} {'Tok/s':<12}\")\n",
          "print(\"-\"*80)\n",
          "\n",
          "for method_name, metrics in dense_results.items():\n",
          "    tok_s = f\"{metrics['tokens_per_sec']:,.0f}\" if metrics['tokens_per_sec'] else \"N/A\"\n",
          "    print(f\"{method_name:<20} {metrics['vocab_size']:<10,} {metrics['oov_rate']:<8.2f} \"\n",
          "          f\"{metrics['top100_coverage']:<9.2f} {metrics['top500_coverage']:<9.2f} \"\n",
          "          f\"{metrics['train_time_sec']:<10.2f} {tok_s:<12}\")\n",
          "\n",
          "print(\"\\n\" + \"=\"*80)\n",
          "print(\"âœ… Dense methods training complete!\")\n",
          "print(\"=\"*80)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸŽ‰ Notebook 03: Dense Methods - COMPLETE!\")\n",
          "print(\"\\nNext steps:\")\n",
          "print(\"  1. Run notebook 04: Classification\")\n",
          "print(\"  2. Train classifiers on all representations\")\n",
          "print(\"  3. Compare performance on TEST set\")"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8.0"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }