{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# BBC Text Representations - Classification\n",
          "\n",
          "**Roll Number:** SE22UARI195\n",
          "\n",
          "**Tasks:**\n",
          "1. Load all 9 representations (4 sparse + 5 dense)\n",
          "2. Train Logistic Regression with C tuning on DEV\n",
          "3. Evaluate on TEST: Macro-F1 (primary) and Accuracy\n",
          "4. Generate preds_test.csv for best representation\n",
          "5. Compare all methods\n",
          "\n",
          "---"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 1. Setup & Imports"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Core libraries\n",
          "import pandas as pd\n",
          "import numpy as np\n",
          "import pickle\n",
          "import json\n",
          "from pathlib import Path\n",
          "import time\n",
          "\n",
          "# Scikit-learn\n",
          "from sklearn.linear_model import LogisticRegression\n",
          "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
          "\n",
          "# Progress bar\n",
          "from tqdm.notebook import tqdm\n",
          "\n",
          "# Warnings\n",
          "import warnings\n",
          "warnings.filterwarnings('ignore')\n",
          "\n",
          "print(\"âœ… Imports successful!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Configuration\n",
          "ROLL = \"SE22UARI195\"\n",
          "CACHE_DIR = Path(\"../cache\")\n",
          "MODELS_DIR = Path(\"../models\")\n",
          "OUTPUTS_DIR = Path(\"../outputs\")\n",
          "\n",
          "# Create outputs directory\n",
          "OUTPUTS_DIR.mkdir(exist_ok=True)\n",
          "\n",
          "print(f\"Roll Number: {ROLL}\")\n",
          "print(f\"Cache Directory: {CACHE_DIR}\")\n",
          "print(f\"Models Directory: {MODELS_DIR}\")\n",
          "print(f\"Outputs Directory: {OUTPUTS_DIR}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 2. Load Preprocessed Data & Labels"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"ðŸ“‚ Loading preprocessed data...\\n\")\n",
          "\n",
          "with open(CACHE_DIR / 'train_processed.pkl', 'rb') as f:\n",
          "    train_df = pickle.load(f)\n",
          "\n",
          "with open(CACHE_DIR / 'dev_processed.pkl', 'rb') as f:\n",
          "    dev_df = pickle.load(f)\n",
          "\n",
          "with open(CACHE_DIR / 'test_processed.pkl', 'rb') as f:\n",
          "    test_df = pickle.load(f)\n",
          "\n",
          "# Extract labels\n",
          "y_train = train_df['label'].values\n",
          "y_dev = dev_df['label'].values\n",
          "y_test = test_df['label'].values\n",
          "\n",
          "print(f\"âœ… TRAIN: {len(train_df)} documents, {len(set(y_train))} classes\")\n",
          "print(f\"âœ… DEV: {len(dev_df)} documents\")\n",
          "print(f\"âœ… TEST: {len(test_df)} documents\")\n",
          "print(f\"\\nClasses: {sorted(set(y_train))}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 3. Load All Representations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“Š Loading sparse representations...\\n\")\n",
          "\n",
          "# Load sparse matrices\n",
          "from scipy.sparse import load_npz\n",
          "\n",
          "representations = {}\n",
          "\n",
          "# One-Hot Encoding\n",
          "representations['ohe'] = {\n",
          "    'train': load_npz(MODELS_DIR / 'X_train_ohe.npz'),\n",
          "    'dev': load_npz(MODELS_DIR / 'X_dev_ohe.npz'),\n",
          "    'test': load_npz(MODELS_DIR / 'X_test_ohe.npz')\n",
          "}\n",
          "print(f\"âœ… OHE: {representations['ohe']['train'].shape}\")\n",
          "\n",
          "# Bag-of-Words\n",
          "representations['bow'] = {\n",
          "    'train': load_npz(MODELS_DIR / 'X_train_bow.npz'),\n",
          "    'dev': load_npz(MODELS_DIR / 'X_dev_bow.npz'),\n",
          "    'test': load_npz(MODELS_DIR / 'X_test_bow.npz')\n",
          "}\n",
          "print(f\"âœ… BOW: {representations['bow']['train'].shape}\")\n",
          "\n",
          "# N-grams\n",
          "representations['ngram'] = {\n",
          "    'train': load_npz(MODELS_DIR / 'X_train_ngram.npz'),\n",
          "    'dev': load_npz(MODELS_DIR / 'X_dev_ngram.npz'),\n",
          "    'test': load_npz(MODELS_DIR / 'X_test_ngram.npz')\n",
          "}\n",
          "print(f\"âœ… N-grams: {representations['ngram']['train'].shape}\")\n",
          "\n",
          "# TF-IDF\n",
          "representations['tfidf'] = {\n",
          "    'train': load_npz(MODELS_DIR / 'X_train_tfidf.npz'),\n",
          "    'dev': load_npz(MODELS_DIR / 'X_dev_tfidf.npz'),\n",
          "    'test': load_npz(MODELS_DIR / 'X_test_tfidf.npz')\n",
          "}\n",
          "print(f\"âœ… TF-IDF: {representations['tfidf']['train'].shape}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“Š Loading dense representations...\\n\")\n",
          "\n",
          "# Word2Vec Skip-gram NS\n",
          "representations['w2v_sg_ns'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_w2v_sg_ns.npy'),\n",
          "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_sg_ns.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_w2v_sg_ns.npy')\n",
          "}\n",
          "print(f\"âœ… W2V Skip-gram NS: {representations['w2v_sg_ns']['train'].shape}\")\n",
          "\n",
          "# Word2Vec CBOW NS\n",
          "representations['w2v_cbow_ns'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_w2v_cbow_ns.npy'),\n",
          "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_cbow_ns.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_w2v_cbow_ns.npy')\n",
          "}\n",
          "print(f\"âœ… W2V CBOW NS: {representations['w2v_cbow_ns']['train'].shape}\")\n",
          "\n",
          "# Word2Vec Skip-gram HS\n",
          "representations['w2v_sg_hs'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_w2v_sg_hs.npy'),\n",
          "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_sg_hs.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_w2v_sg_hs.npy')\n",
          "}\n",
          "print(f\"âœ… W2V Skip-gram HS: {representations['w2v_sg_hs']['train'].shape}\")\n",
          "\n",
          "# Word2Vec CBOW HS\n",
          "representations['w2v_cbow_hs'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_w2v_cbow_hs.npy'),\n",
          "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_cbow_hs.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_w2v_cbow_hs.npy')\n",
          "}\n",
          "print(f\"âœ… W2V CBOW HS: {representations['w2v_cbow_hs']['train'].shape}\")\n",
          "\n",
          "# GloVe\n",
          "representations['glove'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_glove.npy'),\n",
          "    'dev': np.load(MODELS_DIR / 'X_dev_glove.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_glove.npy')\n",
          "}\n",
          "print(f\"âœ… GloVe: {representations['glove']['train'].shape}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 4. Classification Helper Functions"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def train_and_evaluate(X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
          "                       method_name, C_values=[0.01, 0.1, 1.0, 10.0, 100.0]):\n",
          "    \"\"\"\n",
          "    Train Logistic Regression with hyperparameter tuning on DEV.\n",
          "    Report best performance on TEST.\n",
          "    \n",
          "    Returns:\n",
          "        dict with best_C, macro_f1, accuracy, predictions\n",
          "    \"\"\"\n",
          "    print(f\"\\n{'='*60}\")\n",
          "    print(f\"Training: {method_name}\")\n",
          "    print(f\"{'='*60}\")\n",
          "    \n",
          "    best_C = None\n",
          "    best_f1_dev = -1\n",
          "    best_model = None\n",
          "    \n",
          "    # Tune C on DEV set\n",
          "    print(f\"\\nðŸ” Tuning hyperparameter C on DEV set...\")\n",
          "    for C in C_values:\n",
          "        clf = LogisticRegression(\n",
          "            C=C,\n",
          "            max_iter=1000,\n",
          "            random_state=42,\n",
          "            solver='lbfgs',\n",
          "            multi_class='multinomial'\n",
          "        )\n",
          "        \n",
          "        clf.fit(X_train, y_train)\n",
          "        y_pred_dev = clf.predict(X_dev)\n",
          "        \n",
          "        f1_dev = f1_score(y_dev, y_pred_dev, average='macro')\n",
          "        acc_dev = accuracy_score(y_dev, y_pred_dev)\n",
          "        \n",
          "        print(f\"  C={C:7.2f} -> DEV Macro-F1: {f1_dev:.4f}, Accuracy: {acc_dev:.4f}\")\n",
          "        \n",
          "        if f1_dev > best_f1_dev:\n",
          "            best_f1_dev = f1_dev\n",
          "            best_C = C\n",
          "            best_model = clf\n",
          "    \n",
          "    print(f\"\\nâœ… Best C: {best_C} (DEV Macro-F1: {best_f1_dev:.4f})\")\n",
          "    \n",
          "    # Evaluate on TEST set\n",
          "    print(f\"\\nðŸ“Š Evaluating on TEST set...\")\n",
          "    y_pred_test = best_model.predict(X_test)\n",
          "    \n",
          "    f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
          "    acc_test = accuracy_score(y_test, y_pred_test)\n",
          "    \n",
          "    print(f\"\\nðŸŽ¯ TEST Results:\")\n",
          "    print(f\"   Macro-F1: {f1_test:.4f}\")\n",
          "    print(f\"   Accuracy: {acc_test:.4f}\")\n",
          "    \n",
          "    # Per-class results\n",
          "    print(f\"\\nðŸ“‹ Classification Report:\")\n",
          "    print(classification_report(y_test, y_pred_test, digits=4))\n",
          "    \n",
          "    return {\n",
          "        'method': method_name,\n",
          "        'best_C': best_C,\n",
          "        'macro_f1': float(f1_test),\n",
          "        'accuracy': float(acc_test),\n",
          "        'predictions': y_pred_test\n",
          "    }\n",
          "\n",
          "print(\"âœ… Helper functions defined!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 5. Train Classifiers on All Representations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\n\" + \"=\"*80)\n",
          "print(\"TRAINING CLASSIFIERS ON ALL REPRESENTATIONS\")\n",
          "print(\"=\"*80)\n",
          "\n",
          "results_classification = {}"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### 5.1 Sparse Methods"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# One-Hot Encoding\n",
          "result = train_and_evaluate(\n",
          "    representations['ohe']['train'], y_train,\n",
          "    representations['ohe']['dev'], y_dev,\n",
          "    representations['ohe']['test'], y_test,\n",
          "    method_name='One-Hot Encoding (OHE)'\n",
          ")\n",
          "results_classification['ohe'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Bag-of-Words\n",
          "result = train_and_evaluate(\n",
          "    representations['bow']['train'], y_train,\n",
          "    representations['bow']['dev'], y_dev,\n",
          "    representations['bow']['test'], y_test,\n",
          "    method_name='Bag-of-Words (BOW)'\n",
          ")\n",
          "results_classification['bow'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# N-grams\n",
          "result = train_and_evaluate(\n",
          "    representations['ngram']['train'], y_train,\n",
          "    representations['ngram']['dev'], y_dev,\n",
          "    representations['ngram']['test'], y_test,\n",
          "    method_name='N-grams (1,2)'\n",
          ")\n",
          "results_classification['ngram'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# TF-IDF (save predictions for submission)\n",
          "result = train_and_evaluate(\n",
          "    representations['tfidf']['train'], y_train,\n",
          "    representations['tfidf']['dev'], y_dev,\n",
          "    representations['tfidf']['test'], y_test,\n",
          "    method_name='TF-IDF'\n",
          ")\n",
          "results_classification['tfidf'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}\n",
          "\n",
          "# Save TF-IDF predictions\n",
          "tfidf_predictions = result['predictions']"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### 5.2 Dense Methods (TF-IDF Weighted)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec Skip-gram NS\n",
          "result = train_and_evaluate(\n",
          "    representations['w2v_sg_ns']['train'], y_train,\n",
          "    representations['w2v_sg_ns']['dev'], y_dev,\n",
          "    representations['w2v_sg_ns']['test'], y_test,\n",
          "    method_name='Word2Vec Skip-gram NS + TF-IDF'\n",
          ")\n",
          "results_classification['w2v_ns_tfidf'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec CBOW NS (optional - for comparison)\n",
          "result = train_and_evaluate(\n",
          "    representations['w2v_cbow_ns']['train'], y_train,\n",
          "    representations['w2v_cbow_ns']['dev'], y_dev,\n",
          "    representations['w2v_cbow_ns']['test'], y_test,\n",
          "    method_name='Word2Vec CBOW NS + TF-IDF'\n",
          ")\n",
          "# Note: Not required in results.json but good for comparison"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec Skip-gram HS\n",
          "result = train_and_evaluate(\n",
          "    representations['w2v_sg_hs']['train'], y_train,\n",
          "    representations['w2v_sg_hs']['dev'], y_dev,\n",
          "    representations['w2v_sg_hs']['test'], y_test,\n",
          "    method_name='Word2Vec Skip-gram HS + TF-IDF'\n",
          ")\n",
          "results_classification['w2v_hs_tfidf'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Word2Vec CBOW HS (optional - for comparison)\n",
          "result = train_and_evaluate(\n",
          "    representations['w2v_cbow_hs']['train'], y_train,\n",
          "    representations['w2v_cbow_hs']['dev'], y_dev,\n",
          "    representations['w2v_cbow_hs']['test'], y_test,\n",
          "    method_name='Word2Vec CBOW HS + TF-IDF'\n",
          ")\n",
          "# Note: Not required in results.json but good for comparison"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# GloVe\n",
          "result = train_and_evaluate(\n",
          "    representations['glove']['train'], y_train,\n",
          "    representations['glove']['dev'], y_dev,\n",
          "    representations['glove']['test'], y_test,\n",
          "    method_name='GloVe + TF-IDF'\n",
          ")\n",
          "results_classification['glove_tfidf'] = {\n",
          "    'macro_f1': result['macro_f1'],\n",
          "    'accuracy': result['accuracy']\n",
          "}"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 6. Summary Comparison"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\n\" + \"=\"*80)\n",
          "print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
          "print(\"=\"*80)\n",
          "\n",
          "print(f\"\\n{'Method':<30} {'Macro-F1':<12} {'Accuracy':<12}\")\n",
          "print(\"-\"*80)\n",
          "\n",
          "# Sort by Macro-F1\n",
          "sorted_results = sorted(results_classification.items(), \n",
          "                       key=lambda x: x[1]['macro_f1'], \n",
          "                       reverse=True)\n",
          "\n",
          "for method, metrics in sorted_results:\n",
          "    print(f\"{method:<30} {metrics['macro_f1']:<12.4f} {metrics['accuracy']:<12.4f}\")\n",
          "\n",
          "print(\"\\n\" + \"=\"*80)\n",
          "\n",
          "# Find best method\n",
          "best_method = sorted_results[0][0]\n",
          "best_f1 = sorted_results[0][1]['macro_f1']\n",
          "print(f\"\\nðŸ† Best Method: {best_method} (Macro-F1: {best_f1:.4f})\")\n",
          "print(\"=\"*80)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 7. Generate preds_test.csv"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ’¾ Generating preds_test.csv...\")\n",
          "\n",
          "# Create predictions DataFrame\n",
          "preds_df = pd.DataFrame({\n",
          "    'id': test_df['id'].values,\n",
          "    'pred': tfidf_predictions\n",
          "})\n",
          "\n",
          "# Save to outputs\n",
          "preds_df.to_csv(OUTPUTS_DIR / 'preds_test.csv', index=False)\n",
          "\n",
          "print(f\"âœ… Saved: {OUTPUTS_DIR / 'preds_test.csv'}\")\n",
          "print(f\"   Shape: {preds_df.shape}\")\n",
          "print(f\"\\nFirst 5 predictions:\")\n",
          "print(preds_df.head())"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 8. Save Classification Results"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Save classification results\n",
          "with open(CACHE_DIR / 'classification_results.json', 'w') as f:\n",
          "    json.dump(results_classification, f, indent=2)\n",
          "\n",
          "print(\"\\nðŸ’¾ Classification results saved to cache/classification_results.json\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸŽ‰ Notebook 04: Classification - COMPLETE!\")\n",
          "print(\"\\nNext steps:\")\n",
          "print(\"  1. Run notebook 05: Retrieval\")\n",
          "print(\"  2. Generate deterministic queries\")\n",
          "print(\"  3. Calculate MAP@5, Recall@10, Negation Top-1%\")\n",
          "print(\"  4. Merge all notebooks into final submission\")"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8.0"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }