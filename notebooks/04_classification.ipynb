{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BBC Text Representations - Classification\n",
        "\n",
        "**Roll Number:** SE22UARI195\n",
        "\n",
        "**Tasks:**\n",
        "1. Load all 9 representations (4 sparse + 5 dense)\n",
        "2. Train Logistic Regression with C tuning on DEV\n",
        "3. Evaluate on TEST: Macro-F1 (primary) and Accuracy\n",
        "4. Generate preds_test.csv for best representation\n",
        "5. Compare all methods\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Roll Number: SE22UARI195\n",
            "Cache Directory: ../cache\n",
            "Models Directory: ../models\n",
            "Outputs Directory: ../outputs\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "ROLL = \"SE22UARI195\"\n",
        "CACHE_DIR = Path(\"../cache\")\n",
        "MODELS_DIR = Path(\"../models\")\n",
        "OUTPUTS_DIR = Path(\"../outputs\")\n",
        "\n",
        "# Create outputs directory\n",
        "OUTPUTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Roll Number: {ROLL}\")\n",
        "print(f\"Cache Directory: {CACHE_DIR}\")\n",
        "print(f\"Models Directory: {MODELS_DIR}\")\n",
        "print(f\"Outputs Directory: {OUTPUTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Preprocessed Data & Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Loading preprocessed data...\n",
            "\n",
            "‚úÖ TRAIN: 1335 documents, 5 classes\n",
            "‚úÖ DEV: 445 documents\n",
            "‚úÖ TEST: 445 documents\n",
            "\n",
            "Classes: ['business', 'entertainment', 'politics', 'sport', 'tech']\n"
          ]
        }
      ],
      "source": [
        "print(\"üìÇ Loading preprocessed data...\\n\")\n",
        "\n",
        "with open(CACHE_DIR / 'train_processed.pkl', 'rb') as f:\n",
        "    train_df = pickle.load(f)\n",
        "\n",
        "with open(CACHE_DIR / 'dev_processed.pkl', 'rb') as f:\n",
        "    dev_df = pickle.load(f)\n",
        "\n",
        "with open(CACHE_DIR / 'test_processed.pkl', 'rb') as f:\n",
        "    test_df = pickle.load(f)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_df['label'].values\n",
        "y_dev = dev_df['label'].values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "print(f\"‚úÖ TRAIN: {len(train_df)} documents, {len(set(y_train))} classes\")\n",
        "print(f\"‚úÖ DEV: {len(dev_df)} documents\")\n",
        "print(f\"‚úÖ TEST: {len(test_df)} documents\")\n",
        "print(f\"\\nClasses: {sorted(set(y_train))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load All Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Loading sparse representations...\n",
            "\n",
            "‚úÖ OHE: (1335, 2000)\n",
            "‚úÖ BOW: (1335, 11515)\n",
            "‚úÖ N-grams: (1335, 18625)\n",
            "‚úÖ TF-IDF: (1335, 11515)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìä Loading sparse representations...\\n\")\n",
        "\n",
        "# Load sparse matrices\n",
        "from scipy.sparse import load_npz\n",
        "\n",
        "representations = {}\n",
        "\n",
        "# One-Hot Encoding\n",
        "representations['ohe'] = {\n",
        "    'train': load_npz(MODELS_DIR / 'X_train_ohe.npz'),\n",
        "    'dev': load_npz(MODELS_DIR / 'X_dev_ohe.npz'),\n",
        "    'test': load_npz(MODELS_DIR / 'X_test_ohe.npz')\n",
        "}\n",
        "print(f\"‚úÖ OHE: {representations['ohe']['train'].shape}\")\n",
        "\n",
        "# Bag-of-Words\n",
        "representations['bow'] = {\n",
        "    'train': load_npz(MODELS_DIR / 'X_train_bow.npz'),\n",
        "    'dev': load_npz(MODELS_DIR / 'X_dev_bow.npz'),\n",
        "    'test': load_npz(MODELS_DIR / 'X_test_bow.npz')\n",
        "}\n",
        "print(f\"‚úÖ BOW: {representations['bow']['train'].shape}\")\n",
        "\n",
        "# N-grams\n",
        "representations['ngram'] = {\n",
        "    'train': load_npz(MODELS_DIR / 'X_train_ngram.npz'),\n",
        "    'dev': load_npz(MODELS_DIR / 'X_dev_ngram.npz'),\n",
        "    'test': load_npz(MODELS_DIR / 'X_test_ngram.npz')\n",
        "}\n",
        "print(f\"‚úÖ N-grams: {representations['ngram']['train'].shape}\")\n",
        "\n",
        "# TF-IDF\n",
        "representations['tfidf'] = {\n",
        "    'train': load_npz(MODELS_DIR / 'X_train_tfidf.npz'),\n",
        "    'dev': load_npz(MODELS_DIR / 'X_dev_tfidf.npz'),\n",
        "    'test': load_npz(MODELS_DIR / 'X_test_tfidf.npz')\n",
        "}\n",
        "print(f\"‚úÖ TF-IDF: {representations['tfidf']['train'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Loading dense representations...\n",
            "\n",
            "‚úÖ W2V Skip-gram NS: (1335, 100)\n",
            "‚úÖ W2V CBOW NS: (1335, 100)\n",
            "‚úÖ W2V Skip-gram HS: (1335, 100)\n",
            "‚úÖ W2V CBOW HS: (1335, 100)\n",
            "‚úÖ GloVe: (1335, 100)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìä Loading dense representations...\\n\")\n",
        "\n",
        "# Word2Vec Skip-gram NS\n",
        "representations['w2v_sg_ns'] = {\n",
        "    'train': np.load(MODELS_DIR / 'X_train_w2v_sg_ns.npy'),\n",
        "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_sg_ns.npy'),\n",
        "    'test': np.load(MODELS_DIR / 'X_test_w2v_sg_ns.npy')\n",
        "}\n",
        "print(f\"‚úÖ W2V Skip-gram NS: {representations['w2v_sg_ns']['train'].shape}\")\n",
        "\n",
        "# Word2Vec CBOW NS\n",
        "representations['w2v_cbow_ns'] = {\n",
        "    'train': np.load(MODELS_DIR / 'X_train_w2v_cbow_ns.npy'),\n",
        "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_cbow_ns.npy'),\n",
        "    'test': np.load(MODELS_DIR / 'X_test_w2v_cbow_ns.npy')\n",
        "}\n",
        "print(f\"‚úÖ W2V CBOW NS: {representations['w2v_cbow_ns']['train'].shape}\")\n",
        "\n",
        "# Word2Vec Skip-gram HS\n",
        "representations['w2v_sg_hs'] = {\n",
        "    'train': np.load(MODELS_DIR / 'X_train_w2v_sg_hs.npy'),\n",
        "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_sg_hs.npy'),\n",
        "    'test': np.load(MODELS_DIR / 'X_test_w2v_sg_hs.npy')\n",
        "}\n",
        "print(f\"‚úÖ W2V Skip-gram HS: {representations['w2v_sg_hs']['train'].shape}\")\n",
        "\n",
        "# Word2Vec CBOW HS\n",
        "representations['w2v_cbow_hs'] = {\n",
        "    'train': np.load(MODELS_DIR / 'X_train_w2v_cbow_hs.npy'),\n",
        "    'dev': np.load(MODELS_DIR / 'X_dev_w2v_cbow_hs.npy'),\n",
        "    'test': np.load(MODELS_DIR / 'X_test_w2v_cbow_hs.npy')\n",
        "}\n",
        "print(f\"‚úÖ W2V CBOW HS: {representations['w2v_cbow_hs']['train'].shape}\")\n",
        "\n",
        "# GloVe\n",
        "representations['glove'] = {\n",
        "    'train': np.load(MODELS_DIR / 'X_train_glove.npy'),\n",
        "    'dev': np.load(MODELS_DIR / 'X_dev_glove.npy'),\n",
        "    'test': np.load(MODELS_DIR / 'X_test_glove.npy')\n",
        "}\n",
        "print(f\"‚úÖ GloVe: {representations['glove']['train'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Classification Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate(X_train, y_train, X_dev, y_dev, X_test, y_test, \n",
        "                       method_name, C_values=[0.01, 0.1, 1.0, 10.0, 100.0]):\n",
        "    \"\"\"\n",
        "    Train Logistic Regression with hyperparameter tuning on DEV.\n",
        "    Report best performance on TEST.\n",
        "    \n",
        "    Returns:\n",
        "        dict with best_C, macro_f1, accuracy, predictions\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {method_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    best_C = None\n",
        "    best_f1_dev = -1\n",
        "    best_model = None\n",
        "    \n",
        "    # Tune C on DEV set\n",
        "    print(f\"\\nüîç Tuning hyperparameter C on DEV set...\")\n",
        "    for C in C_values:\n",
        "        clf = LogisticRegression(\n",
        "            C=C,\n",
        "            max_iter=1000,\n",
        "            random_state=42,\n",
        "            solver='lbfgs',\n",
        "            multi_class='multinomial'\n",
        "        )\n",
        "        \n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred_dev = clf.predict(X_dev)\n",
        "        \n",
        "        f1_dev = f1_score(y_dev, y_pred_dev, average='macro')\n",
        "        acc_dev = accuracy_score(y_dev, y_pred_dev)\n",
        "        \n",
        "        print(f\"  C={C:7.2f} -> DEV Macro-F1: {f1_dev:.4f}, Accuracy: {acc_dev:.4f}\")\n",
        "        \n",
        "        if f1_dev > best_f1_dev:\n",
        "            best_f1_dev = f1_dev\n",
        "            best_C = C\n",
        "            best_model = clf\n",
        "    \n",
        "    print(f\"\\n‚úÖ Best C: {best_C} (DEV Macro-F1: {best_f1_dev:.4f})\")\n",
        "    \n",
        "    # Evaluate on TEST set\n",
        "    print(f\"\\nüìä Evaluating on TEST set...\")\n",
        "    y_pred_test = best_model.predict(X_test)\n",
        "    \n",
        "    f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
        "    acc_test = accuracy_score(y_test, y_pred_test)\n",
        "    \n",
        "    print(f\"\\nüéØ TEST Results:\")\n",
        "    print(f\"   Macro-F1: {f1_test:.4f}\")\n",
        "    print(f\"   Accuracy: {acc_test:.4f}\")\n",
        "    \n",
        "    # Per-class results\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_test, digits=4))\n",
        "    \n",
        "    return {\n",
        "        'method': method_name,\n",
        "        'best_C': best_C,\n",
        "        'macro_f1': float(f1_test),\n",
        "        'accuracy': float(acc_test),\n",
        "        'predictions': y_pred_test\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Classifiers on All Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING CLASSIFIERS ON ALL REPRESENTATIONS\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING CLASSIFIERS ON ALL REPRESENTATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_classification = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Sparse Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: One-Hot Encoding (OHE)\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9519, Accuracy: 0.9528\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9610, Accuracy: 0.9618\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9635, Accuracy: 0.9640\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9729, Accuracy: 0.9730\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9656, Accuracy: 0.9663\n",
            "\n",
            "‚úÖ Best C: 10.0 (DEV Macro-F1: 0.9729)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9654\n",
            "   Accuracy: 0.9663\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9519    0.9706    0.9612       102\n",
            "entertainment     0.9867    0.9610    0.9737        77\n",
            "     politics     0.9759    0.9643    0.9701        84\n",
            "        sport     0.9714    1.0000    0.9855       102\n",
            "         tech     0.9487    0.9250    0.9367        80\n",
            "\n",
            "     accuracy                         0.9663       445\n",
            "    macro avg     0.9669    0.9642    0.9654       445\n",
            " weighted avg     0.9664    0.9663    0.9662       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# One-Hot Encoding\n",
        "result = train_and_evaluate(\n",
        "    representations['ohe']['train'], y_train,\n",
        "    representations['ohe']['dev'], y_dev,\n",
        "    representations['ohe']['test'], y_test,\n",
        "    method_name='One-Hot Encoding (OHE)'\n",
        ")\n",
        "results_classification['ohe'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: Bag-of-Words (BOW)\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9612, Accuracy: 0.9618\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9636, Accuracy: 0.9640\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9638, Accuracy: 0.9640\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9611, Accuracy: 0.9618\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9591, Accuracy: 0.9596\n",
            "\n",
            "‚úÖ Best C: 1.0 (DEV Macro-F1: 0.9638)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9683\n",
            "   Accuracy: 0.9685\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9604    0.9510    0.9557       102\n",
            "entertainment     0.9870    0.9870    0.9870        77\n",
            "     politics     0.9759    0.9643    0.9701        84\n",
            "        sport     0.9714    1.0000    0.9855       102\n",
            "         tech     0.9494    0.9375    0.9434        80\n",
            "\n",
            "     accuracy                         0.9685       445\n",
            "    macro avg     0.9688    0.9680    0.9683       445\n",
            " weighted avg     0.9685    0.9685    0.9684       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Bag-of-Words\n",
        "result = train_and_evaluate(\n",
        "    representations['bow']['train'], y_train,\n",
        "    representations['bow']['dev'], y_dev,\n",
        "    representations['bow']['test'], y_test,\n",
        "    method_name='Bag-of-Words (BOW)'\n",
        ")\n",
        "results_classification['bow'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: N-grams (1,2)\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9633, Accuracy: 0.9640\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9636, Accuracy: 0.9640\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9635, Accuracy: 0.9640\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9611, Accuracy: 0.9618\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9611, Accuracy: 0.9618\n",
            "\n",
            "‚úÖ Best C: 0.1 (DEV Macro-F1: 0.9636)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9639\n",
            "   Accuracy: 0.9640\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9505    0.9412    0.9458       102\n",
            "entertainment     0.9870    0.9870    0.9870        77\n",
            "     politics     0.9643    0.9643    0.9643        84\n",
            "        sport     0.9714    1.0000    0.9855       102\n",
            "         tech     0.9487    0.9250    0.9367        80\n",
            "\n",
            "     accuracy                         0.9640       445\n",
            "    macro avg     0.9644    0.9635    0.9639       445\n",
            " weighted avg     0.9639    0.9640    0.9639       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# N-grams\n",
        "result = train_and_evaluate(\n",
        "    representations['ngram']['train'], y_train,\n",
        "    representations['ngram']['dev'], y_dev,\n",
        "    representations['ngram']['test'], y_test,\n",
        "    method_name='N-grams (1,2)'\n",
        ")\n",
        "results_classification['ngram'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: TF-IDF\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.2862, Accuracy: 0.4742\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9214, Accuracy: 0.9213\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9751, Accuracy: 0.9753\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9686, Accuracy: 0.9685\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9686, Accuracy: 0.9685\n",
            "\n",
            "‚úÖ Best C: 1.0 (DEV Macro-F1: 0.9751)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9639\n",
            "   Accuracy: 0.9640\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9505    0.9412    0.9458       102\n",
            "entertainment     0.9870    0.9870    0.9870        77\n",
            "     politics     0.9643    0.9643    0.9643        84\n",
            "        sport     0.9714    1.0000    0.9855       102\n",
            "         tech     0.9487    0.9250    0.9367        80\n",
            "\n",
            "     accuracy                         0.9640       445\n",
            "    macro avg     0.9644    0.9635    0.9639       445\n",
            " weighted avg     0.9639    0.9640    0.9639       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF (save predictions for submission)\n",
        "result = train_and_evaluate(\n",
        "    representations['tfidf']['train'], y_train,\n",
        "    representations['tfidf']['dev'], y_dev,\n",
        "    representations['tfidf']['test'], y_test,\n",
        "    method_name='TF-IDF'\n",
        ")\n",
        "results_classification['tfidf'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}\n",
        "\n",
        "# Save TF-IDF predictions\n",
        "tfidf_predictions = result['predictions']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Dense Methods (TF-IDF Weighted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: Word2Vec Skip-gram NS + TF-IDF\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9325, Accuracy: 0.9326\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9390, Accuracy: 0.9393\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9387, Accuracy: 0.9393\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9339, Accuracy: 0.9348\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9306, Accuracy: 0.9326\n",
            "\n",
            "‚úÖ Best C: 0.1 (DEV Macro-F1: 0.9390)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9306\n",
            "   Accuracy: 0.9326\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9293    0.9020    0.9154       102\n",
            "entertainment     0.9367    0.9610    0.9487        77\n",
            "     politics     0.9070    0.9286    0.9176        84\n",
            "        sport     0.9623    1.0000    0.9808       102\n",
            "         tech     0.9200    0.8625    0.8903        80\n",
            "\n",
            "     accuracy                         0.9326       445\n",
            "    macro avg     0.9310    0.9308    0.9306       445\n",
            " weighted avg     0.9323    0.9326    0.9321       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec Skip-gram NS\n",
        "result = train_and_evaluate(\n",
        "    representations['w2v_sg_ns']['train'], y_train,\n",
        "    representations['w2v_sg_ns']['dev'], y_dev,\n",
        "    representations['w2v_sg_ns']['test'], y_test,\n",
        "    method_name='Word2Vec Skip-gram NS + TF-IDF'\n",
        ")\n",
        "results_classification['w2v_ns_tfidf'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: Word2Vec CBOW NS + TF-IDF\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9016, Accuracy: 0.9011\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9186, Accuracy: 0.9191\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9299, Accuracy: 0.9303\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9322, Accuracy: 0.9326\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9309, Accuracy: 0.9303\n",
            "\n",
            "‚úÖ Best C: 10.0 (DEV Macro-F1: 0.9322)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9250\n",
            "   Accuracy: 0.9258\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9038    0.9216    0.9126       102\n",
            "entertainment     0.9367    0.9610    0.9487        77\n",
            "     politics     0.9059    0.9167    0.9112        84\n",
            "        sport     0.9515    0.9608    0.9561       102\n",
            "         tech     0.9324    0.8625    0.8961        80\n",
            "\n",
            "     accuracy                         0.9258       445\n",
            "    macro avg     0.9261    0.9245    0.9250       445\n",
            " weighted avg     0.9260    0.9258    0.9256       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec CBOW NS (optional - for comparison)\n",
        "result = train_and_evaluate(\n",
        "    representations['w2v_cbow_ns']['train'], y_train,\n",
        "    representations['w2v_cbow_ns']['dev'], y_dev,\n",
        "    representations['w2v_cbow_ns']['test'], y_test,\n",
        "    method_name='Word2Vec CBOW NS + TF-IDF'\n",
        ")\n",
        "# Note: Not required in results.json but good for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: Word2Vec Skip-gram HS + TF-IDF\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9328, Accuracy: 0.9326\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9392, Accuracy: 0.9393\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9362, Accuracy: 0.9371\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9342, Accuracy: 0.9348\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9318, Accuracy: 0.9326\n",
            "\n",
            "‚úÖ Best C: 0.1 (DEV Macro-F1: 0.9392)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9327\n",
            "   Accuracy: 0.9348\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9388    0.9020    0.9200       102\n",
            "entertainment     0.9241    0.9481    0.9359        77\n",
            "     politics     0.9070    0.9286    0.9176        84\n",
            "        sport     0.9714    1.0000    0.9855       102\n",
            "         tech     0.9221    0.8875    0.9045        80\n",
            "\n",
            "     accuracy                         0.9348       445\n",
            "    macro avg     0.9327    0.9332    0.9327       445\n",
            " weighted avg     0.9347    0.9348    0.9345       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec Skip-gram HS\n",
        "result = train_and_evaluate(\n",
        "    representations['w2v_sg_hs']['train'], y_train,\n",
        "    representations['w2v_sg_hs']['dev'], y_dev,\n",
        "    representations['w2v_sg_hs']['test'], y_test,\n",
        "    method_name='Word2Vec Skip-gram HS + TF-IDF'\n",
        ")\n",
        "results_classification['w2v_hs_tfidf'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: Word2Vec CBOW HS + TF-IDF\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9325, Accuracy: 0.9326\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9347, Accuracy: 0.9348\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9391, Accuracy: 0.9393\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9337, Accuracy: 0.9348\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9249, Accuracy: 0.9258\n",
            "\n",
            "‚úÖ Best C: 1.0 (DEV Macro-F1: 0.9391)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9289\n",
            "   Accuracy: 0.9303\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9293    0.9020    0.9154       102\n",
            "entertainment     0.9136    0.9610    0.9367        77\n",
            "     politics     0.9157    0.9048    0.9102        84\n",
            "        sport     0.9615    0.9804    0.9709       102\n",
            "         tech     0.9231    0.9000    0.9114        80\n",
            "\n",
            "     accuracy                         0.9303       445\n",
            "    macro avg     0.9286    0.9296    0.9289       445\n",
            " weighted avg     0.9303    0.9303    0.9301       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec CBOW HS (optional - for comparison)\n",
        "result = train_and_evaluate(\n",
        "    representations['w2v_cbow_hs']['train'], y_train,\n",
        "    representations['w2v_cbow_hs']['dev'], y_dev,\n",
        "    representations['w2v_cbow_hs']['test'], y_test,\n",
        "    method_name='Word2Vec CBOW HS + TF-IDF'\n",
        ")\n",
        "# Note: Not required in results.json but good for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training: GloVe + TF-IDF\n",
            "============================================================\n",
            "\n",
            "üîç Tuning hyperparameter C on DEV set...\n",
            "  C=   0.01 -> DEV Macro-F1: 0.9206, Accuracy: 0.9213\n",
            "  C=   0.10 -> DEV Macro-F1: 0.9371, Accuracy: 0.9371\n",
            "  C=   1.00 -> DEV Macro-F1: 0.9390, Accuracy: 0.9393\n",
            "  C=  10.00 -> DEV Macro-F1: 0.9433, Accuracy: 0.9438\n",
            "  C= 100.00 -> DEV Macro-F1: 0.9386, Accuracy: 0.9393\n",
            "\n",
            "‚úÖ Best C: 10.0 (DEV Macro-F1: 0.9433)\n",
            "\n",
            "üìä Evaluating on TEST set...\n",
            "\n",
            "üéØ TEST Results:\n",
            "   Macro-F1: 0.9267\n",
            "   Accuracy: 0.9281\n",
            "\n",
            "üìã Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business     0.9355    0.8529    0.8923       102\n",
            "entertainment     0.9487    0.9610    0.9548        77\n",
            "     politics     0.8889    0.9524    0.9195        84\n",
            "        sport     0.9623    1.0000    0.9808       102\n",
            "         tech     0.8974    0.8750    0.8861        80\n",
            "\n",
            "     accuracy                         0.9281       445\n",
            "    macro avg     0.9266    0.9283    0.9267       445\n",
            " weighted avg     0.9283    0.9281    0.9274       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# GloVe\n",
        "result = train_and_evaluate(\n",
        "    representations['glove']['train'], y_train,\n",
        "    representations['glove']['dev'], y_dev,\n",
        "    representations['glove']['test'], y_test,\n",
        "    method_name='GloVe + TF-IDF'\n",
        ")\n",
        "results_classification['glove_tfidf'] = {\n",
        "    'macro_f1': result['macro_f1'],\n",
        "    'accuracy': result['accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASSIFICATION RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Method                         Macro-F1     Accuracy    \n",
            "--------------------------------------------------------------------------------\n",
            "bow                            0.9683       0.9685      \n",
            "ohe                            0.9654       0.9663      \n",
            "ngram                          0.9639       0.9640      \n",
            "tfidf                          0.9639       0.9640      \n",
            "w2v_hs_tfidf                   0.9327       0.9348      \n",
            "w2v_ns_tfidf                   0.9306       0.9326      \n",
            "glove_tfidf                    0.9267       0.9281      \n",
            "\n",
            "================================================================================\n",
            "\n",
            "üèÜ Best Method: bow (Macro-F1: 0.9683)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSIFICATION RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Method':<30} {'Macro-F1':<12} {'Accuracy':<12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Sort by Macro-F1\n",
        "sorted_results = sorted(results_classification.items(), \n",
        "                       key=lambda x: x[1]['macro_f1'], \n",
        "                       reverse=True)\n",
        "\n",
        "for method, metrics in sorted_results:\n",
        "    print(f\"{method:<30} {metrics['macro_f1']:<12.4f} {metrics['accuracy']:<12.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Find best method\n",
        "best_method = sorted_results[0][0]\n",
        "best_f1 = sorted_results[0][1]['macro_f1']\n",
        "print(f\"\\nüèÜ Best Method: {best_method} (Macro-F1: {best_f1:.4f})\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate preds_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Generating preds_test.csv...\n",
            "‚úÖ Saved: ../outputs/preds_test.csv\n",
            "   Shape: (445, 2)\n",
            "\n",
            "First 5 predictions:\n",
            "          id           pred\n",
            "0  bbc_00006       politics\n",
            "1  bbc_00018       business\n",
            "2  bbc_00022          sport\n",
            "3  bbc_00024           tech\n",
            "4  bbc_00029  entertainment\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüíæ Generating preds_test.csv...\")\n",
        "\n",
        "# Create predictions DataFrame\n",
        "preds_df = pd.DataFrame({\n",
        "    'id': test_df['id'].values,\n",
        "    'pred': tfidf_predictions\n",
        "})\n",
        "\n",
        "# Save to outputs\n",
        "preds_df.to_csv(OUTPUTS_DIR / 'preds_test.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Saved: {OUTPUTS_DIR / 'preds_test.csv'}\")\n",
        "print(f\"   Shape: {preds_df.shape}\")\n",
        "print(f\"\\nFirst 5 predictions:\")\n",
        "print(preds_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Classification Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Classification results saved to cache/classification_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save classification results\n",
        "with open(CACHE_DIR / 'classification_results.json', 'w') as f:\n",
        "    json.dump(results_classification, f, indent=2)\n",
        "\n",
        "print(\"\\nüíæ Classification results saved to cache/classification_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ Notebook 04: Classification - COMPLETE!\n",
            "\n",
            "Next steps:\n",
            "  1. Run notebook 05: Retrieval\n",
            "  2. Generate deterministic queries\n",
            "  3. Calculate MAP@5, Recall@10, Negation Top-1%\n",
            "  4. Merge all notebooks into final submission\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüéâ Notebook 04: Classification - COMPLETE!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Run notebook 05: Retrieval\")\n",
        "print(\"  2. Generate deterministic queries\")\n",
        "print(\"  3. Calculate MAP@5, Recall@10, Negation Top-1%\")\n",
        "print(\"  4. Merge all notebooks into final submission\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
