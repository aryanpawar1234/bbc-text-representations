{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC Text Representations - Setup & Preprocessing\n",
    "\n",
    "**Roll Number:** SE22UARI195\n",
    "\n",
    "**Tasks:**\n",
    "1. Create master.csv with stratified 5-fold splits\n",
    "2. Generate deterministic train/dev/test split from roll number\n",
    "3. Build preprocessing pipeline\n",
    "4. Save processed data to cache\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import zlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK data downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "print(\"‚úÖ NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roll Number: SE22UARI195\n",
      "Data Directory: ../data\n",
      "Cache Directory: ../cache\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "ROLL = \"SE22UARI195\"  # Your roll number\n",
    "SEED = 137  # Fixed seed for reproducible folds\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "CACHE_DIR = Path(\"../cache\")\n",
    "SRC_FILE = DATA_DIR / \"bbc-text.csv\"\n",
    "MASTER_FILE = DATA_DIR / \"master.csv\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Roll Number: {ROLL}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Cache Directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Master CSV with 5-Fold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  master.csv already exists. Loading existing file...\n",
      "Loaded 2225 documents from master.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if master.csv already exists\n",
    "if MASTER_FILE.exists():\n",
    "    print(\"‚ö†Ô∏è  master.csv already exists. Loading existing file...\")\n",
    "    df = pd.read_csv(MASTER_FILE)\n",
    "    print(f\"Loaded {len(df)} documents from master.csv\")\n",
    "else:\n",
    "    print(\"Creating master.csv...\")\n",
    "    \n",
    "    # Load BBC dataset\n",
    "    if not SRC_FILE.exists():\n",
    "        print(f\"\\n‚ùå Error: {SRC_FILE} not found!\")\n",
    "        print(\"\\nPlease place 'bbc-text.csv' in the data/ folder.\")\n",
    "        print(\"You can download it from: [ADD DATASET LINK]\")\n",
    "    else:\n",
    "        df = pd.read_csv(SRC_FILE)\n",
    "        print(f\"‚úÖ Loaded {len(df)} documents from bbc-text.csv\")\n",
    "        \n",
    "        # Rename category to label\n",
    "        df = df.rename(columns={\"category\": \"label\"})\n",
    "        df = df[[\"text\", \"label\"]]\n",
    "        \n",
    "        # Add sequential IDs\n",
    "        df[\"id\"] = [f\"bbc_{i:05d}\" for i in range(len(df))]\n",
    "        \n",
    "        # Create 5 stratified folds\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        folds = np.zeros(len(df), dtype=int)\n",
    "        \n",
    "        for fold_num, (_, val_idx) in enumerate(skf.split(df[\"text\"], df[\"label\"])):\n",
    "            folds[val_idx] = fold_num\n",
    "        \n",
    "        df[\"fold5\"] = folds\n",
    "        \n",
    "        # Reorder columns\n",
    "        df = df[[\"id\", \"text\", \"label\", \"fold5\"]]\n",
    "        \n",
    "        # Save master.csv\n",
    "        df.to_csv(MASTER_FILE, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\n‚úÖ Saved master.csv with {len(df)} documents\")\n",
    "        \n",
    "        # Quick sanity check\n",
    "        assert df[\"id\"].is_unique, \"IDs are not unique!\"\n",
    "        assert df[\"fold5\"].between(0, 4).all(), \"Folds not in range 0-4!\"\n",
    "        print(\"‚úÖ Validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Dataset Statistics:\n",
      "Total documents: 2225\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "sport            511\n",
      "business         510\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fold distribution:\n",
      "fold5\n",
      "0    445\n",
      "1    445\n",
      "2    445\n",
      "3    445\n",
      "4    445\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic statistics\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"Total documents: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "print(f\"\\nFold distribution:\")\n",
    "print(df[\"fold5\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sample Documents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>fold5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bbc_00000</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbc_00001</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc_00002</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bbc_00003</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>sport</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc_00004</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0  bbc_00000  tv future in the hands of viewers with home th...   \n",
       "1  bbc_00001  worldcom boss  left books alone  former worldc...   \n",
       "2  bbc_00002  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  bbc_00003  yeading face newcastle in fa cup premiership s...   \n",
       "4  bbc_00004  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "           label  fold5  \n",
       "0           tech      2  \n",
       "1       business      0  \n",
       "2          sport      2  \n",
       "3          sport      0  \n",
       "4  entertainment      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample documents\n",
    "print(\"\\nüìÑ Sample Documents:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Train/Dev/Test Split from Roll Number\n",
    "\n",
    "The split is **deterministic** based on your roll number using CRC32 hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Roll Number: SE22UARI195\n",
      "üé≤ CRC32 Hash: 1507797122\n",
      "\n",
      "üìä Fold Assignment:\n",
      "  DEV fold:  2\n",
      "  TEST fold: 4\n",
      "  TRAIN folds: [0, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Calculate dev and test folds from roll number\n",
    "r = zlib.crc32(ROLL.encode())\n",
    "dev_fold = r % 5\n",
    "test_fold = (r // 5) % 5\n",
    "\n",
    "# Ensure dev and test folds are different\n",
    "if test_fold == dev_fold:\n",
    "    test_fold = (test_fold + 1) % 5\n",
    "\n",
    "print(f\"üé≤ Roll Number: {ROLL}\")\n",
    "print(f\"üé≤ CRC32 Hash: {r}\")\n",
    "print(f\"\\nüìä Fold Assignment:\")\n",
    "print(f\"  DEV fold:  {dev_fold}\")\n",
    "print(f\"  TEST fold: {test_fold}\")\n",
    "print(f\"  TRAIN folds: {[f for f in range(5) if f not in [dev_fold, test_fold]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Split Sizes:\n",
      "  TRAIN: 1335 documents (60.0%)\n",
      "  DEV:   445 documents (20.0%)\n",
      "  TEST:  445 documents (20.0%)\n",
      "  TOTAL: 2225 documents\n",
      "\n",
      "‚úÖ No overlap between splits!\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "DEV = df[df.fold5 == dev_fold].copy()\n",
    "TEST = df[df.fold5 == test_fold].copy()\n",
    "TRAIN = df[~df.fold5.isin([dev_fold, test_fold])].copy()\n",
    "\n",
    "print(f\"\\nüìà Split Sizes:\")\n",
    "print(f\"  TRAIN: {len(TRAIN)} documents ({len(TRAIN)/len(df)*100:.1f}%)\")\n",
    "print(f\"  DEV:   {len(DEV)} documents ({len(DEV)/len(df)*100:.1f}%)\")\n",
    "print(f\"  TEST:  {len(TEST)} documents ({len(TEST)/len(df)*100:.1f}%)\")\n",
    "print(f\"  TOTAL: {len(TRAIN) + len(DEV) + len(TEST)} documents\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(set(TRAIN.id) & set(DEV.id)) == 0, \"TRAIN and DEV overlap!\"\n",
    "assert len(set(TRAIN.id) & set(TEST.id)) == 0, \"TRAIN and TEST overlap!\"\n",
    "assert len(set(DEV.id) & set(TEST.id)) == 0, \"DEV and TEST overlap!\"\n",
    "print(\"\\n‚úÖ No overlap between splits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Class Distribution Across Splits:\n",
      "\n",
      "TRAIN:\n",
      "label\n",
      "sport            307\n",
      "business         306\n",
      "politics         250\n",
      "tech             241\n",
      "entertainment    231\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DEV:\n",
      "label\n",
      "sport            102\n",
      "business         102\n",
      "politics          83\n",
      "tech              80\n",
      "entertainment     78\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEST:\n",
      "label\n",
      "business         102\n",
      "sport            102\n",
      "politics          84\n",
      "tech              80\n",
      "entertainment     77\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in each split\n",
    "print(\"\\nüìä Class Distribution Across Splits:\")\n",
    "print(\"\\nTRAIN:\")\n",
    "print(TRAIN[\"label\"].value_counts())\n",
    "print(\"\\nDEV:\")\n",
    "print(DEV[\"label\"].value_counts())\n",
    "print(\"\\nTEST:\")\n",
    "print(TEST[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing Pipeline\n",
    "\n",
    "Steps:\n",
    "1. Lowercase\n",
    "2. Remove punctuation\n",
    "3. Normalize whitespace\n",
    "4. Tokenize\n",
    "5. Remove stopwords\n",
    "6. Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords loaded: 198 words\n",
      "Sample stopwords: ['itself', 'wasn', 'who', 'its', 'each', 'doing', 'there', 'below', 'hasn', 'again']\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Stopwords loaded: {len(stop_words)} words\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing function defined!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Preprocess a single text document.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "        lemmatize: Whether to lemmatize tokens\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'raw': original text\n",
    "        - 'tokens': list of processed tokens\n",
    "        - 'text': space-joined processed tokens\n",
    "    \"\"\"\n",
    "    # Store original\n",
    "    raw_text = text\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation (keep only alphanumeric and spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # 3. Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 4. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 5. Remove stopwords (optional)\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # 6. Lemmatize (optional)\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Remove empty tokens and numbers-only tokens\n",
    "    tokens = [t for t in tokens if len(t) > 1 and not t.isdigit()]\n",
    "    \n",
    "    return {\n",
    "        'raw': raw_text,\n",
    "        'tokens': tokens,\n",
    "        'text': ' '.join(tokens)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Preprocessing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original Text (first 300 chars):\n",
      "worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (¬£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not resp...\n",
      "\n",
      "\n",
      "üîß Processed Tokens (first 30):\n",
      "['worldcom', 'bos', 'left', 'book', 'alone', 'former', 'worldcom', 'bos', 'bernie', 'ebbers', 'accused', 'overseeing', '11bn', '8bn', 'fraud', 'never', 'made', 'accounting', 'decision', 'witness', 'told', 'juror', 'david', 'myers', 'made', 'comment', 'questioning', 'defence', 'lawyer', 'arguing']\n",
      "\n",
      "Total tokens: 187\n",
      "\n",
      "üìù Processed Text (first 300 chars):\n",
      "worldcom bos left book alone former worldcom bos bernie ebbers accused overseeing 11bn 8bn fraud never made accounting decision witness told juror david myers made comment questioning defence lawyer arguing mr ebbers responsible worldcom problem phone company collapsed prosecutor claim loss hidden p...\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessing on a sample document\n",
    "sample_text = TRAIN.iloc[0]['text']\n",
    "print(\"üìÑ Original Text (first 300 chars):\")\n",
    "print(sample_text[:300] + \"...\\n\")\n",
    "\n",
    "processed = preprocess_text(sample_text)\n",
    "print(\"\\nüîß Processed Tokens (first 30):\")\n",
    "print(processed['tokens'][:30])\n",
    "print(f\"\\nTotal tokens: {len(processed['tokens'])}\")\n",
    "\n",
    "print(\"\\nüìù Processed Text (first 300 chars):\")\n",
    "print(processed['text'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process All Splits and Save to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing function defined!\n"
     ]
    }
   ],
   "source": [
    "def process_split(split_df, split_name):\n",
    "    \"\"\"\n",
    "    Process all documents in a split.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Processing {split_name} split ({len(split_df)} documents)...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed = split_df['text'].progress_apply(preprocess_text)\n",
    "    \n",
    "    # Create new dataframe\n",
    "    result_df = split_df.copy()\n",
    "    result_df['text_raw'] = processed.apply(lambda x: x['raw'])\n",
    "    result_df['tokens'] = processed.apply(lambda x: x['tokens'])\n",
    "    result_df['text_processed'] = processed.apply(lambda x: x['text'])\n",
    "    result_df['token_count'] = result_df['tokens'].apply(len)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nüìä {split_name} Statistics:\")\n",
    "    print(f\"  Total documents: {len(result_df)}\")\n",
    "    print(f\"  Total tokens: {result_df['token_count'].sum():,}\")\n",
    "    print(f\"  Avg tokens/doc: {result_df['token_count'].mean():.1f}\")\n",
    "    print(f\"  Min tokens: {result_df['token_count'].min()}\")\n",
    "    print(f\"  Max tokens: {result_df['token_count'].max()}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"‚úÖ Processing function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Processing TRAIN split (1335 documents)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a069ef368514489a7a308da401db350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TRAIN Statistics:\n",
      "  Total documents: 1335\n",
      "  Total tokens: 285,829\n",
      "  Avg tokens/doc: 214.1\n",
      "  Min tokens: 61\n",
      "  Max tokens: 1635\n"
     ]
    }
   ],
   "source": [
    "# Process TRAIN split\n",
    "train_processed = process_split(TRAIN, \"TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Processing DEV split (445 documents)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7234a2930149208d04285a1e5b7b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DEV Statistics:\n",
      "  Total documents: 445\n",
      "  Total tokens: 97,572\n",
      "  Avg tokens/doc: 219.3\n",
      "  Min tokens: 79\n",
      "  Max tokens: 1769\n"
     ]
    }
   ],
   "source": [
    "# Process DEV split\n",
    "dev_processed = process_split(DEV, \"DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Processing TEST split (445 documents)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e134eb757a7f469f83545ca24bccb79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TEST Statistics:\n",
      "  Total documents: 445\n",
      "  Total tokens: 100,831\n",
      "  Avg tokens/doc: 226.6\n",
      "  Min tokens: 48\n",
      "  Max tokens: 2180\n"
     ]
    }
   ],
   "source": [
    "# Process TEST split\n",
    "test_processed = process_split(TEST, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Building vocabulary from TRAIN split...\n",
      "\n",
      "üìä Vocabulary Statistics:\n",
      "  Total tokens: 285,829\n",
      "  Unique tokens: 20,404\n",
      "\n",
      "üîù Top 20 most frequent tokens:\n",
      "  said            :  4415\n",
      "  year            :  1912\n",
      "  mr              :  1880\n",
      "  would           :  1570\n",
      "  also            :  1292\n",
      "  people          :  1214\n",
      "  new             :  1205\n",
      "  one             :  1125\n",
      "  time            :   923\n",
      "  could           :   922\n",
      "  game            :   906\n",
      "  last            :   813\n",
      "  two             :   778\n",
      "  first           :   773\n",
      "  world           :   750\n",
      "  say             :   740\n",
      "  film            :   692\n",
      "  company         :   679\n",
      "  firm            :   666\n",
      "  make            :   647\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from TRAIN only\n",
    "print(\"\\nüìö Building vocabulary from TRAIN split...\")\n",
    "\n",
    "# Flatten all tokens\n",
    "all_train_tokens = []\n",
    "for tokens in train_processed['tokens']:\n",
    "    all_train_tokens.extend(tokens)\n",
    "\n",
    "# Count frequencies\n",
    "from collections import Counter\n",
    "vocab_counter = Counter(all_train_tokens)\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Statistics:\")\n",
    "print(f\"  Total tokens: {len(all_train_tokens):,}\")\n",
    "print(f\"  Unique tokens: {len(vocab_counter):,}\")\n",
    "print(f\"\\nüîù Top 20 most frequent tokens:\")\n",
    "for token, count in vocab_counter.most_common(20):\n",
    "    print(f\"  {token:15s} : {count:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving processed data to cache...\n",
      "  ‚úÖ Saved: train_processed.pkl\n",
      "  ‚úÖ Saved: dev_processed.pkl\n",
      "  ‚úÖ Saved: test_processed.pkl\n",
      "  ‚úÖ Saved: vocab_counter.pkl\n",
      "\n",
      "üéâ All data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save processed data to cache\n",
    "print(\"\\nüíæ Saving processed data to cache...\")\n",
    "\n",
    "cache_files = {\n",
    "    'train_processed.pkl': train_processed,\n",
    "    'dev_processed.pkl': dev_processed,\n",
    "    'test_processed.pkl': test_processed,\n",
    "    'vocab_counter.pkl': vocab_counter\n",
    "}\n",
    "\n",
    "for filename, data in cache_files.items():\n",
    "    filepath = CACHE_DIR / filename\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"  ‚úÖ Saved: {filename}\")\n",
    "\n",
    "print(\"\\nüéâ All data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata saved!\n",
      "\n",
      "üìã Metadata:\n",
      "  roll: SE22UARI195\n",
      "  dev_fold: 2\n",
      "  test_fold: 4\n",
      "  train_size: 1335\n",
      "  dev_size: 445\n",
      "  test_size: 445\n",
      "  vocab_size: 20404\n",
      "  total_train_tokens: 285829\n"
     ]
    }
   ],
   "source": [
    "# Save split metadata\n",
    "metadata = {\n",
    "    'roll': ROLL,\n",
    "    'dev_fold': int(dev_fold),\n",
    "    'test_fold': int(test_fold),\n",
    "    'train_size': len(train_processed),\n",
    "    'dev_size': len(dev_processed),\n",
    "    'test_size': len(test_processed),\n",
    "    'vocab_size': len(vocab_counter),\n",
    "    'total_train_tokens': len(all_train_tokens)\n",
    "}\n",
    "\n",
    "metadata_path = CACHE_DIR / 'metadata.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"‚úÖ Metadata saved!\")\n",
    "print(\"\\nüìã Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "‚úÖ **Completed:**\n",
    "- Created master.csv with 5-fold stratified splits\n",
    "- Generated train/dev/test split for roll SE22UARI195\n",
    "- Preprocessed all text (lowercase, tokenize, stopwords, lemmatize)\n",
    "- Built vocabulary from TRAIN split\n",
    "- Saved all processed data to cache/\n",
    "\n",
    "**Next Steps:**\n",
    "- Build sparse representations (OHE, BoW, N-grams, TF-IDF)\n",
    "- Build dense representations (Word2Vec, GloVe)\n",
    "- Train classifiers\n",
    "- Build retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ NOTEBOOK 01: SETUP & PREPROCESSING COMPLETE! üéâ\n",
      "============================================================\n",
      "\n",
      "‚úÖ Processed 2225 documents\n",
      "‚úÖ Built vocabulary of 20,404 unique tokens\n",
      "‚úÖ Saved all data to ../cache\n",
      "\n",
      "üìù Ready for next notebook: 02_sparse_methods.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ NOTEBOOK 01: SETUP & PREPROCESSING COMPLETE! üéâ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Processed {len(train_processed) + len(dev_processed) + len(test_processed)} documents\")\n",
    "print(f\"‚úÖ Built vocabulary of {len(vocab_counter):,} unique tokens\")\n",
    "print(f\"‚úÖ Saved all data to {CACHE_DIR}\")\n",
    "print(\"\\nüìù Ready for next notebook: 02_sparse_methods.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
