{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# BBC Text Representations - Setup & Preprocessing\n",
       "\n",
       "**Roll Number:** SE22UARI195\n",
       "\n",
       "**Tasks:**\n",
       "1. Create master.csv with stratified 5-fold splits\n",
       "2. Generate deterministic train/dev/test split from roll number\n",
       "3. Build preprocessing pipeline\n",
       "4. Save processed data to cache\n",
       "\n",
       "---"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup & Imports"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Core libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import pickle\n",
       "import os\n",
       "import re\n",
       "import zlib\n",
       "from pathlib import Path\n",
       "\n",
       "# Preprocessing\n",
       "import nltk\n",
       "from nltk.tokenize import word_tokenize\n",
       "from nltk.corpus import stopwords\n",
       "from nltk.stem import WordNetLemmatizer\n",
       "\n",
       "# Sklearn\n",
       "from sklearn.model_selection import StratifiedKFold\n",
       "\n",
       "# Progress bar\n",
       "from tqdm.notebook import tqdm\n",
       "tqdm.pandas()\n",
       "\n",
       "print(\"‚úÖ Imports successful!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Download required NLTK data\n",
       "nltk.download('punkt', quiet=True)\n",
       "nltk.download('stopwords', quiet=True)\n",
       "nltk.download('wordnet', quiet=True)\n",
       "nltk.download('omw-1.4', quiet=True)\n",
       "print(\"‚úÖ NLTK data downloaded!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configuration\n",
       "ROLL = \"SE22UARI195\"  # Your roll number\n",
       "SEED = 137  # Fixed seed for reproducible folds\n",
       "\n",
       "# Paths\n",
       "DATA_DIR = Path(\"../data\")\n",
       "CACHE_DIR = Path(\"../cache\")\n",
       "SRC_FILE = DATA_DIR / \"bbc-text.csv\"\n",
       "MASTER_FILE = DATA_DIR / \"master.csv\"\n",
       "\n",
       "# Create directories if they don't exist\n",
       "DATA_DIR.mkdir(exist_ok=True)\n",
       "CACHE_DIR.mkdir(exist_ok=True)\n",
       "\n",
       "print(f\"Roll Number: {ROLL}\")\n",
       "print(f\"Data Directory: {DATA_DIR}\")\n",
       "print(f\"Cache Directory: {CACHE_DIR}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Create Master CSV with 5-Fold Splits"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Check if master.csv already exists\n",
       "if MASTER_FILE.exists():\n",
       "    print(\"‚ö†Ô∏è  master.csv already exists. Loading existing file...\")\n",
       "    df = pd.read_csv(MASTER_FILE)\n",
       "    print(f\"Loaded {len(df)} documents from master.csv\")\n",
       "else:\n",
       "    print(\"Creating master.csv...\")\n",
       "    \n",
       "    # Load BBC dataset\n",
       "    if not SRC_FILE.exists():\n",
       "        print(f\"\\n‚ùå Error: {SRC_FILE} not found!\")\n",
       "        print(\"\\nPlease place 'bbc-text.csv' in the data/ folder.\")\n",
       "        print(\"You can download it from: [ADD DATASET LINK]\")\n",
       "    else:\n",
       "        df = pd.read_csv(SRC_FILE)\n",
       "        print(f\"‚úÖ Loaded {len(df)} documents from bbc-text.csv\")\n",
       "        \n",
       "        # Rename category to label\n",
       "        df = df.rename(columns={\"category\": \"label\"})\n",
       "        df = df[[\"text\", \"label\"]]\n",
       "        \n",
       "        # Add sequential IDs\n",
       "        df[\"id\"] = [f\"bbc_{i:05d}\" for i in range(len(df))]\n",
       "        \n",
       "        # Create 5 stratified folds\n",
       "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
       "        folds = np.zeros(len(df), dtype=int)\n",
       "        \n",
       "        for fold_num, (_, val_idx) in enumerate(skf.split(df[\"text\"], df[\"label\"])):\n",
       "            folds[val_idx] = fold_num\n",
       "        \n",
       "        df[\"fold5\"] = folds\n",
       "        \n",
       "        # Reorder columns\n",
       "        df = df[[\"id\", \"text\", \"label\", \"fold5\"]]\n",
       "        \n",
       "        # Save master.csv\n",
       "        df.to_csv(MASTER_FILE, index=False, encoding=\"utf-8\")\n",
       "        print(f\"\\n‚úÖ Saved master.csv with {len(df)} documents\")\n",
       "        \n",
       "        # Quick sanity check\n",
       "        assert df[\"id\"].is_unique, \"IDs are not unique!\"\n",
       "        assert df[\"fold5\"].between(0, 4).all(), \"Folds not in range 0-4!\"\n",
       "        print(\"‚úÖ Validation passed!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Display basic statistics\n",
       "print(\"\\nüìä Dataset Statistics:\")\n",
       "print(f\"Total documents: {len(df)}\")\n",
       "print(f\"\\nClass distribution:\")\n",
       "print(df[\"label\"].value_counts())\n",
       "print(f\"\\nFold distribution:\")\n",
       "print(df[\"fold5\"].value_counts().sort_index())"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Show sample documents\n",
       "print(\"\\nüìÑ Sample Documents:\")\n",
       "df.head()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Generate Train/Dev/Test Split from Roll Number\n",
       "\n",
       "The split is **deterministic** based on your roll number using CRC32 hash."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate dev and test folds from roll number\n",
       "r = zlib.crc32(ROLL.encode())\n",
       "dev_fold = r % 5\n",
       "test_fold = (r // 5) % 5\n",
       "\n",
       "# Ensure dev and test folds are different\n",
       "if test_fold == dev_fold:\n",
       "    test_fold = (test_fold + 1) % 5\n",
       "\n",
       "print(f\"üé≤ Roll Number: {ROLL}\")\n",
       "print(f\"üé≤ CRC32 Hash: {r}\")\n",
       "print(f\"\\nüìä Fold Assignment:\")\n",
       "print(f\"  DEV fold:  {dev_fold}\")\n",
       "print(f\"  TEST fold: {test_fold}\")\n",
       "print(f\"  TRAIN folds: {[f for f in range(5) if f not in [dev_fold, test_fold]]}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Split the data\n",
       "DEV = df[df.fold5 == dev_fold].copy()\n",
       "TEST = df[df.fold5 == test_fold].copy()\n",
       "TRAIN = df[~df.fold5.isin([dev_fold, test_fold])].copy()\n",
       "\n",
       "print(f\"\\nüìà Split Sizes:\")\n",
       "print(f\"  TRAIN: {len(TRAIN)} documents ({len(TRAIN)/len(df)*100:.1f}%)\")\n",
       "print(f\"  DEV:   {len(DEV)} documents ({len(DEV)/len(df)*100:.1f}%)\")\n",
       "print(f\"  TEST:  {len(TEST)} documents ({len(TEST)/len(df)*100:.1f}%)\")\n",
       "print(f\"  TOTAL: {len(TRAIN) + len(DEV) + len(TEST)} documents\")\n",
       "\n",
       "# Verify no overlap\n",
       "assert len(set(TRAIN.id) & set(DEV.id)) == 0, \"TRAIN and DEV overlap!\"\n",
       "assert len(set(TRAIN.id) & set(TEST.id)) == 0, \"TRAIN and TEST overlap!\"\n",
       "assert len(set(DEV.id) & set(TEST.id)) == 0, \"DEV and TEST overlap!\"\n",
       "print(\"\\n‚úÖ No overlap between splits!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Check class distribution in each split\n",
       "print(\"\\nüìä Class Distribution Across Splits:\")\n",
       "print(\"\\nTRAIN:\")\n",
       "print(TRAIN[\"label\"].value_counts())\n",
       "print(\"\\nDEV:\")\n",
       "print(DEV[\"label\"].value_counts())\n",
       "print(\"\\nTEST:\")\n",
       "print(TEST[\"label\"].value_counts())"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Text Preprocessing Pipeline\n",
       "\n",
       "Steps:\n",
       "1. Lowercase\n",
       "2. Remove punctuation\n",
       "3. Normalize whitespace\n",
       "4. Tokenize\n",
       "5. Remove stopwords\n",
       "6. Lemmatize"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize preprocessing tools\n",
       "lemmatizer = WordNetLemmatizer()\n",
       "stop_words = set(stopwords.words('english'))\n",
       "\n",
       "print(f\"Stopwords loaded: {len(stop_words)} words\")\n",
       "print(f\"Sample stopwords: {list(stop_words)[:10]}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
       "    \"\"\"\n",
       "    Preprocess a single text document.\n",
       "    \n",
       "    Args:\n",
       "        text: Input text string\n",
       "        remove_stopwords: Whether to remove stopwords\n",
       "        lemmatize: Whether to lemmatize tokens\n",
       "    \n",
       "    Returns:\n",
       "        Dictionary with:\n",
       "        - 'raw': original text\n",
       "        - 'tokens': list of processed tokens\n",
       "        - 'text': space-joined processed tokens\n",
       "    \"\"\"\n",
       "    # Store original\n",
       "    raw_text = text\n",
       "    \n",
       "    # 1. Lowercase\n",
       "    text = text.lower()\n",
       "    \n",
       "    # 2. Remove punctuation (keep only alphanumeric and spaces)\n",
       "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
       "    \n",
       "    # 3. Normalize whitespace\n",
       "    text = re.sub(r'\\s+', ' ', text).strip()\n",
       "    \n",
       "    # 4. Tokenize\n",
       "    tokens = word_tokenize(text)\n",
       "    \n",
       "    # 5. Remove stopwords (optional)\n",
       "    if remove_stopwords:\n",
       "        tokens = [t for t in tokens if t not in stop_words]\n",
       "    \n",
       "    # 6. Lemmatize (optional)\n",
       "    if lemmatize:\n",
       "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
       "    \n",
       "    # Remove empty tokens and numbers-only tokens\n",
       "    tokens = [t for t in tokens if len(t) > 1 and not t.isdigit()]\n",
       "    \n",
       "    return {\n",
       "        'raw': raw_text,\n",
       "        'tokens': tokens,\n",
       "        'text': ' '.join(tokens)\n",
       "    }\n",
       "\n",
       "print(\"‚úÖ Preprocessing function defined!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Test preprocessing on a sample document\n",
       "sample_text = TRAIN.iloc[0]['text']\n",
       "print(\"üìÑ Original Text (first 300 chars):\")\n",
       "print(sample_text[:300] + \"...\\n\")\n",
       "\n",
       "processed = preprocess_text(sample_text)\n",
       "print(\"\\nüîß Processed Tokens (first 30):\")\n",
       "print(processed['tokens'][:30])\n",
       "print(f\"\\nTotal tokens: {len(processed['tokens'])}\")\n",
       "\n",
       "print(\"\\nüìù Processed Text (first 300 chars):\")\n",
       "print(processed['text'][:300] + \"...\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Process All Splits and Save to Cache"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def process_split(split_df, split_name):\n",
       "    \"\"\"\n",
       "    Process all documents in a split.\n",
       "    \"\"\"\n",
       "    print(f\"\\nüîß Processing {split_name} split ({len(split_df)} documents)...\")\n",
       "    \n",
       "    # Apply preprocessing\n",
       "    processed = split_df['text'].progress_apply(preprocess_text)\n",
       "    \n",
       "    # Create new dataframe\n",
       "    result_df = split_df.copy()\n",
       "    result_df['text_raw'] = processed.apply(lambda x: x['raw'])\n",
       "    result_df['tokens'] = processed.apply(lambda x: x['tokens'])\n",
       "    result_df['text_processed'] = processed.apply(lambda x: x['text'])\n",
       "    result_df['token_count'] = result_df['tokens'].apply(len)\n",
       "    \n",
       "    # Statistics\n",
       "    print(f\"\\nüìä {split_name} Statistics:\")\n",
       "    print(f\"  Total documents: {len(result_df)}\")\n",
       "    print(f\"  Total tokens: {result_df['token_count'].sum():,}\")\n",
       "    print(f\"  Avg tokens/doc: {result_df['token_count'].mean():.1f}\")\n",
       "    print(f\"  Min tokens: {result_df['token_count'].min()}\")\n",
       "    print(f\"  Max tokens: {result_df['token_count'].max()}\")\n",
       "    \n",
       "    return result_df\n",
       "\n",
       "print(\"‚úÖ Processing function defined!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Process TRAIN split\n",
       "train_processed = process_split(TRAIN, \"TRAIN\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Process DEV split\n",
       "dev_processed = process_split(DEV, \"DEV\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Process TEST split\n",
       "test_processed = process_split(TEST, \"TEST\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Build vocabulary from TRAIN only\n",
       "print(\"\\nüìö Building vocabulary from TRAIN split...\")\n",
       "\n",
       "# Flatten all tokens\n",
       "all_train_tokens = []\n",
       "for tokens in train_processed['tokens']:\n",
       "    all_train_tokens.extend(tokens)\n",
       "\n",
       "# Count frequencies\n",
       "from collections import Counter\n",
       "vocab_counter = Counter(all_train_tokens)\n",
       "\n",
       "print(f\"\\nüìä Vocabulary Statistics:\")\n",
       "print(f\"  Total tokens: {len(all_train_tokens):,}\")\n",
       "print(f\"  Unique tokens: {len(vocab_counter):,}\")\n",
       "print(f\"\\nüîù Top 20 most frequent tokens:\")\n",
       "for token, count in vocab_counter.most_common(20):\n",
       "    print(f\"  {token:15s} : {count:5d}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Save processed data to cache\n",
       "print(\"\\nüíæ Saving processed data to cache...\")\n",
       "\n",
       "cache_files = {\n",
       "    'train_processed.pkl': train_processed,\n",
       "    'dev_processed.pkl': dev_processed,\n",
       "    'test_processed.pkl': test_processed,\n",
       "    'vocab_counter.pkl': vocab_counter\n",
       "}\n",
       "\n",
       "for filename, data in cache_files.items():\n",
       "    filepath = CACHE_DIR / filename\n",
       "    with open(filepath, 'wb') as f:\n",
       "        pickle.dump(data, f)\n",
       "    print(f\"  ‚úÖ Saved: {filename}\")\n",
       "\n",
       "print(\"\\nüéâ All data saved successfully!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Save split metadata\n",
       "metadata = {\n",
       "    'roll': ROLL,\n",
       "    'dev_fold': int(dev_fold),\n",
       "    'test_fold': int(test_fold),\n",
       "    'train_size': len(train_processed),\n",
       "    'dev_size': len(dev_processed),\n",
       "    'test_size': len(test_processed),\n",
       "    'vocab_size': len(vocab_counter),\n",
       "    'total_train_tokens': len(all_train_tokens)\n",
       "}\n",
       "\n",
       "metadata_path = CACHE_DIR / 'metadata.pkl'\n",
       "with open(metadata_path, 'wb') as f:\n",
       "    pickle.dump(metadata, f)\n",
       "\n",
       "print(\"‚úÖ Metadata saved!\")\n",
       "print(\"\\nüìã Metadata:\")\n",
       "for key, value in metadata.items():\n",
       "    print(f\"  {key}: {value}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Summary\n",
       "\n",
       "‚úÖ **Completed:**\n",
       "- Created master.csv with 5-fold stratified splits\n",
       "- Generated train/dev/test split for roll SE22UARI195\n",
       "- Preprocessed all text (lowercase, tokenize, stopwords, lemmatize)\n",
       "- Built vocabulary from TRAIN split\n",
       "- Saved all processed data to cache/\n",
       "\n",
       "**Next Steps:**\n",
       "- Build sparse representations (OHE, BoW, N-grams, TF-IDF)\n",
       "- Build dense representations (Word2Vec, GloVe)\n",
       "- Train classifiers\n",
       "- Build retrieval system"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(\"\\n\" + \"=\"*60)\n",
       "print(\"üéâ NOTEBOOK 01: SETUP & PREPROCESSING COMPLETE! üéâ\")\n",
       "print(\"=\"*60)\n",
       "print(f\"\\n‚úÖ Processed {len(train_processed) + len(dev_processed) + len(test_processed)} documents\")\n",
       "print(f\"‚úÖ Built vocabulary of {len(vocab_counter):,} unique tokens\")\n",
       "print(f\"‚úÖ Saved all data to {CACHE_DIR}\")\n",
       "print(\"\\nüìù Ready for next notebook: 02_sparse_methods.ipynb\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }