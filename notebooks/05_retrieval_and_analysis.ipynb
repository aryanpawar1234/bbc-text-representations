{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# BBC Text Representations - Retrieval\n",
          "\n",
          "**Roll Number:** SE22UARI195\n",
          "\n",
          "**Tasks:**\n",
          "1. Generate 20 deterministic queries (15 TF-IDF + 5 negation)\n",
          "2. Rank TEST documents using cosine similarity\n",
          "3. Calculate MAP@5, Recall@10, Negation Top-1%\n",
          "4. Save queries.json and rankings.json\n",
          "5. Print QUERY_SIGNATURE and RANK_SIGNATURE\n",
          "\n",
          "---"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 1. Setup & Imports"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Core libraries\n",
          "import pandas as pd\n",
          "import numpy as np\n",
          "import pickle\n",
          "import json\n",
          "import hashlib\n",
          "from pathlib import Path\n",
          "from collections import defaultdict\n",
          "\n",
          "# Sklearn\n",
          "from sklearn.metrics.pairwise import cosine_similarity\n",
          "from sklearn.feature_extraction.text import TfidfVectorizer\n",
          "\n",
          "# Scipy\n",
          "from scipy.sparse import load_npz\n",
          "\n",
          "# NLTK\n",
          "import nltk\n",
          "from nltk.corpus import stopwords\n",
          "from nltk.stem import WordNetLemmatizer\n",
          "\n",
          "# Gensim\n",
          "from gensim.models import Word2Vec\n",
          "\n",
          "# Progress bar\n",
          "from tqdm.notebook import tqdm\n",
          "\n",
          "print(\"âœ… Imports successful!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Configuration\n",
          "ROLL = \"SE22UARI195\"\n",
          "CACHE_DIR = Path(\"../cache\")\n",
          "MODELS_DIR = Path(\"../models\")\n",
          "OUTPUTS_DIR = Path(\"../outputs\")\n",
          "DATA_DIR = Path(\"../data\")\n",
          "\n",
          "print(f\"Roll Number: {ROLL}\")\n",
          "print(f\"Cache Directory: {CACHE_DIR}\")\n",
          "print(f\"Models Directory: {MODELS_DIR}\")\n",
          "print(f\"Outputs Directory: {OUTPUTS_DIR}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 2. Load Data & Representations"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"ðŸ“‚ Loading preprocessed data...\\n\")\n",
          "\n",
          "with open(CACHE_DIR / 'train_processed.pkl', 'rb') as f:\n",
          "    train_df = pickle.load(f)\n",
          "\n",
          "with open(CACHE_DIR / 'test_processed.pkl', 'rb') as f:\n",
          "    test_df = pickle.load(f)\n",
          "\n",
          "print(f\"âœ… TRAIN: {len(train_df)} documents\")\n",
          "print(f\"âœ… TEST: {len(test_df)} documents\")\n",
          "\n",
          "# Load master.csv for original text\n",
          "master_df = pd.read_csv(DATA_DIR / 'master.csv')\n",
          "print(f\"âœ… MASTER: {len(master_df)} documents\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“Š Loading TF-IDF vectorizer and representations...\\n\")\n",
          "\n",
          "# Load TF-IDF vectorizer\n",
          "with open(MODELS_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
          "    tfidf_vectorizer = pickle.load(f)\n",
          "\n",
          "print(f\"âœ… TF-IDF vectorizer loaded (vocab size: {len(tfidf_vectorizer.vocabulary_):,})\")\n",
          "\n",
          "# Load representations for retrieval\n",
          "representations = {}\n",
          "\n",
          "representations['tfidf'] = {\n",
          "    'train': load_npz(MODELS_DIR / 'X_train_tfidf.npz'),\n",
          "    'test': load_npz(MODELS_DIR / 'X_test_tfidf.npz')\n",
          "}\n",
          "\n",
          "representations['w2v_ns_tfidf'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_w2v_sg_ns.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_w2v_sg_ns.npy')\n",
          "}\n",
          "\n",
          "representations['w2v_hs_tfidf'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_w2v_sg_hs.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_w2v_sg_hs.npy')\n",
          "}\n",
          "\n",
          "representations['glove_tfidf'] = {\n",
          "    'train': np.load(MODELS_DIR / 'X_train_glove.npy'),\n",
          "    'test': np.load(MODELS_DIR / 'X_test_glove.npy')\n",
          "}\n",
          "\n",
          "print(f\"âœ… All representations loaded\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 3. Generate Deterministic Queries"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ” Generating 20 deterministic queries...\\n\")\n",
          "\n",
          "# Deterministic seed from roll number\n",
          "SEED = int(hashlib.sha256(ROLL.encode()).hexdigest()[:8], 16)\n",
          "rng = np.random.default_rng(SEED)\n",
          "\n",
          "print(f\"Deterministic SEED: {SEED}\")\n",
          "print(f\"Roll: {ROLL}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### 3.1 Generate 15 TF-IDF Queries"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“ Generating 15 TF-IDF queries...\\n\")\n",
          "\n",
          "train_ids = sorted(train_df['id'].tolist())\n",
          "idx = np.linspace(0, len(train_ids)-1, 15, dtype=int)\n",
          "picked_ids = [train_ids[(i + (SEED % 7)) % len(train_ids)] for i in idx]\n",
          "\n",
          "print(f\"Selected 15 TRAIN document IDs:\")\n",
          "for i, doc_id in enumerate(picked_ids, 1):\n",
          "    print(f\"  {i:2d}. {doc_id}\")\n",
          "\n",
          "queries_tfidf = []\n",
          "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
          "\n",
          "print(f\"\\nExtracting top-2 TF-IDF terms...\\n\")\n",
          "\n",
          "for qid_num, doc_id in enumerate(picked_ids, 1):\n",
          "    doc_idx = train_df[train_df['id'] == doc_id].index[0]\n",
          "    train_idx = train_df.index.get_loc(doc_idx)\n",
          "    \n",
          "    tfidf_vec = representations['tfidf']['train'][train_idx].toarray().flatten()\n",
          "    top_indices = np.argsort(tfidf_vec)[-2:][::-1]\n",
          "    top_terms = [feature_names[i] for i in top_indices if tfidf_vec[i] > 0]\n",
          "    query_text = ' '.join(top_terms[:2])\n",
          "    source_label = train_df[train_df['id'] == doc_id]['label'].values[0]\n",
          "    \n",
          "    query = {\n",
          "        'qid': f'Q{qid_num:03d}',\n",
          "        'source_id': doc_id,\n",
          "        'query_text': query_text,\n",
          "        'source_label': source_label,\n",
          "        'type': 'tfidf'\n",
          "    }\n",
          "    \n",
          "    queries_tfidf.append(query)\n",
          "    print(f\"{query['qid']}: '{query_text}' (label: {source_label})\")\n",
          "\n",
          "print(f\"\\nâœ… Generated {len(queries_tfidf)} TF-IDF queries\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### 3.2 Generate 5 Negation Queries"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸš« Generating 5 negation queries...\\n\")\n",
          "\n",
          "train_with_toklen = train_df.copy()\n",
          "train_with_toklen['toklen'] = train_with_toklen['tokens'].apply(len)\n",
          "short5_df = train_with_toklen.sort_values(['toklen', 'id']).head(5)\n",
          "short5_ids = short5_df['id'].tolist()\n",
          "\n",
          "print(f\"5 shortest TRAIN documents:\")\n",
          "for i, (idx, row) in enumerate(short5_df.iterrows(), 1):\n",
          "    print(f\"  {i}. {row['id']} (tokens: {row['toklen']}, label: {row['label']})\")\n",
          "\n",
          "negation_rules = {\n",
          "    'good': 'not good', 'great': 'not great', 'best': 'not best',\n",
          "    'new': 'not new', 'first': 'not first', 'high': 'not high',\n",
          "    'will': 'will not', 'can': 'cannot', 'could': 'could not'\n",
          "}\n",
          "\n",
          "queries_negation = []\n",
          "\n",
          "print(f\"\\nApplying negation transformations...\\n\")\n",
          "\n",
          "for qid_num, doc_id in enumerate(short5_ids, 16):\n",
          "    tokens = train_df[train_df['id'] == doc_id]['tokens'].values[0]\n",
          "    negated_tokens = tokens.copy()\n",
          "    negation_applied = False\n",
          "    \n",
          "    for i, token in enumerate(tokens):\n",
          "        if token in negation_rules:\n",
          "            negated_tokens[i] = negation_rules[token]\n",
          "            negation_applied = True\n",
          "            break\n",
          "    \n",
          "    if not negation_applied and len(tokens) > 0:\n",
          "        negated_tokens = ['not'] + tokens\n",
          "    \n",
          "    query_text = ' '.join(negated_tokens)\n",
          "    source_label = train_df[train_df['id'] == doc_id]['label'].values[0]\n",
          "    \n",
          "    query = {\n",
          "        'qid': f'Q{qid_num:03d}',\n",
          "        'source_id': doc_id,\n",
          "        'query_text': query_text,\n",
          "        'source_label': source_label,\n",
          "        'type': 'neg'\n",
          "    }\n",
          "    \n",
          "    queries_negation.append(query)\n",
          "    print(f\"{query['qid']}: '{query_text[:50]}...'\")\n",
          "\n",
          "print(f\"\\nâœ… Generated {len(queries_negation)} negation queries\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "all_queries = queries_tfidf + queries_negation\n",
          "\n",
          "print(f\"\\nðŸ“‹ Total queries: {len(all_queries)}\")\n",
          "print(f\"   TF-IDF: {len(queries_tfidf)}\")\n",
          "print(f\"   Negation: {len(queries_negation)}\")\n",
          "\n",
          "with open(OUTPUTS_DIR / 'queries.json', 'w', encoding='utf-8') as f:\n",
          "    json.dump(all_queries, f, indent=2, ensure_ascii=False)\n",
          "\n",
          "print(f\"\\nðŸ’¾ Saved: {OUTPUTS_DIR / 'queries.json'}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 4. Ranking Functions"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def rank_documents_sparse(query_text, test_vectors, tfidf_vec, test_ids, top_k=10):\n",
          "    \"\"\"Rank TEST documents (sparse TF-IDF).\"\"\"\n",
          "    query_vec = tfidf_vec.transform([query_text])\n",
          "    similarities = cosine_similarity(query_vec, test_vectors).flatten()\n",
          "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
          "    return [test_ids[i] for i in top_indices]\n",
          "\n",
          "def rank_documents_dense(query_text, test_vectors, word_vectors, tfidf_vec, tfidf_vocab, test_ids, top_k=10):\n",
          "    \"\"\"Rank TEST documents (dense Word2Vec/GloVe with TF-IDF pooling).\"\"\"\n",
          "    tokens = query_text.lower().split()\n",
          "    stop_words = set(stopwords.words('english'))\n",
          "    lemmatizer = WordNetLemmatizer()\n",
          "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
          "    \n",
          "    if hasattr(word_vectors, 'wv'):\n",
          "        vector_size = word_vectors.wv.vector_size\n",
          "        vocab = word_vectors.wv\n",
          "    else:\n",
          "        vector_size = len(next(iter(word_vectors.values())))\n",
          "        vocab = word_vectors\n",
          "    \n",
          "    if len(tokens) == 0:\n",
          "        query_vec = np.zeros(vector_size)\n",
          "    else:\n",
          "        text = ' '.join(tokens)\n",
          "        tfidf_vec_query = tfidf_vec.transform([text]).toarray()[0]\n",
          "        weighted_sum = np.zeros(vector_size)\n",
          "        total_weight = 0.0\n",
          "        \n",
          "        for token in tokens:\n",
          "            if token in tfidf_vocab:\n",
          "                tfidf_idx = tfidf_vocab[token]\n",
          "                tfidf_weight = tfidf_vec_query[tfidf_idx]\n",
          "                \n",
          "                if hasattr(vocab, '__contains__'):\n",
          "                    if token in vocab:\n",
          "                        word_vec = vocab.get_vector(token) if hasattr(vocab, 'get_vector') else vocab[token]\n",
          "                        weighted_sum += tfidf_weight * word_vec\n",
          "                        total_weight += tfidf_weight\n",
          "                else:\n",
          "                    if token in vocab:\n",
          "                        weighted_sum += tfidf_weight * vocab[token]\n",
          "                        total_weight += tfidf_weight\n",
          "        \n",
          "        query_vec = weighted_sum / total_weight if total_weight > 0 else np.zeros(vector_size)\n",
          "    \n",
          "    query_vec = query_vec.reshape(1, -1)\n",
          "    similarities = cosine_similarity(query_vec, test_vectors).flatten()\n",
          "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
          "    return [test_ids[i] for i in top_indices]\n",
          "\n",
          "print(\"âœ… Ranking functions defined!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 5. Rank TEST Documents"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ” Loading models for ranking...\\n\")\n",
          "\n",
          "test_ids = test_df['id'].tolist()\n",
          "rankings = {}\n",
          "\n",
          "w2v_sg_ns = Word2Vec.load(str(MODELS_DIR / 'w2v_sg_ns.model'))\n",
          "w2v_sg_hs = Word2Vec.load(str(MODELS_DIR / 'w2v_sg_hs.model'))\n",
          "print(\"âœ… Word2Vec models loaded\")\n",
          "\n",
          "glove_path = DATA_DIR / 'glove.6B.100d.txt'\n",
          "glove_embeddings = {}\n",
          "\n",
          "with open(glove_path, 'r', encoding='utf-8') as f:\n",
          "    for line in tqdm(f, desc=\"Loading GloVe\", total=400000):\n",
          "        values = line.split()\n",
          "        word = values[0]\n",
          "        vector = np.asarray(values[1:], dtype='float32')\n",
          "        glove_embeddings[word] = vector\n",
          "\n",
          "print(f\"âœ… GloVe loaded: {len(glove_embeddings):,} vectors\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nRanking with tfidf...\")\n",
          "rankings['tfidf'] = {}\n",
          "\n",
          "for query in tqdm(all_queries, desc=\"tfidf\"):\n",
          "    rankings['tfidf'][query['qid']] = rank_documents_sparse(\n",
          "        query['query_text'], representations['tfidf']['test'], \n",
          "        tfidf_vectorizer, test_ids, top_k=10\n",
          "    )\n",
          "\n",
          "print(f\"âœ… Ranked {len(all_queries)} queries\\n\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"Ranking with w2v_ns_tfidf...\")\n",
          "rankings['w2v_ns_tfidf'] = {}\n",
          "\n",
          "for query in tqdm(all_queries, desc=\"w2v_ns\"):\n",
          "    rankings['w2v_ns_tfidf'][query['qid']] = rank_documents_dense(\n",
          "        query['query_text'], representations['w2v_ns_tfidf']['test'],\n",
          "        w2v_sg_ns, tfidf_vectorizer, tfidf_vectorizer.vocabulary_,\n",
          "        test_ids, top_k=10\n",
          "    )\n",
          "\n",
          "print(f\"âœ… Ranked {len(all_queries)} queries\\n\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"Ranking with w2v_hs_tfidf...\")\n",
          "rankings['w2v_hs_tfidf'] = {}\n",
          "\n",
          "for query in tqdm(all_queries, desc=\"w2v_hs\"):\n",
          "    rankings['w2v_hs_tfidf'][query['qid']] = rank_documents_dense(\n",
          "        query['query_text'], representations['w2v_hs_tfidf']['test'],\n",
          "        w2v_sg_hs, tfidf_vectorizer, tfidf_vectorizer.vocabulary_,\n",
          "        test_ids, top_k=10\n",
          "    )\n",
          "\n",
          "print(f\"âœ… Ranked {len(all_queries)} queries\\n\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"Ranking with glove_tfidf...\")\n",
          "rankings['glove_tfidf'] = {}\n",
          "\n",
          "for query in tqdm(all_queries, desc=\"glove\"):\n",
          "    rankings['glove_tfidf'][query['qid']] = rank_documents_dense(\n",
          "        query['query_text'], representations['glove_tfidf']['test'],\n",
          "        glove_embeddings, tfidf_vectorizer, tfidf_vectorizer.vocabulary_,\n",
          "        test_ids, top_k=10\n",
          "    )\n",
          "\n",
          "print(f\"âœ… Ranked {len(all_queries)} queries\\n\")\n",
          "print(\"\\nâœ… All rankings complete!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "with open(OUTPUTS_DIR / 'rankings.json', 'w', encoding='utf-8') as f:\n",
          "    json.dump(rankings['tfidf'], f, indent=2)\n",
          "\n",
          "print(f\"ðŸ’¾ Saved: {OUTPUTS_DIR / 'rankings.json'}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 6. Calculate Retrieval Metrics"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def calculate_map_at_k(queries, rankings, test_df, k=5):\n",
          "    id2label = dict(zip(test_df['id'], test_df['label']))\n",
          "    aps = []\n",
          "    for query in queries:\n",
          "        qid, source_label = query['qid'], query['source_label']\n",
          "        ranked_docs = rankings[qid][:k]\n",
          "        hits, score = 0, 0.0\n",
          "        for i, doc_id in enumerate(ranked_docs, 1):\n",
          "            if id2label[doc_id] == source_label:\n",
          "                hits += 1\n",
          "                score += hits / i\n",
          "        total_relevant = sum(1 for label in id2label.values() if label == source_label)\n",
          "        ap = score / min(k, total_relevant) if total_relevant > 0 else 0.0\n",
          "        aps.append(ap)\n",
          "    return np.mean(aps)\n",
          "\n",
          "def calculate_recall_at_k(queries, rankings, test_df, k=10):\n",
          "    id2label = dict(zip(test_df['id'], test_df['label']))\n",
          "    recalls = []\n",
          "    for query in queries:\n",
          "        qid, source_label = query['qid'], query['source_label']\n",
          "        ranked_docs = rankings[qid][:k]\n",
          "        hits = sum(1 for doc_id in ranked_docs if id2label[doc_id] == source_label)\n",
          "        total_relevant = sum(1 for label in id2label.values() if label == source_label)\n",
          "        recall = hits / total_relevant if total_relevant > 0 else 0.0\n",
          "        recalls.append(recall)\n",
          "    return np.mean(recalls)\n",
          "\n",
          "def calculate_negation_top1(queries, rankings, test_df):\n",
          "    id2label = dict(zip(test_df['id'], test_df['label']))\n",
          "    neg_queries = [q for q in queries if q['type'] == 'neg']\n",
          "    if len(neg_queries) == 0:\n",
          "        return 0.0\n",
          "    correct = 0\n",
          "    for query in neg_queries:\n",
          "        qid, source_label = query['qid'], query['source_label']\n",
          "        top1_doc = rankings[qid][0]\n",
          "        if id2label[top1_doc] == source_label:\n",
          "            correct += 1\n",
          "    return correct / len(neg_queries)\n",
          "\n",
          "print(\"âœ… Metric functions defined!\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸ“Š Calculating retrieval metrics...\\n\")\n",
          "\n",
          "retrieval_results = {}\n",
          "\n",
          "for repr_name, repr_rankings in rankings.items():\n",
          "    print(f\"Evaluating {repr_name}...\")\n",
          "    map5 = calculate_map_at_k(all_queries, repr_rankings, test_df, k=5)\n",
          "    recall10 = calculate_recall_at_k(all_queries, repr_rankings, test_df, k=10)\n",
          "    neg_top1 = calculate_negation_top1(all_queries, repr_rankings, test_df)\n",
          "    \n",
          "    retrieval_results[repr_name] = {\n",
          "        'map@5': float(map5),\n",
          "        'recall@10': float(recall10),\n",
          "        'neg_top1': float(neg_top1)\n",
          "    }\n",
          "    \n",
          "    print(f\"  MAP@5: {map5:.4f}, Recall@10: {recall10:.4f}, Neg Top-1%: {neg_top1:.4f}\\n\")\n",
          "\n",
          "print(\"âœ… Metrics calculated!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 7. Summary & Signatures"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nðŸŽ‰ Notebook 05: Retrieval - COMPLETE!\")\n",
          "print(\"\\nâœ… Generated files:\")\n",
          "print(f\"  - {OUTPUTS_DIR / 'queries.json'}\")\n",
          "print(f\"  - {OUTPUTS_DIR / 'rankings.json'}\")\n",
          "print(\"\\nâœ… Signatures printed above\")\n",
          "print(\"\\nNext steps:\")\n",
          "print(\"  1. Compile results.json from all notebooks\")\n",
          "print(\"  2. Run pre-submission validator\")\n",
          "print(\"  3. Write report.pdf (1-2 pages)\")\n",
          "print(\"  4. Package submission: SE22UARI195/\")"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8.0"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }