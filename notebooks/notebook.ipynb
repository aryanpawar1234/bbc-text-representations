{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC Text Representations - Final Submission\n",
    "\n",
    "**Roll Number:** SE22UARI195  \n",
    "**Course:** Computational Sequence Modeling (CSM)  \n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "This notebook implements and compares multiple text representation methods:\n",
    "- **Sparse:** OHE, BoW, N-grams, TF-IDF\n",
    "- **Dense:** Word2Vec (Skip-gram/CBOW with NS/HS), GloVe\n",
    "- **Tasks:** Classification (Logistic Regression), Retrieval (Cosine Similarity)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1032)>\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import zlib\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Ensure NLTK data\n",
    "for resource in ['punkt', 'stopwords', 'wordnet', 'omw-1.4']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
    "    except LookupError:\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Compute Deterministic Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC32 Hash: 1507797122\n",
      "Fold Assignment:\n",
      "  DEV fold:  2\n",
      "  TEST fold: 4\n",
      "  TRAIN folds: [0, 1, 3]\n",
      "\n",
      "Split Sizes:\n",
      "  TRAIN: 1335 documents (60.0%)\n",
      "  DEV:   445 documents (20.0%)\n",
      "  TEST:  445 documents (20.0%)\n",
      "  TOTAL: 2225 documents\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zlib\n",
    "\n",
    "# Configuration - CORRECTED PATHS\n",
    "ROLL = 'SE22UARI195'\n",
    "MASTER_CSV = '../data/master.csv'\n",
    "CACHE_DIR = Path('../cache')\n",
    "MODELS_DIR = Path('../models')\n",
    "OUTPUTS_DIR = Path('../outputs')\n",
    "\n",
    "# Load master dataset\n",
    "df = pd.read_csv(MASTER_CSV)\n",
    "\n",
    "# Compute deterministic folds from roll number\n",
    "r = zlib.crc32(ROLL.encode())\n",
    "dev_fold = r % 5\n",
    "test_fold = (r // 5) % 5\n",
    "if test_fold == dev_fold:\n",
    "    test_fold = (test_fold + 1) % 5\n",
    "\n",
    "# Create splits\n",
    "DEV = df[df.fold5 == dev_fold].reset_index(drop=True)\n",
    "TEST = df[df.fold5 == test_fold].reset_index(drop=True)\n",
    "TRAIN = df[~df.fold5.isin([dev_fold, test_fold])].reset_index(drop=True)\n",
    "\n",
    "print(f\"CRC32 Hash: {r}\")\n",
    "print(f\"Fold Assignment:\")\n",
    "print(f\"  DEV fold:  {dev_fold}\")\n",
    "print(f\"  TEST fold: {test_fold}\")\n",
    "print(f\"  TRAIN folds: {sorted([i for i in range(5) if i not in [dev_fold, test_fold]])}\")\n",
    "print(f\"\\nSplit Sizes:\")\n",
    "print(f\"  TRAIN: {len(TRAIN)} documents ({len(TRAIN)/len(df)*100:.1f}%)\")\n",
    "print(f\"  DEV:   {len(DEV)} documents ({len(DEV)/len(df)*100:.1f}%)\")\n",
    "print(f\"  TEST:  {len(TEST)} documents ({len(TEST)/len(df)*100:.1f}%)\")\n",
    "print(f\"  TOTAL: {len(df)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Cached Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded processed data from cache\n",
      "  Vocabulary size: 20,404\n",
      "  TRAIN tokens: 285,829\n",
      "  DEV tokens: 97,572\n",
      "  TEST tokens: 100,831\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data from cache\n",
    "train_processed = pd.read_pickle(CACHE_DIR / 'train_processed.pkl')\n",
    "dev_processed = pd.read_pickle(CACHE_DIR / 'dev_processed.pkl')\n",
    "test_processed = pd.read_pickle(CACHE_DIR / 'test_processed.pkl')\n",
    "vocab_counter = pd.read_pickle(CACHE_DIR / 'vocab_counter.pkl')\n",
    "\n",
    "print(f\" Loaded processed data from cache\")\n",
    "print(f\"  Vocabulary size: {len(vocab_counter):,}\")\n",
    "print(f\"  TRAIN tokens: {sum(train_processed['tokens'].apply(len)):,}\")\n",
    "print(f\"  DEV tokens: {sum(dev_processed['tokens'].apply(len)):,}\")\n",
    "print(f\"  TEST tokens: {sum(test_processed['tokens'].apply(len)):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Sparse Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded all sparse representations\n",
      "  OHE shape: (1335, 2000)\n",
      "  BoW shape: (1335, 11515)\n",
      "  N-gram shape: (1335, 18625)\n",
      "  TF-IDF shape: (1335, 11515)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Load sparse representations (saved as .npz files)\n",
    "X_train_ohe = sparse.load_npz(MODELS_DIR / 'X_train_ohe.npz')\n",
    "X_train_bow = sparse.load_npz(MODELS_DIR / 'X_train_bow.npz')\n",
    "X_train_ngram = sparse.load_npz(MODELS_DIR / 'X_train_ngram.npz')\n",
    "X_train_tfidf = sparse.load_npz(MODELS_DIR / 'X_train_tfidf.npz')\n",
    "\n",
    "X_test_ohe = sparse.load_npz(MODELS_DIR / 'X_test_ohe.npz')\n",
    "X_test_bow = sparse.load_npz(MODELS_DIR / 'X_test_bow.npz')\n",
    "X_test_ngram = sparse.load_npz(MODELS_DIR / 'X_test_ngram.npz')\n",
    "X_test_tfidf = sparse.load_npz(MODELS_DIR / 'X_test_tfidf.npz')\n",
    "\n",
    "# Load vectorizers (saved as .pkl files)\n",
    "import pickle\n",
    "with open(MODELS_DIR / 'ohe_vectorizer.pkl', 'rb') as f:\n",
    "    ohe_vec = pickle.load(f)\n",
    "with open(MODELS_DIR / 'bow_vectorizer.pkl', 'rb') as f:\n",
    "    bow_vec = pickle.load(f)\n",
    "with open(MODELS_DIR / 'ngram_vectorizer.pkl', 'rb') as f:\n",
    "    ngram_vec = pickle.load(f)\n",
    "with open(MODELS_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vec = pickle.load(f)\n",
    "\n",
    "print(\" Loaded all sparse representations\")\n",
    "print(f\"  OHE shape: {X_train_ohe.shape}\")\n",
    "print(f\"  BoW shape: {X_train_bow.shape}\")\n",
    "print(f\"  N-gram shape: {X_train_ngram.shape}\")\n",
    "print(f\"  TF-IDF shape: {X_train_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dense Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ glove_embeddings.pkl not found, loading from txt (this may take a moment)...\n",
      "Loaded GloVe: 400,000 words\n",
      " Loaded all dense representations\n",
      "  W2V Skip-gram NS shape: (1335, 100)\n",
      "  W2V Skip-gram HS shape: (1335, 100)\n",
      "  GloVe shape: (1335, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "# Load Word2Vec models\n",
    "w2v_sg_ns = Word2Vec.load(str(MODELS_DIR / 'w2v_sg_ns.model'))\n",
    "w2v_cbow_ns = Word2Vec.load(str(MODELS_DIR / 'w2v_cbow_ns.model'))\n",
    "w2v_sg_hs = Word2Vec.load(str(MODELS_DIR / 'w2v_sg_hs.model'))\n",
    "w2v_cbow_hs = Word2Vec.load(str(MODELS_DIR / 'w2v_cbow_hs.model'))\n",
    "\n",
    "# Load GloVe embeddings dictionary (if it exists, otherwise load from txt)\n",
    "glove_pkl_path = MODELS_DIR / 'glove_embeddings.pkl'\n",
    "if glove_pkl_path.exists():\n",
    "    with open(glove_pkl_path, 'rb') as f:\n",
    "        glove_dict = pickle.load(f)\n",
    "    print(\"Loaded GloVe from pickle\")\n",
    "else:\n",
    "    # Load from original txt file\n",
    "    print(\"⚠️ glove_embeddings.pkl not found, loading from txt (this may take a moment)...\")\n",
    "    glove_dict = {}\n",
    "    glove_path = Path('../data/glove.6B.100d.txt')\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove_dict[word] = vector\n",
    "    print(f\"Loaded GloVe: {len(glove_dict):,} words\")\n",
    "\n",
    "# Load dense document representations (saved as .npy files)\n",
    "X_train_w2v_ns = np.load(MODELS_DIR / 'X_train_w2v_sg_ns.npy')\n",
    "X_train_w2v_hs = np.load(MODELS_DIR / 'X_train_w2v_sg_hs.npy')\n",
    "X_train_glove = np.load(MODELS_DIR / 'X_train_glove.npy')\n",
    "\n",
    "X_test_w2v_ns = np.load(MODELS_DIR / 'X_test_w2v_sg_ns.npy')\n",
    "X_test_w2v_hs = np.load(MODELS_DIR / 'X_test_w2v_sg_hs.npy')\n",
    "X_test_glove = np.load(MODELS_DIR / 'X_test_glove.npy')\n",
    "\n",
    "print(\" Loaded all dense representations\")\n",
    "print(f\"  W2V Skip-gram NS shape: {X_train_w2v_ns.shape}\")\n",
    "print(f\"  W2V Skip-gram HS shape: {X_train_w2v_hs.shape}\")\n",
    "print(f\"  GloVe shape: {X_train_glove.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregate Health Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Health metrics compiled\n"
     ]
    }
   ],
   "source": [
    "# Health metrics from your notebook outputs\n",
    "health_metrics = {\n",
    "    'ohe': {\n",
    "        'V': 2000,\n",
    "        'nnz': int(X_train_ohe.nnz),\n",
    "        'sparsity': 0.9498,\n",
    "        'oov': 0.2863,\n",
    "        'topk_100': 0.0,\n",
    "        'topk_500': 0.0,\n",
    "        'fit_s': 0.224,\n",
    "        'ms_per_doc': 0.124,\n",
    "        'mem_mb': 1.539\n",
    "    },\n",
    "    'bow': {\n",
    "        'V': 11515,\n",
    "        'nnz': int(X_train_bow.nnz),\n",
    "        'sparsity': 0.9879,\n",
    "        'oov': 0.0706,\n",
    "        'topk_100': 0.1195,\n",
    "        'topk_500': 0.3344,\n",
    "        'fit_s': 0.221,\n",
    "        'ms_per_doc': 0.173,\n",
    "        'mem_mb': 2.142\n",
    "    },\n",
    "    'ngram': {\n",
    "        'V': 18625,\n",
    "        'nnz': int(X_train_ngram.nnz),\n",
    "        'sparsity': 0.9907,\n",
    "        'oov': 0.0935,\n",
    "        'topk_100': 0.0,\n",
    "        'topk_500': 0.0,\n",
    "        'fit_s': 0.724,\n",
    "        'ms_per_doc': 0.374,\n",
    "        'mem_mb': 2.648\n",
    "    },\n",
    "    'tfidf': {\n",
    "        'V': 11515,\n",
    "        'nnz': int(X_train_tfidf.nnz),\n",
    "        'sparsity': 0.9879,\n",
    "        'oov': 0.0706,\n",
    "        'topk_100': 0.1195,\n",
    "        'topk_500': 0.3344,\n",
    "        'fit_s': 0.200,\n",
    "        'ms_per_doc': 0.141,\n",
    "        'mem_mb': 2.142\n",
    "    },\n",
    "    'w2v_ns': {\n",
    "        'tokens_per_sec': 45760.0\n",
    "    },\n",
    "    'w2v_hs': {\n",
    "        'tokens_per_sec': 44775.0\n",
    "    },\n",
    "    'glove': {}\n",
    "}\n",
    "\n",
    "print(\" Health metrics compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification metrics compiled\n",
      "\n",
      " Best method: bow (Macro-F1: 0.9683)\n"
     ]
    }
   ],
   "source": [
    "# Classification metrics from your notebook outputs\n",
    "classification_metrics = {\n",
    "    'ohe': {\n",
    "        'macro_f1': 0.9654,\n",
    "        'accuracy': 0.9663\n",
    "    },\n",
    "    'bow': {\n",
    "        'macro_f1': 0.9683,\n",
    "        'accuracy': 0.9685\n",
    "    },\n",
    "    'ngram': {\n",
    "        'macro_f1': 0.9639,\n",
    "        'accuracy': 0.9640\n",
    "    },\n",
    "    'tfidf': {\n",
    "        'macro_f1': 0.9639,\n",
    "        'accuracy': 0.9640\n",
    "    },\n",
    "    'w2v_ns_tfidf': {\n",
    "        'macro_f1': 0.9306,\n",
    "        'accuracy': 0.9326\n",
    "    },\n",
    "    'w2v_hs_tfidf': {\n",
    "        'macro_f1': 0.9327,\n",
    "        'accuracy': 0.9348\n",
    "    },\n",
    "    'glove_tfidf': {\n",
    "        'macro_f1': 0.9267,\n",
    "        'accuracy': 0.9281\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\" Classification metrics compiled\")\n",
    "print(f\"\\n Best method: bow (Macro-F1: {classification_metrics['bow']['macro_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and Compute Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 20 queries and rankings\n",
      " Retrieval metrics computed\n",
      "  MAP@5: 0.6993\n",
      "  Recall@10: 0.0710\n",
      "  Negation Top-1%: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Load queries and rankings\n",
    "with open(OUTPUTS_DIR / 'queries.json', 'r') as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "with open(OUTPUTS_DIR / 'rankings.json', 'r') as f:\n",
    "    rankings = json.load(f)\n",
    "\n",
    "print(f\" Loaded {len(queries)} queries and rankings\")\n",
    "\n",
    "# Helper functions for retrieval metrics\n",
    "def map_at_k(truth, ranks, k=5):\n",
    "    \"\"\"Calculate MAP@k\"\"\"\n",
    "    ap_scores = []\n",
    "    for qid, ranked_docs in ranks.items():\n",
    "        relevant = truth.get(qid, [])\n",
    "        if not relevant:\n",
    "            continue\n",
    "        hits = 0\n",
    "        score = 0.0\n",
    "        for i, doc_id in enumerate(ranked_docs[:k], 1):\n",
    "            if doc_id in relevant:\n",
    "                hits += 1\n",
    "                score += hits / i\n",
    "        ap_scores.append(score / min(k, len(relevant)))\n",
    "    return float(np.mean(ap_scores)) if ap_scores else 0.0\n",
    "\n",
    "def recall_at_k(truth, ranks, k=10):\n",
    "    \"\"\"Calculate Recall@k\"\"\"\n",
    "    recalls = []\n",
    "    for qid, ranked_docs in ranks.items():\n",
    "        relevant = truth.get(qid, [])\n",
    "        if not relevant:\n",
    "            continue\n",
    "        retrieved = sum(1 for doc_id in ranked_docs[:k] if doc_id in relevant)\n",
    "        recalls.append(retrieved / len(relevant))\n",
    "    return float(np.mean(recalls)) if recalls else 0.0\n",
    "\n",
    "def neg_top1_accuracy(queries_list, ranks):\n",
    "    \"\"\"Calculate negation top-1% accuracy\"\"\"\n",
    "    neg_queries = [q for q in queries_list if q['type'] == 'neg']\n",
    "    if not neg_queries:\n",
    "        return 0.0\n",
    "    correct = 0\n",
    "    for q in neg_queries:\n",
    "        qid = q['qid']\n",
    "        source_label = q['source_label']\n",
    "        ranked = ranks.get(qid, [])\n",
    "        if ranked:\n",
    "            top_doc_id = ranked[0]\n",
    "            # Check if top doc has same label as source\n",
    "            top_doc_label = TEST[TEST['id'] == top_doc_id]['label'].values\n",
    "            if len(top_doc_label) > 0 and top_doc_label[0] == source_label:\n",
    "                correct += 1\n",
    "    return float(correct / len(neg_queries))\n",
    "\n",
    "# Build ground truth (same-label docs as relevant)\n",
    "ground_truth = {}\n",
    "for q in queries:\n",
    "    qid = q['qid']\n",
    "    source_label = q['source_label']\n",
    "    relevant_docs = TEST[TEST['label'] == source_label]['id'].tolist()\n",
    "    ground_truth[qid] = relevant_docs\n",
    "\n",
    "# Compute retrieval metrics (using TF-IDF rankings from your output)\n",
    "retrieval_metrics = {\n",
    "    'tfidf': {\n",
    "        'map@5': map_at_k(ground_truth, rankings, k=5),\n",
    "        'recall@10': recall_at_k(ground_truth, rankings, k=10),\n",
    "        'neg_top1': neg_top1_accuracy(queries, rankings)\n",
    "    },\n",
    "    'w2v_ns_tfidf': {\n",
    "        'map@5': 0.0,  # Replace with actual if computed\n",
    "        'recall@10': 0.0,\n",
    "        'neg_top1': 0.0\n",
    "    },\n",
    "    'w2v_hs_tfidf': {\n",
    "        'map@5': 0.0,\n",
    "        'recall@10': 0.0,\n",
    "        'neg_top1': 0.0\n",
    "    },\n",
    "    'glove_tfidf': {\n",
    "        'map@5': 0.0,\n",
    "        'recall@10': 0.0,\n",
    "        'neg_top1': 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\" Retrieval metrics computed\")\n",
    "print(f\"  MAP@5: {retrieval_metrics['tfidf']['map@5']:.4f}\")\n",
    "print(f\"  Recall@10: {retrieval_metrics['tfidf']['recall@10']:.4f}\")\n",
    "print(f\"  Negation Top-1%: {retrieval_metrics['tfidf']['neg_top1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing retrieval rankings for dense methods...\n",
      "============================================================\n",
      "\n",
      "1. Computing W2V Skip-gram NS rankings...\n",
      "2. Computing W2V Skip-gram HS rankings...\n",
      "3. Computing GloVe rankings...\n",
      "\n",
      " All rankings computed!\n",
      "\n",
      "============================================================\n",
      "Computing Retrieval Metrics\n",
      "============================================================\n",
      "\n",
      "W2V Skip-gram NS:\n",
      "  MAP@5: 0.8325\n",
      "  Recall@10: 0.0911\n",
      "  Negation Top-1%: 1.0000\n",
      "\n",
      "W2V Skip-gram HS:\n",
      "  MAP@5: 0.8402\n",
      "  Recall@10: 0.0916\n",
      "  Negation Top-1%: 1.0000\n",
      "\n",
      "GloVe:\n",
      "  MAP@5: 0.8673\n",
      "  Recall@10: 0.0943\n",
      "  Negation Top-1%: 1.0000\n",
      "\n",
      "============================================================\n",
      " All retrieval metrics computed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Compute Retrieval Metrics for W2V and GloVe\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"Computing retrieval rankings for dense methods...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Helper function to get document vectors for queries\n",
    "def get_query_vector(query_text, model_type='w2v_ns', tfidf_vec=None):\n",
    "    \"\"\"\n",
    "    Convert query text to vector using the specified method\n",
    "    \"\"\"\n",
    "    # Tokenize and preprocess query\n",
    "    tokens = query_text.lower().split()\n",
    "    \n",
    "    if model_type == 'w2v_ns':\n",
    "        model = w2v_sg_ns\n",
    "    elif model_type == 'w2v_hs':\n",
    "        model = w2v_sg_hs\n",
    "    elif model_type == 'glove':\n",
    "        model = glove_dict\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = []\n",
    "    word_weights = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if model_type in ['w2v_ns', 'w2v_hs']:\n",
    "            if token in model.wv:\n",
    "                word_vectors.append(model.wv[token])\n",
    "                # Use uniform weighting for query\n",
    "                word_weights.append(1.0)\n",
    "        else:  # GloVe\n",
    "            if token in model:\n",
    "                word_vectors.append(model[token])\n",
    "                word_weights.append(1.0)\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        # Return zero vector if no words found\n",
    "        return np.zeros(100)\n",
    "    \n",
    "    # Weighted average\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    word_weights = np.array(word_weights)\n",
    "    doc_vector = np.average(word_vectors, axis=0, weights=word_weights)\n",
    "    \n",
    "    return doc_vector\n",
    "\n",
    "# Function to compute rankings for a method\n",
    "def compute_rankings_for_method(X_test_dense, method_name):\n",
    "    \"\"\"\n",
    "    Compute top-10 rankings for all queries using cosine similarity\n",
    "    \"\"\"\n",
    "    rankings_dict = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        qid = query['qid']\n",
    "        query_text = query['query_text']\n",
    "        \n",
    "        # Get query vector\n",
    "        if 'w2v_ns' in method_name or 'sg_ns' in method_name:\n",
    "            query_vec = get_query_vector(query_text, model_type='w2v_ns')\n",
    "        elif 'w2v_hs' in method_name or 'sg_hs' in method_name:\n",
    "            query_vec = get_query_vector(query_text, model_type='w2v_hs')\n",
    "        else:  # GloVe\n",
    "            query_vec = get_query_vector(query_text, model_type='glove')\n",
    "        \n",
    "        # Compute cosine similarity with all test documents\n",
    "        query_vec_reshaped = query_vec.reshape(1, -1)\n",
    "        similarities = cosine_similarity(query_vec_reshaped, X_test_dense)[0]\n",
    "        \n",
    "        # Get top-10 document indices\n",
    "        top_10_indices = np.argsort(similarities)[::-1][:10]\n",
    "        \n",
    "        # Map indices to document IDs\n",
    "        top_10_doc_ids = TEST.iloc[top_10_indices]['id'].tolist()\n",
    "        \n",
    "        rankings_dict[qid] = top_10_doc_ids\n",
    "    \n",
    "    return rankings_dict\n",
    "\n",
    "# Compute rankings for each dense method\n",
    "print(\"\\n1. Computing W2V Skip-gram NS rankings...\")\n",
    "rankings_w2v_ns = compute_rankings_for_method(X_test_w2v_ns, 'w2v_ns')\n",
    "\n",
    "print(\"2. Computing W2V Skip-gram HS rankings...\")\n",
    "rankings_w2v_hs = compute_rankings_for_method(X_test_w2v_hs, 'w2v_hs')\n",
    "\n",
    "print(\"3. Computing GloVe rankings...\")\n",
    "rankings_glove = compute_rankings_for_method(X_test_glove, 'glove')\n",
    "\n",
    "print(\"\\n All rankings computed!\")\n",
    "\n",
    "# Now compute retrieval metrics for each method\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Computing Retrieval Metrics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build ground truth (same as before)\n",
    "ground_truth = {}\n",
    "for q in queries:\n",
    "    qid = q['qid']\n",
    "    source_label = q['source_label']\n",
    "    relevant_docs = TEST[TEST['label'] == source_label]['id'].tolist()\n",
    "    ground_truth[qid] = relevant_docs\n",
    "\n",
    "# Compute metrics for W2V NS\n",
    "map_w2v_ns = map_at_k(ground_truth, rankings_w2v_ns, k=5)\n",
    "recall_w2v_ns = recall_at_k(ground_truth, rankings_w2v_ns, k=10)\n",
    "neg_w2v_ns = neg_top1_accuracy(queries, rankings_w2v_ns)\n",
    "\n",
    "print(f\"\\nW2V Skip-gram NS:\")\n",
    "print(f\"  MAP@5: {map_w2v_ns:.4f}\")\n",
    "print(f\"  Recall@10: {recall_w2v_ns:.4f}\")\n",
    "print(f\"  Negation Top-1%: {neg_w2v_ns:.4f}\")\n",
    "\n",
    "# Compute metrics for W2V HS\n",
    "map_w2v_hs = map_at_k(ground_truth, rankings_w2v_hs, k=5)\n",
    "recall_w2v_hs = recall_at_k(ground_truth, rankings_w2v_hs, k=10)\n",
    "neg_w2v_hs = neg_top1_accuracy(queries, rankings_w2v_hs)\n",
    "\n",
    "print(f\"\\nW2V Skip-gram HS:\")\n",
    "print(f\"  MAP@5: {map_w2v_hs:.4f}\")\n",
    "print(f\"  Recall@10: {recall_w2v_hs:.4f}\")\n",
    "print(f\"  Negation Top-1%: {neg_w2v_hs:.4f}\")\n",
    "\n",
    "# Compute metrics for GloVe\n",
    "map_glove = map_at_k(ground_truth, rankings_glove, k=5)\n",
    "recall_glove = recall_at_k(ground_truth, rankings_glove, k=10)\n",
    "neg_glove = neg_top1_accuracy(queries, rankings_glove)\n",
    "\n",
    "print(f\"\\nGloVe:\")\n",
    "print(f\"  MAP@5: {map_glove:.4f}\")\n",
    "print(f\"  Recall@10: {recall_glove:.4f}\")\n",
    "print(f\"  Negation Top-1%: {neg_glove:.4f}\")\n",
    "\n",
    "# Update retrieval_metrics dictionary\n",
    "retrieval_metrics = {\n",
    "    'tfidf': {\n",
    "        'map@5': retrieval_metrics['tfidf']['map@5'],  # Keep existing\n",
    "        'recall@10': retrieval_metrics['tfidf']['recall@10'],\n",
    "        'neg_top1': retrieval_metrics['tfidf']['neg_top1']\n",
    "    },\n",
    "    'w2v_ns_tfidf': {\n",
    "        'map@5': map_w2v_ns,\n",
    "        'recall@10': recall_w2v_ns,\n",
    "        'neg_top1': neg_w2v_ns\n",
    "    },\n",
    "    'w2v_hs_tfidf': {\n",
    "        'map@5': map_w2v_hs,\n",
    "        'recall@10': recall_w2v_hs,\n",
    "        'neg_top1': neg_w2v_hs\n",
    "    },\n",
    "    'glove_tfidf': {\n",
    "        'map@5': map_glove,\n",
    "        'recall@10': recall_glove,\n",
    "        'neg_top1': neg_glove\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" All retrieval metrics computed!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: ../outputs/results.json\n",
      "\n",
      " Results Summary:\n",
      "  Roll: SE22UARI195\n",
      "  DEV fold: 2, TEST fold: 4\n",
      "  Best classification: bow (F1: 0.9683)\n",
      "  TF-IDF MAP@5: 0.6993\n"
     ]
    }
   ],
   "source": [
    "# Compile all results\n",
    "results = {\n",
    "    'roll': ROLL,\n",
    "    'dev_fold': int(dev_fold),\n",
    "    'test_fold': int(test_fold),\n",
    "    'health': health_metrics,\n",
    "    'classification': classification_metrics,\n",
    "    'retrieval': retrieval_metrics\n",
    "}\n",
    "\n",
    "# Save results.json\n",
    "results_path = OUTPUTS_DIR / 'results.json'\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\" Saved: {results_path}\")\n",
    "print(f\"\\n Results Summary:\")\n",
    "print(f\"  Roll: {ROLL}\")\n",
    "print(f\"  DEV fold: {dev_fold}, TEST fold: {test_fold}\")\n",
    "print(f\"  Best classification: bow (F1: {classification_metrics['bow']['macro_f1']:.4f})\")\n",
    "print(f\"  TF-IDF MAP@5: {retrieval_metrics['tfidf']['map@5']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Print Query and Ranking Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Submission Signatures:\n",
      "QUERY_SIGNATURE: 76a19dbc0010bdf730183e42c05c434bbe5f0d1c3066dbc6563d55f0555208d8\n",
      "RANK_SIGNATURE:  30da9df4e76e94f4af33e5e276067e51c6471c1d87140e5dde634114d82f94cc\n"
     ]
    }
   ],
   "source": [
    "# Generate deterministic signatures\n",
    "query_strings = sorted(f\"{q['qid']}|{q['query_text']}\" for q in queries)\n",
    "QUERY_SIGNATURE = hashlib.sha256('\\n'.join(query_strings).encode()).hexdigest()\n",
    "\n",
    "rank_strings = [f\"{qid}|{','.join(rankings[qid][:10])}\" for qid in sorted(rankings.keys())]\n",
    "RANK_SIGNATURE = hashlib.sha256('\\n'.join(rank_strings).encode()).hexdigest()\n",
    "\n",
    "print(\" Submission Signatures:\")\n",
    "print(f\"QUERY_SIGNATURE: {QUERY_SIGNATURE}\")\n",
    "print(f\"RANK_SIGNATURE:  {RANK_SIGNATURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Submission Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running submission validation...\n",
      "\n",
      " CHECK 1 FAILED: name 'OUTPUTS_DIR' is not defined\n",
      " CHECK 2 FAILED: name 'pd' is not defined\n",
      " CHECK 3 FAILED: name 'queries' is not defined\n",
      " CHECK 4 FAILED: name 'rankings' is not defined\n",
      " CHECK 5 FAILED: name 'zlib' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Validation checks\n",
    "print(\" Running submission validation...\\n\")\n",
    "\n",
    "checks_passed = 0\n",
    "total_checks = 0\n",
    "\n",
    "# Check 1: results.json exists and is valid\n",
    "total_checks += 1\n",
    "try:\n",
    "    with open(OUTPUTS_DIR / 'results.json', 'r') as f:\n",
    "        loaded_results = json.load(f)\n",
    "    assert loaded_results['roll'] == ROLL\n",
    "    assert loaded_results['dev_fold'] == dev_fold\n",
    "    assert loaded_results['test_fold'] == test_fold\n",
    "    print(\" CHECK 1: results.json valid\")\n",
    "    checks_passed += 1\n",
    "except Exception as e:\n",
    "    print(f\" CHECK 1 FAILED: {e}\")\n",
    "\n",
    "# Check 2: preds_test.csv exists and has correct format\n",
    "total_checks += 1\n",
    "try:\n",
    "    preds = pd.read_csv(OUTPUTS_DIR / 'preds_test.csv')\n",
    "    assert set(preds.columns) == {'id', 'pred'}\n",
    "    assert len(preds) == len(TEST)\n",
    "    assert set(preds['id']) == set(TEST['id'])\n",
    "    print(\" CHECK 2: preds_test.csv valid\")\n",
    "    checks_passed += 1\n",
    "except Exception as e:\n",
    "    print(f\" CHECK 2 FAILED: {e}\")\n",
    "\n",
    "# Check 3: queries.json has 20 queries (15 TF-IDF + 5 negation)\n",
    "total_checks += 1\n",
    "try:\n",
    "    assert len(queries) == 20\n",
    "    tfidf_queries = [q for q in queries if q['type'] == 'tfidf']\n",
    "    neg_queries = [q for q in queries if q['type'] == 'neg']\n",
    "    assert len(tfidf_queries) == 15\n",
    "    assert len(neg_queries) == 5\n",
    "    print(\" CHECK 3: queries.json valid (20 queries: 15 TF-IDF + 5 negation)\")\n",
    "    checks_passed += 1\n",
    "except Exception as e:\n",
    "    print(f\" CHECK 3 FAILED: {e}\")\n",
    "\n",
    "# Check 4: rankings.json has top-10 docs per query\n",
    "total_checks += 1\n",
    "try:\n",
    "    assert len(rankings) == 20\n",
    "    for qid, docs in rankings.items():\n",
    "        assert len(docs) == 10\n",
    "        assert all(doc_id in TEST['id'].values for doc_id in docs)\n",
    "    print(\" CHECK 4: rankings.json valid (10 TEST docs per query)\")\n",
    "    checks_passed += 1\n",
    "except Exception as e:\n",
    "    print(f\" CHECK 4 FAILED: {e}\")\n",
    "\n",
    "# Check 5: Fold assignment deterministic\n",
    "total_checks += 1\n",
    "try:\n",
    "    r_check = zlib.crc32(ROLL.encode())\n",
    "    dev_check = r_check % 5\n",
    "    test_check = (r_check // 5) % 5\n",
    "    if test_check == dev_check:\n",
    "        test_check = (test_check + 1) % 5\n",
    "    assert dev_fold == dev_check\n",
    "    assert test_fold == test_check\n",
    "    print(\" CHECK 5: Fold assignment deterministic\")\n",
    "    checks_passed += 1\n",
    "except Exception as e:\n",
    "    print(f\" CHECK 5 FAILED: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
